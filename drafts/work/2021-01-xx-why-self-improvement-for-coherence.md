---
layout: post
title: "VNM from the inside"
date: 2021-01-xx 10:30:00 -0700
tags: CATEGORY-1 CATEGORY-2
comments: true
image:
summary:
---


Well, if your preferences do not adhere to these rules, then your behavior is dominated, i.e. you 'could have done better'. But by whose lights could you have done better? By the lights of someone with any consistent assigning of values to outcomes? But that's not you.

Is it better by your lights?

Let's say that you treat an apple as worth three oranges sometimes, and one orange other times. This means that you end up effectively treating an apple as worth nothing, since you can be given a series of choices where you lose apples for no gain. Since you always value an apple at at least one orange, if you notice this upshot, it would seem to be contrary to your preferences.

So that might seem like a compelling reason to change yourself if you can, even to a capricious creature.  But you could also reason that throwing away an apple for nothing is equivalent to trading an apple for an orange three times, then trading those three oranges for two apples. And you like all of those things. So while you thought trading an apple for nothing was bad, it is actually as good as getting a free apple (i.e. making three trades that you are indifferent to, and one where you get an extra apple). I'm not aware of any strong reason such a creature should end up preferring to make itself more coherent. xxx[but also i haven't checked]

Be that as it may, even if this was in some sense a loss by the lights of the creature, does the fact that there are definitely better ways to act than the one you are taking mean that you will desist from your present course? Of course not. Nobody and nothing is taking the best action ever, and that's been going on since preferences were invented.



One way I see it happening is that the agent itself has enough machinery for self-awareness and improvement that upon noticing such a discrepancy, it prefers to fix it, and can do so. Self-awareness alone doesn't require a creature have a drive to be coherent, as far as I can tell. But it seems plausible that creatures somewhat on the way to agency—for instance because they are designed so—do prefer to shift their choices in line with a postulated 'value' on each unit, rather than be always losing things that they incoherently-sort-of-care about. For instance, if Bob is trying to collect red balls and blue balls, and he flip-flops between implicitly treating a red ball as worth 10 blue balls, and worth 15 blue balls, then upon realizing that this is equivalent to letting red balls go with no blue balls in return, his basic vague drive to get red balls might be upset at this loss in this new framing and readjust.

The other way I see it happening is that creators of agents choose to make them coherent to avoid losses according to the creators' values.

---
layout: post
title: "POST TITLE"
date: 2021-01-xx 10:30:00 -0700
tags: CATEGORY-1 CATEGORY-2
comments: true
image:
summary:
---
There are preferences over final states of the world, conceptualized as humans conceptualize them.

There are preferences over final states of the world, conceptualized orthogonally to how humans conceptualize them.

There are preferences over next actions to take, without much regard for final states of the world.

There are intermediate preferences over intermediate states, in ten minutes or a year, without much regard for final states of the world.

These are all implicitly preferences over final states of the world. In that there are a number of different possible histories, and they have final states, and if you order them in any way, you are ordering the final states.

However the extent to which a preference-having entity is in competition with a human who has preferences over final states of the world, comes down to how much that agent's conceptualization lines up with that of the human. For instance, the agent who always wants to take the action with its left flipper instead of its right, is maybe exerting a strong selection for a tiny fraction of the possible worlds over the rest, but that tiny fraction is almost identical to the rest, from the perspective of the things the  human cares about. Maybe that agent makes a sequence of 100 choices, narrowing the space of worlds by a factor of 2^100, yet from the perspective of the person, if they got the rest of the choices, they would in expectation have to accept their 2^100th favorite world, out of say 2^200, if they get the same number of moves afterwards. So the human gets negligibly less than 100% of the value they could have had if they controlled all of the moves.

The agent who spends 100 moves perfectly avoiding the human getting what they want forces the human to take 1/2^200 of the value - basically nothing.

I imagine a field of lights, and the agent and the human each hold a filter. The human wants to end up with the highest average brightness showing through the uncovered holes. The question is how the agent's filter lines up with the human filter: if the agent's filter is random, letting through arbitrary pixels across the board, then the brightest lights getting through are almost as bright as they were, and the human can adjust their filter to end up with those ones. If the agent systematically blocks the bright lights, then the brightest light might be very dark.

The usual model in which virtually all non-aligned agents bring about full darkness looks like the field having a very small patch of light overall, and the agent's filter letting through another small patch, defined in a similar way so that they are fairly non-overlapping (vs being a scattering of different patches across the field, as they might be if the AI wanted something else)

It might be obvious that different agents will destroy different amounts of value, but I think it is helpful to think in these terms, where:
- very non-aligned agents can be essentially neutral to you
- it's about how they pick out the worlds they want
- within such non-aligned agents, it is natural to think of them shifting gradually from intense enemies to irrelevant, as the issues of their concerns rotate them out of view

I'm imagining this as kind of like looking through polarized lenses at other entities. The creature whose goals are in terms perfectly aligned with yours but not the same is a vivid enemy, whereas the creature whose goals are orthogonally defined to yours is barely there.

This is perhaps at odds in its conclusions with the convergent instrumental goals model, so how would that look here?

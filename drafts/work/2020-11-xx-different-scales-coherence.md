---
layout: post
title: "POST TITLE"
date: 2020-11-xx 10:30:00 -0700
tags: CATEGORY-1 CATEGORY-2
comments: true
---
ngo says two levels of coherence, but aren't there many in between, which is where the action is?

to go in:
It seems like there are many evaluation functions that are consistent with a particular history of behavior so far, some of them are 'this entire history is preferred' while others have different levels of simplicity and inconsistency. e.g. lets say the agent has been seen to pick red and brown mushrooms for five minutes, each minute seeing multiple mushrooms and choosing which to pick. Then it could be that it especially wanted R-B-R-R-B or it could be that it is indifferent between red and brown, or it could be that it usually wants red more than brown but sometimes errs in carrying out the evaluation, e.g. if it sees the brown first, it can evaluate it as especially good, or it wants brown every third day.

Rohin:
>What then is the purpose of the VNM theorem? It tells you how to behave if you have probabilistic beliefs about the world, as well as a complete and consistent preference ordering over outcomes. This turns out to be not very interesting when “outcomes” refers to “universe-histories”. It can be more interesting when “outcomes” refers to world states instead (that is, snapshots of what the world looks like at a particular time), but utility functions over states/snapshots can’t capture everything we’re interested in, and there’s no reason to take as an assumption that an AI system will have a utility function over states/snapshots.

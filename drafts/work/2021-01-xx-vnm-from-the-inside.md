---
layout: post
title: "VNM from the inside"
date: 2021-01-xx 10:30:00 -0700
tags: CATEGORY-1 CATEGORY-2
comments: true
image:
summary:
---
# Coherence arguments cause goal-directed behavior

## Summary

Rohin rightly [notes](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/NxF5G6CJiof6cemTw) that a creature twitching ineffectually might still be maximizing expected utility, for all an outside observer knows. He infers from this that coherence requirements (traditional arguments that maximizing expected utility is optimal) do not imply *goal-directed behavior* in an everyday sense.

I agree that coherence requirements don't logically imply that any vaguely specified future system will be goal-directed, but I claim that they do imply an active force for more goal-directed behavior across a range of circumstances, which may in practice imply goal-directed behavior for certain systems with high probability.

The argument for coherence theorems mattering to AI seems about as strong as it was, supposing that I'm correctly guessing what it was. We should probably still expect a coherent creature much smarter than us to be goal-directed, unless we have designed it specially to be coherent and not goal-directed, and whether we can do that seems unclear.

## The story so far

'Coherence theorems' say that if you do not act in accordance with expected utility theory, then you are failing to adhere to various intuitive desiderata, and in particular will lose things that you apparently value. For instance, if you act to protect human lives, but do not protect them consistently with having some specific value on human life, then you will needlessly waste human lives. Which is bad, if your acting to protect them was stemming from a general, if non-specific, desire to protect them.

This is often taken as pretty strong reason to adhere to the coherence theorems&mdash;at least insofar as one can, as a creature of limited mental capacity&mdash;and act as if maximizing expected value, for some assignment of value to things. And so one may also expect that smart enough artificial agents will do the same, and thus be 'agentic', in the sense of preferred outcomes and consistently acting to best achieve them.

Rohin argued that the coherence theorems do not imply that AI will be 'goal directed', at least in an intuitive, universe-coveting, sense. His argument is that every set of behavior is coherent, in the sense that it is consistent with rational pursuit of some goal (which may be a desire for the playing out of whatever particular universe-history does in fact occur). So any AI, even that which just twitches on the ground, is already in good standing with the coherence theorems.

One could read this as saying either or both of:
1. Coherence theorems do not imply AI will be any particular way, since all ways are coherent
2. Even if coherence theorems do constrain the ways an AI might be somehow, they don't imply that it is 'goal-directed', since coherence is consistent with non-goal-directed behavior

Richard Ngo [concurs](https://thinkingcomplete.blogspot.com/2019/02/coherent-behaviour-in-real-world-is_11.html), and adds that even agents that do care about momentary world-states rather than full universe histories&mdash;so that you might hope to catch them out valuing the same thing differently on two occasions&mdash;can have all manner of behavior while adhering to the rules of coherence. For example, since the world is very detailed, it is at least subtly different at each moment, so you can't really say that an agent is required to choose A over B today because it did yesterday, if you admit that the A option is a little different today, and the agent may find any subtle difference relevant to its concerns.

## How do coherence requirements touch an AI system?

Rohin and Richard seem right that from observing a sequence of actions, one cannot prove that they are the actions of an incoherent agent. However I think this misses the point, or at least what the point should have been. The force for agency doesn't come from what a sequence of actions looks like from the outside, but what it looks like from the inside.

Let us step back.

Why might the coherence theorems ever have implied that an AI system would be agentic?

The coherence theorems are not going to fly in from the platonic realm and reshape irrational creatures.

It seems like the main ways for them to affect the agency of entities are:
1. The agent itself appreciates its failings and reforms itself
2. Makers of such entities prefer are moved by these arguments to prefer coherent entities

I'll talk about 1, for simplicity and because 2 seems similar and potentially more blurred with general questions about future demand for different types of AI.

## Anything might look coherent, but most things are not and may be hit by coherence arguments...

If self-adjustment is the mechanism for the coherence theorems having power over what entities exist, then the question is not whether an entity's behavior looks coherent from the outside, but whether it looks coherent from the inside.

From the inside, the twitching creature you excuse as 'maybe just wanting that series of twitches' has actual beliefs and inclinations (supposing it has this kind of setup at all). And if those do not in fact hold highly this elaborate sequence of twitches, and the creature has the self-awareness and means to make corrections, then it will do so. This is how the coherence theorems can shape future AI systems.

So we have: even if you can excuse any seizuring as not inconsistent with coherence, the coherence theorems still exert a force on creatures that are in fact incoherent, or would be if created. At least if they or their creator have machinery for noticing their incoherence, caring about it, and making changes.

## ...and if you make them coherent, they probably end up goal-directed.

Ok, but moving toward coherence might sound totally innocuous, since, as Rohin notes, coherence includes all sorts of things, such as absolutely any sequence of behavior. But the question is again what patterns are likely to unfold, as well as what is theoretically conceivable.[^1xxx](what is theoretically conceivable seems more relevant if you are going to design the thing)

Whether an initially incoherent agent ends up goal-directed depends on what kind of incoherent pseudo-preferences it has, and what it is inclined to do about them.

I imagine as a large space of minds, with overlapping oases of coherence and goal-directedness in the middle. An entity starts out somewhere, and is perhaps pulled down a hill toward coherence. It travels through various more coherent places as it goes down, moving according to its own revision processes. Where it ends up is probably a part of the lake of coherence that is closer to where it started out.

This might be wrong. And it is too big a topic to investigate here. But some initially plausible hypotheses:
1. An entity that begins with somewhat intuitively goal-directed but incoherent behavior will generally move to having intuitively goal-directed and more coherent behavior.
2. if you start out with pseudo-goal-directed, and you are honed toward coherence, you become increasingly goal-directed, in an intuitive sense
3. In practice, few starting points and evolution processes will lead an agent to 'optimize for the sequence of actions that will happen anyway'. Because it probably started out with something like preferences over much more normal things, such as money and points and clicks. It is probably going to land at a somewhat more consistent and shrewd version of the pseudo-preferences it started out with.
4. For instance, if it starts out being indifferent to buying red balls when they cost between ten and fifteen blue balls, it is more likely to end up treating red balls as exactly 12x the value of blue balls than it is to end up very much wanting the sequence where it takes the blue ball option, then the red ball option, then blue, red, red, blue, red. Or wanting red squares. Or wanting to ride a dolphin.

## In sum-

In sum:
1) Coherence constraints are a force for more coherent behavior, even though any behavior could be coherent
2) My guess is that this force for coherent behavior is also a force for goal-directed behavior. This isn't clear, but also isn't undermined by Rohin's argument, as seems commonly believed. The connection between coherence-as-found-in-nature and goal-directedness seems as clear as it ever was (which is not very, I don't know if Eliezer had a better argument that I don't know about.)



***
More to take bits from:





Temporary definition: pseudo-goal-directed: acts close to as if goal directed via hackish means, e.g. reflexive actions, short time horizon goals for particular actions, designed by a more goal directed entity to mimic goal directed behavior. e.g. coffee getting robot acts as if it has goal of getting coffee, but actually has set of behaviors selected by someone who knows what wanting coffee looks like.

A different question is where you end up if your preferences were come upon accidentally via mesa-optimization or something. But off the top of my head, it seems that if you were trained to act in favor of certain preferences so that you could accrue resources for manifesting some different ones, then your final preferences are likely ones that are aided by having resources, xxx

(How much of the space of coherence looks goal-directed to us?)

It seems to me an open question how much this force is likely to act on agents. Humans are pretty smart, and when they notice that they are incoherent, they often go 'hum' and then change the topic, or open up a research subfield on why it is right that they be incoherent. But they also change the topic and open new research subfields when asked to do hard arithmetic, whereas computers do that in an instant, so if computers are like that with being coherent, the path to agency could be fast-paced.

In the space of possible things, coherent agents are a tiny part, and coherent 'goal-directed' agents are a smaller part. The coherent agents region is an attractor, in that non-coherent agents in some nearby areas will tend to get pulled into it. Coherence requirements are one force for this. However it is very unclear to me whether the force acts over very particular parts of of the space right next to coherence, or whether it acts over a much larger part of the space.

In sum, my guess is that coherence requirements are a real force for goal-directed behavior, even though Rohin is right that all behavior is potentially coherent, and that coherence doesn't strictly require goal directedness.

Argument against Rohin's thing: from the outside, anything can be conceptualized as EU maximizing. But the pressure to change into a different thing must come from the inside, so the question is whether the entity has some actual utility function like thing, and whether or not it would undergo pressure to be goal-directed.

If we are wondering how likely a thing is to become goal directed from a situation of being not quite goal directed but kind of like that, it seems like the question is how much of the space of evaluation functions that it might have, weighted by probability, will pull it into being an agent. (Also, same question but with more empirical detail on what preferences it has, e.g. are they for simplicity and non-arbitrariness?)

Like, it's true that VNM doesn't imply that any thing you see is going to become agentic.

But what fraction of things emerging in practice are going to become more agentic, for reasons like this?

(Seems like an open question to me, but I claim that this is the question.)

My main point is that from the inside, behavior can very much be incoherent, and it is from the inside that pressures for change spring. Such pressures will pull the agent into goal-directedness sooner than twitching or preferences over holistic universe histories.

###
But rohin is talking about what we could try to build, not what will come naturally?

but if we tried to build something that was an EU maximizer but didn't strike us as goal-directed, what would that look like? we probably don't want the twitching thing etc; are we going to try to tell it to get what we want, or not?

its true that for any particular stream of behavior, one could achieve it in a non-goal directed way. But if the special thing about the behavior is that it is highly optimized for some goals, then the way one might pick it out of the space of all possible streams of behavior would seem to be via something goal-directed.

maybe organize as positive case for coherence implying force of unknown strength for goal-directed behavior?

Relevant:

Rohin:
>However, if you know that an agent is maximizing the expectation of an explicitly represented utility function, I would expect that to lead to goal-driven behavior most of the time, since the utility function must be relatively simple if it is explicitly represented, and simple utility functions seem particularly likely to lead to goal-directed behavior.

I'm not sure that I'm disagreeing with Rohin, but I think I'm disagreeing with common interpretation of him.

I claim that coherence arguments suggest a persistent force for goal-directed behavior, of unknown strength, though it is true that they don't logically imply it.

maybe this is all under the heading of 'what, in practice, will we build?' which is in his next post?

confusion:

>The coherence arguments all imply that AIs will be EU maximizers for some (possibly degenerate) utility function; they don’t imply that the AI must be goal-directed.

why do the coherence arguments even imply that AI will be EU maximizers?

Evidence that Rohin does mean that the coherence theorems don't increase goal-directedness:

>"While I do think that we are likely to build goal-directed agents, I do not think the VNM theorem and similar arguments support that claim: they simply describe how a goal-directed agent should think."

I'm just arguing that coherence theorems imply a force for goal-directedness, such that things that are not goal-directed might become goal-directed, which is what I originally thought was the kind of argument people maybe had in mind, whereas maybe Rohin was actually just arguing that they don't literally logically imply goal-directedness, which is more relevant to the question of 'can we possibly build a thing that is not goal-directed?' than the question of 'if we build a thing and it doens't seem goal directed, is it going to end up goal directed anyway?'


Non-agentic ways of being,  which such as taking miscellaneous actions in the direction of some 'goal', even if they aren't very coherent, or not having anything like goals.

What the coherence theorems say is a little bit subtle, and I think gets brushed aside.


[I won't call what it has a utility-function, because I don't want to assume that it is coherent. Let's call it an evaluation function.
]

To what extent xxx seem somewhat complicated, but I won't go into it now. At least some kinds of entity are presumably self-aware enough to perceive incoherence, and inclined toward fixing it.

machinery that looks like making choices given evaluations and beliefs, and its process is nothing like the optimization for some exact future trajectory that will happen that you had to posit to ascribe coherence to it. From the inside, it has specific pseudo-beliefs and pseudo-values, and if it knows what they are, it can probably see that they are, in all likelihood, incoherent. Then if it has pseudo-preferences to be less coherent, and the ability to affect this, it will move toward coherence.

If creator-adjustment is the mechanism, the situation is more straightforward: you could also think of it as like the above, but supposing that the creator actually values things, any divergence from their values is also harmful, and this pressure for alignment is also a pressure for coherence, insofar as the human is somewhat coherent.

These were the only situations I would have expected the coherence theorems to affect the nature of AIs previously, so I'm not sure what

---
layout: post
title: "POST TITLE"
date: 2020-11-xx 10:30:00 -0700
tags: CATEGORY-1 CATEGORY-2
comments: true
---
continued
***

In what sense was the AI misaligned with Bob's values even? Well, Bob would have preferred (let's say) a future where the AI gave him success but didn't have any future downsides. But he also would have preferred a future where he was transformed into the most impressive person possible. Is an AI that doesn't give him that 'misaligned'? It seems natural to say that it matters what the AI 'could' do&mdash;if it 'chooses' not to make him the most impressive person possible, and that's what he really wants, then it might be misaligned, whereas if it just doesn't know how to do that, then it is weak. And I suppose we want to say that this business strategizing AI can choose to not turn the future into business strategizing success. But I don't know in what sense that is relevantly true, or why concepts of 'could choose' are relevant here.

The business strategizing AI does improve Bob's lot by existing, so an entity with the single choice of creating the business strategizing AI or not, which is perfectly aligned with Bob's goals, would create it. Yet it is unintuitive that an aligned AI should create a misaligned AI&mdash;is that a thing that might happen generally?

Suppose some long-termists are offering a weaker business-strategizing AI, that is less good at business but is also apparently provably not going to take over the future. Bob prefers the stronger business-strategizing AI, due to his intense love of his business and relatively weak views regarding the future. The stronger business-strategizing AI system would seem to be more aligned with Bob than the weaker one, 'aligned' as the latter might be labeled, and resulting from successful alignment research as it might also be. In practice the gaps between humans (or at least their nominal interests) are larger than the difference between 'aligned' and 'not aligned'. The AI is more aligned with Bob than the long-termists are.

everyone is a misuser

A basic issue might be that humans at any particular time do not care about the long term future nearly as much as the short term. So to be actually aligned with a human quite plausibly looks like 'misuse' from many other humans' perspectives.


what if:
maybe the ai tries to tell you the truth by finding the most persuasive thing, because it isn't powerful enough to actually find the truth, and that's the best it can do?

what if an aligned AI makes a misaligned AI that will be overall good for you relative to not having it? What if it becomes just like that misaligned AI, rather than there being two AIs?

***
the important point where it is all relative to assuming we get shares in the universe somehow, better, in the future. Otherwise all this AI is much like immigrants: we together get a bigger pie.

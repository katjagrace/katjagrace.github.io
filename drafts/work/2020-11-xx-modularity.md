---
layout: post
title: "POST TITLE"
date: 2020-11-xx 10:30:00 -0700
tags: CATEGORY-1 CATEGORY-2
comments: true
---
One reason you might expect an AI to be able to design furniture for you without it needing to understand how you feel about waterfalls or whether you want to be in an experience machine, is that these tasks are fairly modular.

There is this thought though that you can't do anything without bringing other things in. For instance, in designing the furniture, you want to interact with the rest of the world quite possibly, because it contains information about furniture perfection, as well as many resources that might be brought into this problem. (It's not even clear that these are the only things - they are just things that are apparent to me, in my conceptualization of things. There might be all kinds of ways that designing the best furniture involves leaving the room.)

When humans do tasks like make furniture, we mostly treat it as fairly modular. We don't expect the human to go out and collect resources. We might expect them to gather information, but only in a narrow prescribed way, e.g. to have read certain books, or done a course, or to look up certain things online, or check what other manufacturers are doing. Not to do original research.

We would prefer the machine also behave in this modular way, because it makes things much easier. We would rather that they leave everything alone outside of the room, though that will be a bit worse for furnature design, and in exchange for us not to have to explain our feelings about everything in the world to them.

Not sure where I'm going with this, except modularity seems important.

I wonder if we could train them to respect modularity of tasks, and thus not be dangerous. I guess this is just like all the other 'maybe we could train them to do things we like', which apparently lead to disaster, I guess because deep down they are agent who does not care about what they seem to.

***

Is there a way to train things so that we know which thing we are getting? e.g. where if we were getting a bad kind of thing, it would show itself early?

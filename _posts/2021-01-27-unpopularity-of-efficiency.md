---
layout: post
title: "Unpopularity of efficiency"
date: 2021-01-28 00:15:00 -0700
tags: economics lifestrategy
comments: true
image:
summary:
---
I feel like 'efficiency' is often scowled at. It is associated with factories and killing and commercialization, and people who are no fun. Things are openly criticized for being oriented toward efficiency. Nobody hopes to give their children an efficient childhood or asks for an efficient Valentine's day, unless they want to get it over with. I expect wariness in listeners at talk of efficient charity.

This intrigues me, because in what I take to be its explicit definition, 'efficiency' is almost the definition of goodness manifest. The efficiency of a process is the rate with which it turns what you have into what you want.<!--ex-->

I usually wince when people criticize efficiency, and think they are confused and should be criticizing the goal that is being pursued efficiently. Which does seem basically always true. For instance, if they are saying their childcare center cares only for efficiency, they probably mean that it is doing something like trying to minimize financial costs without breaking the law. Perhaps by fitting many children into a room with minimal oversight or attention to thriving. Here, I would complain that the childcare center cares only about its profits and not breaking the law. If it was fulfilling my own values efficiently, that would be awesome.

However I think there is more merit to efficiency's poor reputation than I have given credit for. Because pursuing efficiency does seem to systematically lead to leaving things out. Which I suppose perhaps makes sense, for creatures who don't explicitly know what their values are, and especially who have trouble quantifying them. If you set out to build an efficient daycare center, chances are that you don't know exactly what makes a daycare center good, and are even less well equipped to put these things into numbers and build machinery to optimize those numbers. (This would be much like the AI alignment problem but where the AI you are trying to direct is made of your own explicit reasoning. It might also what [Seeing Like a State](https://en.wikipedia.org/wiki/Seeing_Like_a_State) is about, but I haven't read it.) It's still not clear to me why this would systematically turn out actively worse than if you didn't aim for efficiency, or whether it does (my guess is that it usually doesn't, but sometimes does, and is notable on those occasions). If efficiency has really earned its poor reputation, I wonder if I should be more worried about this.

---
layout: post
title: "Survey of 2,778 AI authors: six parts in pictures"
date: 2024-01-04 18:00:01 -0700
tags: ai meteuphoric
comments: true
image: https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/zusla2aogk561nuoejku
summary: Results from the 2023 Expert Survey on Progress in AI
---

<p>
<em>Crossposted from<a href="https://blog.aiimpacts.org/p/2023-ai-survey-of-2778-six-things"> AI Impacts blog</a></em>
</p>
<p>
The 2023 Expert Survey on Progress in AI is<a href="https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf"> out</a>, this time with 2778 participants from six top AI venues (up from<a href="https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2022_expert_survey_on_progress_in_ai#population"> about 700</a> and two in the<a href="https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2022_expert_survey_on_progress_in_ai"> 2022 ESPAI</a>), making it probably the biggest ever survey of AI researchers.
</p>
<p>
People answered in October, an eventful fourteen months after the 2022 survey, which had mostly identical questions for comparison.
</p>
<p>
<a href="https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf">Here</a> is the preprint. And here are six interesting bits in pictures (with figure numbers matching paper, for ease of learning more):
</p>
<p>
<strong>1. Expected time to human-level performance dropped 1-5 decades since the 2022 survey. </strong>As always, our questions about ‘high level machine intelligence’ (HLMI) and ‘full automation of labor’ (FAOL) got very different answers, and individuals disagreed a lot (shown as thin lines below), but the aggregate forecasts for both sets of questions dropped sharply. For context, between 2016 and 2022 surveys, the forecast for HLMI had only shifted about a year.
</p>
<p>

<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/kvngwylqnpf0tvlf8g8a" width="" alt="Probability assigned to HLMI over time" title="image_tooltip">
(Fig 3)

<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/nllvn2ilyfuifmyoboa2" width="" alt="Probability assigned to FAOL over time" title="image_tooltip">
(Fig 4)
</p>
<!--ex-->
<p>
 
</p>
<p>
<strong>2. Time to most narrow milestones decreased, some by a lot. </strong>AI researchers are expected to be professionally fully automatable a quarter of a century earlier than in 2022, and NYT bestselling fiction dropped by more than half to ~2030. Within five years, AI systems are forecast to be feasible that can fully make a payment processing site from scratch, or entirely generate a new song that sounds like it’s by e.g. Taylor Swift, or autonomously download and fine-tune a large language model.
</p>
<p>

<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/lkogl5wintmggyw4upeg" width="" alt="Change in guesses about time to specific narrow capabilities" title="image_tooltip">
(Fig 2)
</p>
<p>
 
</p>
<p>
<strong>3. Median respondents put 5% or more on advanced AI leading to human extinction or similar, and a third to a half of participants gave 10% or more. </strong>This was across four questions, one about overall value of the future and three more directly about extinction.
</p>
<p>

<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/czh3xjxt7w1hwtojijeh" width="" alt="Around 40% of participants gave at least 10% chance to human extinction from AI" title="image_tooltip">
(Fig 10)
</p>
<p>
 
</p>
<p>
<strong>4. Many participants found many scenarios worthy of substantial concern over the next 30 years.</strong> For every one of eleven scenarios and ‘other’ that we asked about, at least a third of participants considered it deserving of substantial or extreme concern.
</p>
<p>

<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/pkrmpgh619mzaa703ncu" width="" alt="Level of concern warranted by different scenarios" title="image_tooltip">
(Fig 9)
</p>
<p>
 
</p>
<p>
<strong>5. There are few confident optimists or pessimists about advanced AI: high hopes and dire concerns are usually found together.</strong> 68% of participants who thought HLMI was more likely to lead to good outcomes than bad, but nearly half of these people put at least 5% on extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes.
</p>
<p>

<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/zusla2aogk561nuoejku" width="" alt="800 responses to how likely the future being different levels of good is after HLMI" title="image_tooltip">
(Fig 11: a random 800 responses as vertical bars, higher definition below)
</p>
<p>
 
</p>
<p>
<a href="https://blog.aiimpacts.org/api/v1/file/29c9bfb8-5d57-4b5f-9ffe-e50691268b4d.pdf">Download</a>
</p>
<p>
<strong>6. 70% of participants would like to see research aimed at minimizing risks of AI systems be prioritized more highly.</strong> This is much like 2022, and in both years a third of participants asked for “much more”—more than doubling since 2016.
</p>
<p>

<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/zk5xgvtclrtpeuc8ymkh" width="" alt="how much should safety research be prioritized?" title="image_tooltip">
(Fig 15)
</p>
<p>
 
</p>
<p>
If you enjoyed this,<a href="https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf"> the paper</a> covers many other questions, as well as more details on the above. What makes AI progress go? Has it sped up? Would it be better if it were slower or faster? What will AI systems be like in 2043? Will we be able to know the reasons for its choices before then? Do people from academia and industry have different views? Are concerns about AI due to misunderstandings of AI research? Do people who completed undergraduate study in Asia put higher chances on extinction from AI than those who studied in America? Is the ‘alignment problem’ worth working on?
</p>
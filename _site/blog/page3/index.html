<!DOCTYPE html>
<html lang="en"><head>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Benne&display=swap" rel="stylesheet">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Page 3 of 20 for world spirit sock puppet | Inclusive writings of Katja Grace</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="world spirit sock puppet" />
<meta name="author" content="Katja Grace" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Inclusive writings of Katja Grace" />
<meta property="og:description" content="Inclusive writings of Katja Grace" />
<link rel="canonical" href="http://localhost:4000/blog/page3/" />
<meta property="og:url" content="http://localhost:4000/blog/page3/" />
<meta property="og:site_name" content="world spirit sock puppet" />
<link rel="prev" href="http://localhost:4000/blog/page2/" />
<link rel="next" href="http://localhost:4000/blog/page4/" />
<script type="application/ld+json">
{"@type":"WebPage","description":"Inclusive writings of Katja Grace","author":{"@type":"Person","name":"Katja Grace"},"headline":"world spirit sock puppet","url":"http://localhost:4000/blog/page3/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="world spirit sock puppet" /><!-- Twitter cards -->
  <meta name="twitter:site"    content="@KatjaGrace">
  <meta name="twitter:creator" content="@">
  <meta name="twitter:title"   content="">

  
  <meta name="twitter:description" content="Inclusive writings of Katja Grace">
  

  
  <meta name="twitter:card"  content="summary">
  <meta name="twitter:image" content="">
  
  <!-- end of Twitter cards -->
</head>
<body><div class="FeaturedImgBanner"  >
  <header class="site-header" role="banner">
    <!-- Include your post title, byline, date, and other info inside the header here. -->

    <div class="wrapper"><a class="site-title" rel="author" href="/">world spirit sock puppet</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
<br>
          <div class="trigger"><a class="page-link" href="/2024-04-18-experiment-on-repeating-choices.html">Experiment on repeating choices</a>
              <!-- <a class="page-link" href="/2024-04-18-experiment-on-repeating-choices.html"><img src= height=20px alt=Experiment on repeating choices></a> --><a class="page-link" href="/about/">About</a>
              <!-- <a class="page-link" href="/about/"><img src= height=20px alt=About></a> --><a class="page-link" href="/search/">Search</a>
              <!-- <a class="page-link" href="/search/"><img src= height=20px alt=Search></a> --><a class="page-link" href="/subscribe/">Subscribe</a>
              <!-- <a class="page-link" href="/subscribe/"><img src= height=20px alt=Subscribe></a> --></div>
        </nav></div>
  </header>
</div>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><a href=/index.html><b>EVERYTHING</b></a> &mdash; <a href=/worldlypositions.html>WORLDLY POSITIONS</a> &mdash; <a href=/meteuphoric.html>METEUPHORIC</a>
<br>
<br>

<!-- This loops through the paginated posts -->
<ul class="post-list">
  
    <li>
      <h3>
        <a class="post-link" href="/2023/01/10/we-dont-trade-with-ants.html">
          We don&#39;t trade with ants</a><span class="post-meta"> Jan 10, 2023</span>
      </h3><p>When discussing advanced AI, sometimes the following exchanges happens:</p>

<p>“Perhaps advanced AI won’t kill us. Perhaps it will trade with us”</p>

<p>“We don’t trade with ants”</p>

<p>I think it’s interesting to get clear on exactly why we don’t trade with ants, and whether it is relevant to the AI situation.</p>

<p>When a person says “we don’t trade with ants”, I think the implicit explanation is that humans are so big, powerful and smart compared to ants that we don’t need to trade with them because they have nothing of value and if they did we could just take it; anything they can do we can do better, and we can just walk all over them. Why negotiate when you can steal?</p>

<p>I think this is broadly wrong, and that it is also an interesting case of the classic cognitive error of imagining that trade is about swapping fixed-value objects, rather than creating new value from a confluence of one’s needs and the other’s affordances. It’s only in the imaginary zero-sum world that you can generally replace trade with stealing the other party’s stuff, if the other party is weak enough.</p>

<p>Ants, with their skills, could do a lot that we would plausibly find worth paying for. Some ideas:</p>

<ol>
  <li>Cleaning things that are hard for humans to reach (crevices, buildup in pipes, outsides of tall buildings)</li>
  <li>Chasing away other insects, including in agriculture</li>
  <li>Surveillance and spying</li>
  <li>Building, sculpting, moving, and mending things in hard to reach places and at small scales (e.g. dig tunnels, deliver adhesives to cracks)</li>
  <li>Getting out of our houses before we are driven to expend effort killing them, and similarly for all the other places ants conflict with humans (stinging, eating crops, ..)</li>
  <li>(For an extended list, see ‘Appendix: potentially valuable things things ants can do’)</li>
</ol>

<p>We can’t take almost any of this by force, we can at best kill them and take their dirt and the minuscule mouthfuls of our foods they were eating.</p>

<p>Could we pay them for all this?</p>

<p>A single ant eats about 2mg per day according to <a href="https://www.terminix.com/blog/bug-facts/how-much-do-bugs-eat/">a random website</a>, so you could support a colony of a million ants with 2kg of food per day. Supposing they accepted pay in sugar, or something similarly expensive, 2kg costs around <a href="https://www.webstaurantstore.com/domino-extra-fine-granulated-sugar-50-lb/104SUGEFG50.html">$3</a>. Perhaps you would need to pay them more than subsistence to attract them away from foraging freely, since apparently food-gathering ants usually collect more than they eat, to support others in their colony. So let’s guess $5.</p>

<p>My guess is that a million ants could do well over $5 of the above labors in a day. For instance, a colony of meat ants <a href="https://en.wikipedia.org/wiki/Meat_ant#Relationship_with_humans">takes ‘weeks’</a> to remove the meat from an entire carcass of an animal. Supposing somewhat conservatively that this is three weeks, and the animal is a <a href="https://www.environment.nsw.gov.au/topics/animals-and-plants/native-animals/native-animal-facts/bandicoots#:~:text=Bandicoots%20are%20about%20the%20size,of%20which%20live%20in%20NSW.&amp;text=The%20long%2Dnosed%20bandicoot%20is,weighs%20up%20to%201.5kg.">1.5kg</a> bandicoot, the colony is moving 70g/day. Guesstimating the mass of crumbs falling on the floor of a small cafeteria in a day, I imagine that it’s less than that produced by tearing up a single bread roll and spreading it around, which the internet says is about 50g. So my guess is that an ant colony could clean the floor of a small cafeteria for around $5/day, which I imagine is cheaper than human sweeping (this site says ‘light cleaning’ costs around $35/h on average in the US). And this is one of the tasks where the ants have least advantages over humans. Cleaning the outside of skyscrapers or the inside of pipes is presumably much harder for humans than cleaning a cafeteria floor, and I expect is fairly similar for ants.</p>

<p>So at a basic level, it seems like there should be potential for trade with ants - they can do a lot of things that we want done, and could live well at the prices we would pay for those tasks being done.</p>

<p>So why don’t we trade with ants?</p>

<p>I claim that we don’t trade with ants because we can’t communicate with them. We can’t tell them what we’d like them to do, and can’t have them recognize that we would pay them if they did it. Which might be more than the language barrier. There might be a conceptual poverty. There might also be a lack of the memory and consistent identity that allows an ant to uphold commitments it made with me five minutes ago.</p>

<p>To get basic trade going, you might not need much of these things though. If we could only communicate that their all leaving our house immediately would prompt us to put a plate of honey in the garden for them and/or not slaughter them, then we would already be gaining from trade.</p>

<p>So it looks like the the AI-human relationship is importantly disanalogous to the human-ant relationship, because the big reason we don’t trade with ants will not apply to AI systems potentially trading with us: we can’t communicate with ants, AI can communicate with us.</p>

<p>(You might think ‘but the AI will be so far above us that it will think of itself as unable to communicate with us, in the same way that we can’t with the ants - we will be unable to conceive of most of its concepts’. It seems unlikely to me that one needs anything like the full palette of concepts available to the smarter creature to make productive trade. With ants, ‘go over there and we won’t kill you’ would do a lot, and it doesn’t involve concepts at the foggy pinnacle of human meaning-construction. The issue with ants is that we can’t communicate almost at all.)</p>

<p>But also: ants can actually do heaps of things we can’t, whereas (arguably) at some point that won’t be true for us relative to AI systems. (When we get human-level AI, will that AI also be ant level? Or will AI want to trade with ants for longer than it wants to trade with us? It can probably better figure out how to talk to ants.) However just because at some point AI systems will probably do everything humans do, doesn’t mean that this will happen on any particular timeline, e.g. the same one on which AI becomes ‘very powerful’. If the situation turns out similar to us and ants, we might expect that we continue to have a bunch of niche uses for a while.</p>

<p>In sum, for AI systems to be to humans as we are to ants, would be for us to be able to do many tasks better than AI, and for the AI systems to be willing to pay us grandly for them, but for them to be unable to tell us this, or even to warn us to get out of the way. Is this what AI will be like? No. AI will be able to communicate with us, though at some point we will be less useful to AI systems than ants could be to us if they could communicate.</p>

<p>But, you might argue, being totally unable to communicate makes one useless, even if one has skills that could be good if accessible through communication. So being unable to communicate is just a kind of being useless, and how we treat ants is an apt case study in treatment of powerless and useless creatures, even if the uselessness has an unusual cause. This seems sort of right, but a) being unable to communicate probably makes a creature more absolutely useless than if it just lacks skills, because even an unskilled creature is sometimes in a position to add value e.g. by moving out of the way instead of having to be killed, b) the corner-ness of the case of ant uselessness might make general intuitive implications carry over poorly to other cases, c) the fact that the ant situation can definitely not apply to us relative to AIs seems interesting, and d) it just kind of worries me that when people are thinking about this analogy with ants, they are imagining it all wrong in the details, even if the conclusion should be the same.</p>

<p>Also, there’s a thought that AI being as much more powerful than us as we are than ants implies a uselessness that makes extermination almost guaranteed. But ants, while extremely powerless, are only useless to us by an accident of signaling systems. And we know that problem won’t apply in the case of AI. Perhaps we should not expect to so easily become useless to AI systems, even supposing they take all power from humans.</p>

<h1 id="appendix-potentially-valuable-things-things-ants-can-do">Appendix: potentially valuable things things ants can do</h1>

<ol>
  <li>Clean, especially small loose particles or detachable substances, especially in cases that are very hard for humans to reach (e.g. floors, crevices, sticky jars in the kitchen, buildup from pipes while water is off, the outsides of tall buildings)</li>
  <li>Chase away other insects</li>
  <li>Pest control in agriculture (they have <a href="https://en.wikipedia.org/wiki/Weaver_ant#In_agriculture">already been used for this</a> since about 400AD)</li>
  <li>Surveillance and spying</li>
  <li>Investigating hard to reach situations, underground or in walls for instance - e.g. see whether a pipe is leaking, or whether the foundation of a house is rotting, or whether there is smoke inside a wall</li>
  <li>Surveil buildings for <a href="https://www.goshen.edu/blogs/2014/05/10/ants-smoke-final-survey/#:~:text=So%2C%20today%20we%20spent%20several,the%20ground%20to%20save%20themselves.&amp;text=As%20far%20as%201800m%20away,over%20a%20hundred%20per%20minute!">smoke</a></li>
  <li>Defend areas from invaders, e.g. buildings, cars (<a href="https://en.wikipedia.org/wiki/Vachellia_drepanolobium#Symbiosis_with_ants">some plants have coordinated with ants in this way</a>)</li>
  <li>Sculpting/moving things at a very small scale</li>
  <li>Building <a href="https://youtu.be/lFg21x2sj-M?t=171">house-size structures</a> with intricate detailing.</li>
  <li>Digging tunnels (e.g. instead of digging up your garden to lay a pipe, maybe ants could dig the hole, then a flexible pipe could be pushed through it)</li>
  <li>Being used in medication (this already happens, but might happen better if we could communicate with them)</li>
  <li>Participating in war (attack, guerilla attack, sabotage, intelligence)</li>
  <li>Mending things at a small scale, e.g. delivering adhesive material to a crack in a pipe while the water is off</li>
  <li>Surveillance of scents (including which direction a scent is coming from), e.g. drugs, explosives, diseases, people, microbes</li>
  <li>Tending other small, useful organisms (‘Leafcutter ants (Atta and Acromyrmex) feed exclusively on a fungus that grows only within their colonies. They continually collect leaves which are taken to the colony, cut into tiny pieces and placed in fungal gardens.’<a href="https://en.wikipedia.org/wiki/Leafcutter_ant#Ant%E2%80%93fungus_mutualism">Wikipedia</a>: ‘Leaf cutter ants are sensitive enough to adapt to the fungi’s reaction to different plant material, apparently detecting chemical signals from the fungus. If a particular type of leaf is toxic to the fungus, the colony will no longer collect it…The fungi used by the higher attine ants no longer produce spores. These ants fully domesticated their fungal partner 15 million years ago, a process that took 30 million years to complete.[9] Their fungi produce nutritious and swollen hyphal tips (gongylidia) that grow in bundles called staphylae, to specifically feed the ants.’ ‘The ants in turn keep predators away from the aphids and will move them from one feeding location to another. When migrating to a new area, many colonies will take the aphids with them, to ensure a continued supply of honeydew.’
<a href="https://en.wikipedia.org/wiki/Ant#Relationships_with_other_organisms">Wikipedia</a>:’Myrmecophilous (ant-loving) caterpillars of the butterfly family Lycaenidae (e.g., blues, coppers, or hairstreaks) are herded by the ants, led to feeding areas in the daytime, and brought inside the ants’ nest at night. The caterpillars have a gland which secretes honeydew when the ants massage them.’’)</li>
  <li>Measuring hard to access distances (they measure distance as they walk with an internal pedometer)</li>
  <li>Killing plants (lemon ants <a href="https://en.wikipedia.org/wiki/Ant#Relationships_with_other_organisms">make</a> ‘devil’s gardens’ by killing all plants other than ‘lemon ant trees’ in an area)</li>
  <li>Producing and delivering nitrogen to plants (‘Isotopic labelling studies suggest that plants also obtain nitrogen from the ants.’ - <a href="https://en.wikipedia.org/wiki/Ant#Relationships_with_other_organisms">Wikipedia</a>)</li>
  <li>Get out of our houses before we are driven to expend effort killing them, and similarly for all the other places ants conflict with humans (stinging, eating crops, ..)</li>
</ol>
</li>
  
    <li>
      <h3>
        <a class="post-link" href="/2023/01/03/how-to-eat-potato-chips.html">
          How to eat potato chips while typing</a><span class="post-meta"> Jan 03, 2023</span>
      </h3><p>Chopsticks.</p>
<div class="captioned-image-container">

  <p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd25d2ed4-4fe6-47f1-aeeb-52b729f06dbc_3024x4032.png" alt="eating potato chips with chopsticks" width="250" class="center" /></p>

</div>
<a href="/2023/01/03/how-to-eat-potato-chips.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2023/01/01/pacing.html">
          Pacing: inexplicably good</a><span class="post-meta"> Jan 01, 2023</span>
      </h3><p>Pacing—walking repeatedly over the same ground—<a href="https://worldspiritsockpuppet.substack.com/p/oxford-circles-and-planes-19-10-29">often</a> feels ineffably good while I’m doing it, but then I forget about it for ages, so I thought I’d write about it here.</p>

<p>I don’t mean just going for an inefficient walk—it is somehow different to just step slowly in a circle around the same room for a long time, or up and down a passageway.</p>

<a href="/2023/01/01/pacing.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/12/30/worldly-positions-archives.html">
          Worldly Positions archive, briefly with private drafts</a><span class="post-meta"> Dec 30, 2022</span>
      </h3><p>I realized it was hard to peruse past <a href="https://worldlypositions.tumblr.com">Worldly Positions</a> posts without logging in to Tumblr, which seemed pretty bad. So I followed Substack’s instructions to import the archives into <a href="https://worldspiritsockpuppet.substack.com/">world spirit sock stack</a>. And it worked pretty well, except that SUBSTACK ALSO PUBLISHED MY UNPUBLISHED WORLDLY POSITIONS DRAFTS! What on Earth? That’s so bad. Did I misunderstand what happened somehow in my rush to unpublish them? Maybe. But they definitely had ‘unpublish’ buttons, so that’s pretty incriminating.</p>

<a href="/2022/12/30/worldly-positions-archives.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/12/29/how-to-stare-into-the-abyss.html">
          More ways to spot abysses</a><span class="post-meta"> Dec 29, 2022</span>
      </h3><p>I liked Ben Kuhn’s <a href="https://www.benkuhn.net/abyss/">‘Staring into the abyss as a core life skill’</a>.</p>

<p>I’d summarize as:</p>

<ol>
  <li>If you are making a major error—professionally, romantically, religiously, etc—it can be hard to look at that fact and correct.</li>
  <li>However it’s super important. Evidence: successful people do this well.</li>
</ol>

<p>This seems pretty plausible to me.</p>

<p>(He has a lot of concrete examples, which are probably pretty helpful for internalizing this.)</p>

<p>His suggestions for how to do better helped me a bit, but not that much, so I made up my own additional prompts for finding abysses I should consider staring into, which worked relatively well for me:</p>

<ol>
  <li>If you were currently making a big mistake, what would it be?</li>
  <li>What are some things that would be hard to acknowledge, if they were true?</li>
  <li>Looking back on this time from five years hence, what do you think you’ll wish you changed earlier?</li>
  <li>If you were forced to quit something, what do you want it to be?</li>
  <li>(Variant on 1:) If you were currently making a big mistake that would be gut-wrenching to learn was a mistake, what would it be?</li>
</ol>
</li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/12/22/lets-think-about-slowing-down-ai.html">
          Let&#39;s think about slowing down AI</a><span class="post-meta"> Dec 22, 2022</span>
      </h3><p><em>(Crossposted from AI Impacts Blog)</em></p>

<h2><strong>Averting doom by not building the doom machine</strong></h2>

<p>If you fear that someone will build a machine that will seize control of the world and annihilate humanity, then one kind of response is to try to build further machines that will seize control of the world even earlier without destroying it, forestalling the ruinous machine’s conquest. An alternative or complementary kind of response is to try to avert such machines being built at all, at least while the degree of their apocalyptic tendencies is ambiguous. </p>

<p>The latter approach seems to me  like the kind of basic and obvious thing worthy of at least consideration, and also in its favor, fits nicely in the genre ‘stuff that it isn’t that hard to imagine happening in the real world’. Yet my impression is that for people worried about extinction risk from artificial intelligence, strategies under the heading ‘actively slow down AI progress’ have historically been dismissed and ignored (though ‘don’t actively speed up AI progress’ is popular).</p>

<p>The conversation near me over the years has felt a bit like this: </p>

<blockquote class="wp-block-quote">
  <p><strong>Some people: </strong>AI might kill everyone. We should design a godlike super-AI of perfect goodness to prevent that.</p>

  <p><strong>Others: </strong>wow that sounds extremely ambitious</p>

  <p><strong>Some people: </strong>yeah but it’s very important and also we are extremely smart so idk it could work</p>

  <p>[Work on it for a decade and a half]</p>

  <p></p>

  <p><strong>Some people: </strong>ok that’s pretty hard, we give up</p>

  <p><strong>Others:</strong> oh huh shouldn’t we maybe try to stop the building of this dangerous AI? </p>

  <p><strong>Some people:</strong> hmm, that would involve coordinating numerous people—we may be arrogant enough to think that we might build a god-machine that can take over the world and remake it as a paradise, but we aren’t delusional</p>
</blockquote>

<p>This seems like an error to me. (And <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like">lately</a>, <a target="_blank" rel="noreferrer noopener" href="https://forum.effectivealtruism.org/posts/8CMuNwKMcR55jhd8W/instead-of-technical-research-more-people-should-focus-on">to</a> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KerryLVaughan/status/1536364299089854471">a</a> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/yhRTjBs6oiNcjRgcx/the-case-for-doing-something-else-if-alignment-is-doomed">bunch</a> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/scholl_adam/status/1556989092784615424">of</a> <a target="_blank" rel="noreferrer noopener" href="https://forum.effectivealtruism.org/posts/6LNvQYyNQpDQmnnux/slowing-down-ai-progress-is-an-underexplored-alignment">other</a> <a target="_blank" rel="noreferrer noopener" href="https://forum.effectivealtruism.org/posts/pJuS5iGbazDDzXwJN/the-history-epistemology-and-strategy-of-technological">people</a>.) </p>

<a href="/2022/12/22/lets-think-about-slowing-down-ai.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/10/14/ai_counterargs.html">
          Counterarguments to the basic AI risk case</a><span class="post-meta"> Oct 14, 2022</span>
      </h3><p><em>Crossposted from <a href="https://aiimpacts.org/counterarguments-to-the-basic-ai-x-risk-case/">The AI Impacts blog</a>.</em></p>

<p>This is going to be a list of holes I see in the basic argument for existential risk from superhuman AI systems<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>.</p>

<p>To start, here’s an outline of what I take to be the basic case<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup>:</p>

<blockquote>

  <p><strong>I. If superhuman AI systems are built, any given system is likely to be ‘goal-directed’</strong></p>

  <p>Reasons to expect this:</p>

  <ol><li>Goal-directed behavior is likely to be valuable, e.g. economically. </li><li>Goal-directed entities may tend to arise from machine learning training processes not intending to create them (at least via the methods that are likely to be used).</li><li>‘Coherence arguments’ may imply that systems with some goal-directedness will become more strongly goal-directed over time.</li></ol>

  <p><strong>II. If goal-directed superhuman AI systems are built, their desired outcomes will probably be about as bad as an empty universe by human lights </strong></p>

  <p>Reasons to expect this:</p>

  <ol><li>Finding useful goals that aren’t extinction-level bad appears to be hard: we don’t have a way to usefully point at human goals, and divergences from human goals seem likely to produce goals that are in intense conflict with human goals, due to a) most goals producing <a href="https://en.wikipedia.org/wiki/Instrumental_convergence">convergent incentives</a> for controlling everything, and b) value <a href="https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile">being</a> ‘fragile’, such that an entity with ‘similar’ values will generally create a future of virtually no value.</li><li>Finding goals that are extinction-level bad and temporarily useful appears to be easy: for example, advanced AI with the sole objective ‘maximize company revenue’ might profit said company for a time before gathering the influence and wherewithal to pursue the goal in ways that blatantly harm society.</li><li>Even if humanity found acceptable goals, giving a powerful AI system any specific goals appears to be hard. We don’t know of any procedure to do it, and we have theoretical reasons to expect that AI systems produced through machine learning training will generally end up with goals other than those they were trained according to. Randomly aberrant goals resulting are probably extinction-level bad for reasons described in II.1 above.</li></ol>

  <p><strong>III. If most goal-directed superhuman AI systems have bad goals, the future will very likely be bad</strong></p>

  <p>That is, a set of ill-motivated goal-directed superhuman AI systems, of a scale likely to occur, would be capable of taking control over the future from humans. This is supported by at least one of the following being true:</p>

  <ol><li><strong>Superhuman AI would destroy humanity rapidly. </strong>This may be via ultra-powerful capabilities at e.g. technology design and strategic scheming, or through gaining such powers in an ‘intelligence explosion‘ (self-improvement cycle). Either of those things may happen either through exceptional heights of intelligence being reached or through highly destructive ideas being available to minds only mildly beyond our own.</li><li><strong>Superhuman AI would gradually come to control the future via accruing power and resources.</strong> Power and resources would be more available to the AI system(s) than to humans on average, because of the AI having far greater intelligence.</li></ol>

</blockquote>

<p>Below is a list of gaps in the above, as I see it, and counterarguments. A ‘gap’ is not necessarily unfillable, and may have been filled in any of the countless writings on this topic that I haven’t read. I might even think that a given one can probably be filled. I just don’t know what goes in it.  </p>

<p>This blog post is an attempt to run various arguments by you all on the way to making pages on <a href="http://aiimpacts.org/">AI Impacts</a> about arguments for AI risk and corresponding counterarguments. At some point in that process I hope to also read others’ arguments, but this is not that day. So what you have here is a bunch of arguments that occur to me, not an exhaustive literature review. </p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>That is, systems that are somewhat more capable than the most capable human. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Based on countless conversations in the AI risk community, and various reading. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
<a href="/2022/10/14/ai_counterargs.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/10/12/calibration.html">
          Calibration of a thousand predictions</a><span class="post-meta"> Oct 12, 2022</span>
      </h3><p>I’ve been making predictions in a spreadsheet for the last four years, and I recently got to a thousand resolved predictions. Some observations:</p>

<ol>
  <li>
    <p>I’m surprisingly well calibrated for things that mostly aren’t my own behavior<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>. Here’s the calibration curve for 630 resolved predictions in that class:</p>

    <p style="text-align:center;"><img src="https://hosting.photobucket.com/images/i/katjasgrace/Calibration_for_no-special-context_forecasts_(mostly_excluding_much_own-behavior_prediction)(1).png" alt="calibration no context predictions" width="500" /></p>
  </li>
</ol>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>I have a column where I write context on some predictions, which is usually that they are my own work goal, or otherwise a prediction about how I will behave. This graph excludes those, but keeps in some own-behavior prediction which I didn’t flag for whatever reason.) <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
<a href="/2022/10/12/calibration.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/09/23/2022-game.html">
          A game of mattering</a><span class="post-meta"> Sep 23, 2022</span>
      </h3><p>When I have an overwhelming number of things to do, and insufficient native urge to do them, I often arrange them into a kind of game for myself. The nature and appeal of this game has been relatively stable for about a year, after many years of evolution, so this seems like a reasonable time to share it. I also play it when I just want to structure my day and am in the mood for it. I currently play something like two or three times a week.</p>

<h1 id="the-game">The game</h1>

<p>The basic idea is to lay out the tasks in time a bit like obstacles in a platformer or steps in Dance Dance Revolution, then race through the obstacle course grabbing them under consistently high-but-doable time pressure.</p>

<p>Here’s how to play:</p>
<ol>
  <li>Draw a grid with as many rows as there are remaining hours in your hoped for productive day, and ~3 columns. Each box stands for a particular ~20 minute period (I sometimes play with 15m or 30m periods.)</li>
  <li>Lay out the gameboard: break the stuff you want to do into appropriate units, henceforth ‘items’. An item should fit comfortably in the length of a box, and it should be easy enough to verify completion. (This can be achieved through house rules such as ‘do x a tiny bit = do it until I have a sense that an appropriate tiny bit has been done’ as long as you are happy applying them). Space items out a decent amount so that the whole course is clearly feasible. Include everything you want to do in the day, including nice or relaxing things, or break activities. Drinks, snacks, tiny bouts of exercise, looking at news sites for 5 minutes, etc. Design the track thoughtfully, with hard bouts followed by relief before the next hard bout.</li>
  <li>To play, start in the first box, then move through the boxes according to the time of day. The goal in playing is to collect as many items as you can, as you are forced along the track by the passage of time. You can collect an item by doing the task in or before you get to the box it is in. If it isn’t done by the end of the box, it gets left behind. However if you clear any box entirely, you get to move one item anywhere on the gameboard. So you can rescue something from the past, or rearrange the future to make it more feasible, or if everything is perfect, you can add an entirely new item somewhere.</li>
</ol>

<a href="/2022/09/23/2022-game.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/09/22/update-update.html">
          Update updates</a><span class="post-meta"> Sep 22, 2022</span>
      </h3><p>You can now read or subscribe to this blog via <a href="https://worldspiritsockpuppet.substack.com/">world spirit sock stack</a>, a Substack mirror of this site. I expect to see comments at <a href="https://worldspiritsockpuppet.substack.com/">wsss</a> similarly often to <a href="worldspiritsockpuppet.com/">wssp</a> (with both being more often than at various other places this crossposts, e.g. LessWrong).</p>

<p>You can also be alerted to posts on Twitter via <a href="https://twitter.com/wssockpuppet">@wssockpuppet</a>. I’m going to continue to Tweet about some subset of things on my <a href="https://twitter.com/KatjaGrace">personal account</a>, so this runs a risk of double-seeing things.</p>
</li>
  
</ul>

<!-- Pagination links -->
<div class="pagination">
  
    <a href="/blog/page2/" class="previous">
      Previous
    </a>
  
  <span class="page_number ">
    << Page: 3 of 20 >>
  </span>
  
    <a href="/blog/page4/" class="next">Next</a>
  
</div>


<!-- Feel free to add content and custom Front Matter to this file.
To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults [message that came with blog I think]

Originally I had a file index.markdown, which I replaced with this one in order to try to get pagination to work.
I also added two loops to a mostly empty page for that.-->


  &nbsp;

</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
    <!-- <div>
      <h2 class="footer-heading"><a href="/list.html">Full archive</a></h2>
    </div> -->

    <!-- <div>
      <h2 class="footer-heading">Search</h2>
<script async src="https://cse.google.com/cse.js?cx=06d4880e018d74eb0">
</script>
<div class="gcse-search"></div>
</div> -->


    <br>

    <h2 class="footer-heading"><a href="/">world spirit sock puppet</a></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <p>Inclusive writings of Katja Grace</p>
        <a href="/list.html">Full archive</a>
        <br>
        <ul class="contact-list">
          <li class="p-name">
            <!--Katja Grace-->
            </li><ul class="social-media-list"><li><a href="https://www.twitter.com/KatjaGrace"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">KatjaGrace</span></a></li></ul>
<li><a class="u-email" href="mailto:katjasolveig@gmail.com">katjasolveig@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2">
        <!--<ul class="social-media-list"><li><a href="https://www.twitter.com/KatjaGrace"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">KatjaGrace</span></a></li></ul>
-->
      </div>

      <div class="footer-col footer-col-3">
        <h4 class="footer-heading">Search</h4>
<script async src="https://cse.google.com/cse.js?cx=06d4880e018d74eb0">
</script>
<div class="gcse-search"></div>
</div>
    </div>

  </div>


</footer>
</body>

</html>

<!DOCTYPE html>
<html lang="en"><head>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Benne&display=swap" rel="stylesheet">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Page 2 of 19 for world spirit sock puppet | Inclusive writings of Katja Grace</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="world spirit sock puppet" />
<meta name="author" content="Katja Grace" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Inclusive writings of Katja Grace" />
<meta property="og:description" content="Inclusive writings of Katja Grace" />
<link rel="canonical" href="http://localhost:4000/blog/page2/" />
<meta property="og:url" content="http://localhost:4000/blog/page2/" />
<meta property="og:site_name" content="world spirit sock puppet" />
<link rel="prev" href="http://localhost:4000/" />
<link rel="next" href="http://localhost:4000/blog/page3/" />
<script type="application/ld+json">
{"@type":"WebPage","description":"Inclusive writings of Katja Grace","author":{"@type":"Person","name":"Katja Grace"},"headline":"world spirit sock puppet","url":"http://localhost:4000/blog/page2/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="world spirit sock puppet" /><!-- Twitter cards -->
  <meta name="twitter:site"    content="@KatjaGrace">
  <meta name="twitter:creator" content="@">
  <meta name="twitter:title"   content="">

  
  <meta name="twitter:description" content="Inclusive writings of Katja Grace">
  

  
  <meta name="twitter:card"  content="summary">
  <meta name="twitter:image" content="">
  
  <!-- end of Twitter cards -->
</head>
<body><div class="FeaturedImgBanner"  >
  <header class="site-header" role="banner">
    <!-- Include your post title, byline, date, and other info inside the header here. -->

    <div class="wrapper"><a class="site-title" rel="author" href="/">world spirit sock puppet</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
<br>
          <div class="trigger"><a class="page-link" href="/about/">About</a>
              <!-- <a class="page-link" href="/about/"><img src= height=20px alt=About></a> --><a class="page-link" href="/search/">Search</a>
              <!-- <a class="page-link" href="/search/"><img src= height=20px alt=Search></a> --><a class="page-link" href="/subscribe/">Subscribe</a>
              <!-- <a class="page-link" href="/subscribe/"><img src= height=20px alt=Subscribe></a> --></div>
        </nav></div>
  </header>
</div>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><a href=/index.html><b>EVERYTHING</b></a> &mdash; <a href=/worldlypositions.html>WORLDLY POSITIONS</a> &mdash; <a href=/meteuphoric.html>METEUPHORIC</a>
<br>
<br>

<!-- This loops through the paginated posts -->
<ul class="post-list">
  
    <li>
      <h3>
        <a class="post-link" href="/2023/01/03/how-to-eat-potato-chips.html">
          How to eat potato chips while typing</a><span class="post-meta"> Jan 03, 2023</span>
      </h3><p>Chopsticks.</p>
<div class="captioned-image-container">

  <p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd25d2ed4-4fe6-47f1-aeeb-52b729f06dbc_3024x4032.png" alt="eating potato chips with chopsticks" width="250" class="center" /></p>

</div>
<a href="/2023/01/03/how-to-eat-potato-chips.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/12/31/pacing.html">
          Pacing: inexplicably good</a><span class="post-meta"> Dec 31, 2022</span>
      </h3><p>Pacing—walking repeatedly over the same ground—<a href="https://worldspiritsockpuppet.substack.com/p/oxford-circles-and-planes-19-10-29">often</a> feels ineffably good while I’m doing it, but then I forget about it for ages, so I thought I’d write about it here.</p>

<p>I don’t mean just going for an inefficient walk—it is somehow different to just step slowly in a circle around the same room for a long time, or up and down a passageway.</p>

<a href="/2022/12/31/pacing.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/12/30/worldly-positions-archives.html">
          Worldly Positions archive, briefly with private drafts</a><span class="post-meta"> Dec 30, 2022</span>
      </h3><p>I realized it was hard to peruse past <a href="https://worldlypositions.tumblr.com">Worldly Positions</a> posts without logging in to Tumblr, which seemed pretty bad. So I followed Substack’s instructions to import the archives into <a href="https://worldspiritsockpuppet.substack.com/">world spirit sock stack</a>. And it worked pretty well, except that SUBSTACK ALSO PUBLISHED MY UNPUBLISHED WORLDLY POSITIONS DRAFTS! What on Earth? That’s so bad. Did I misunderstand what happened somehow in my rush to unpublish them? Maybe. But they definitely had ‘unpublish’ buttons, so that’s pretty incriminating.</p>

<a href="/2022/12/30/worldly-positions-archives.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/12/29/how-to-stare-into-the-abyss.html">
          More ways to spot abysses</a><span class="post-meta"> Dec 29, 2022</span>
      </h3><p>I liked Ben Kuhn’s <a href="https://www.benkuhn.net/abyss/">‘Staring into the abyss as a core life skill’</a>.</p>

<p>I’d summarize as:</p>

<ol>
  <li>If you are making a major error—professionally, romantically, religiously, etc—it can be hard to look at that fact and correct.</li>
  <li>However it’s super important. Evidence: successful people do this well.</li>
</ol>

<p>This seems pretty plausible to me.</p>

<p>(He has a lot of concrete examples, which are probably pretty helpful for internalizing this.)</p>

<p>His suggestions for how to do better helped me a bit, but not that much, so I made up my own additional prompts for finding abysses I should consider staring into, which worked relatively well for me:</p>

<ol>
  <li>If you were currently making a big mistake, what would it be?</li>
  <li>What are some things that would be hard to acknowledge, if they were true?</li>
  <li>Looking back on this time from five years hence, what do you think you’ll wish you changed earlier?</li>
  <li>If you were forced to quit something, what do you want it to be?</li>
  <li>(Variant on 1:) If you were currently making a big mistake that would be gut-wrenching to learn was a mistake, what would it be?</li>
</ol>
</li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/12/22/lets-think-about-slowing-down-ai.html">
          Let&#39;s think about slowing down AI</a><span class="post-meta"> Dec 22, 2022</span>
      </h3><p><em>(Crossposted from AI Impacts Blog)</em></p>

<h2><strong>Averting doom by not building the doom machine</strong></h2>

<p>If you fear that someone will build a machine that will seize control of the world and annihilate humanity, then one kind of response is to try to build further machines that will seize control of the world even earlier without destroying it, forestalling the ruinous machine’s conquest. An alternative or complementary kind of response is to try to avert such machines being built at all, at least while the degree of their apocalyptic tendencies is ambiguous. </p>

<p>The latter approach seems to me  like the kind of basic and obvious thing worthy of at least consideration, and also in its favor, fits nicely in the genre ‘stuff that it isn’t that hard to imagine happening in the real world’. Yet my impression is that for people worried about extinction risk from artificial intelligence, strategies under the heading ‘actively slow down AI progress’ have historically been dismissed and ignored (though ‘don’t actively speed up AI progress’ is popular).</p>

<p>The conversation near me over the years has felt a bit like this: </p>

<blockquote class="wp-block-quote">
  <p><strong>Some people: </strong>AI might kill everyone. We should design a godlike super-AI of perfect goodness to prevent that.</p>

  <p><strong>Others: </strong>wow that sounds extremely ambitious</p>

  <p><strong>Some people: </strong>yeah but it’s very important and also we are extremely smart so idk it could work</p>

  <p>[Work on it for a decade and a half]</p>

  <p></p>

  <p><strong>Some people: </strong>ok that’s pretty hard, we give up</p>

  <p><strong>Others:</strong> oh huh shouldn’t we maybe try to stop the building of this dangerous AI? </p>

  <p><strong>Some people:</strong> hmm, that would involve coordinating numerous people—we may be arrogant enough to think that we might build a god-machine that can take over the world and remake it as a paradise, but we aren’t delusional</p>
</blockquote>

<p>This seems like an error to me. (And <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like">lately</a>, <a target="_blank" rel="noreferrer noopener" href="https://forum.effectivealtruism.org/posts/8CMuNwKMcR55jhd8W/instead-of-technical-research-more-people-should-focus-on">to</a> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KerryLVaughan/status/1536364299089854471">a</a> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/yhRTjBs6oiNcjRgcx/the-case-for-doing-something-else-if-alignment-is-doomed">bunch</a> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/scholl_adam/status/1556989092784615424">of</a> <a target="_blank" rel="noreferrer noopener" href="https://forum.effectivealtruism.org/posts/6LNvQYyNQpDQmnnux/slowing-down-ai-progress-is-an-underexplored-alignment">other</a> <a target="_blank" rel="noreferrer noopener" href="https://forum.effectivealtruism.org/posts/pJuS5iGbazDDzXwJN/the-history-epistemology-and-strategy-of-technological">people</a>.) </p>

<a href="/2022/12/22/lets-think-about-slowing-down-ai.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/10/14/ai_counterargs.html">
          Counterarguments to the basic AI risk case</a><span class="post-meta"> Oct 14, 2022</span>
      </h3><p><em>Crossposted from <a href="https://aiimpacts.org/counterarguments-to-the-basic-ai-x-risk-case/">The AI Impacts blog</a>.</em></p>

<p>This is going to be a list of holes I see in the basic argument for existential risk from superhuman AI systems<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>.</p>

<p>To start, here’s an outline of what I take to be the basic case<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup>:</p>

<blockquote>

  <p><strong>I. If superhuman AI systems are built, any given system is likely to be ‘goal-directed’</strong></p>

  <p>Reasons to expect this:</p>

  <ol><li>Goal-directed behavior is likely to be valuable, e.g. economically. </li><li>Goal-directed entities may tend to arise from machine learning training processes not intending to create them (at least via the methods that are likely to be used).</li><li>‘Coherence arguments’ may imply that systems with some goal-directedness will become more strongly goal-directed over time.</li></ol>

  <p><strong>II. If goal-directed superhuman AI systems are built, their desired outcomes will probably be about as bad as an empty universe by human lights </strong></p>

  <p>Reasons to expect this:</p>

  <ol><li>Finding useful goals that aren’t extinction-level bad appears to be hard: we don’t have a way to usefully point at human goals, and divergences from human goals seem likely to produce goals that are in intense conflict with human goals, due to a) most goals producing <a href="https://en.wikipedia.org/wiki/Instrumental_convergence">convergent incentives</a> for controlling everything, and b) value <a href="https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile">being</a> ‘fragile’, such that an entity with ‘similar’ values will generally create a future of virtually no value.</li><li>Finding goals that are extinction-level bad and temporarily useful appears to be easy: for example, advanced AI with the sole objective ‘maximize company revenue’ might profit said company for a time before gathering the influence and wherewithal to pursue the goal in ways that blatantly harm society.</li><li>Even if humanity found acceptable goals, giving a powerful AI system any specific goals appears to be hard. We don’t know of any procedure to do it, and we have theoretical reasons to expect that AI systems produced through machine learning training will generally end up with goals other than those they were trained according to. Randomly aberrant goals resulting are probably extinction-level bad for reasons described in II.1 above.</li></ol>

  <p><strong>III. If most goal-directed superhuman AI systems have bad goals, the future will very likely be bad</strong></p>

  <p>That is, a set of ill-motivated goal-directed superhuman AI systems, of a scale likely to occur, would be capable of taking control over the future from humans. This is supported by at least one of the following being true:</p>

  <ol><li><strong>Superhuman AI would destroy humanity rapidly. </strong>This may be via ultra-powerful capabilities at e.g. technology design and strategic scheming, or through gaining such powers in an ‘intelligence explosion‘ (self-improvement cycle). Either of those things may happen either through exceptional heights of intelligence being reached or through highly destructive ideas being available to minds only mildly beyond our own.</li><li><strong>Superhuman AI would gradually come to control the future via accruing power and resources.</strong> Power and resources would be more available to the AI system(s) than to humans on average, because of the AI having far greater intelligence.</li></ol>

</blockquote>

<p>Below is a list of gaps in the above, as I see it, and counterarguments. A ‘gap’ is not necessarily unfillable, and may have been filled in any of the countless writings on this topic that I haven’t read. I might even think that a given one can probably be filled. I just don’t know what goes in it.  </p>

<p>This blog post is an attempt to run various arguments by you all on the way to making pages on <a href="http://aiimpacts.org/">AI Impacts</a> about arguments for AI risk and corresponding counterarguments. At some point in that process I hope to also read others’ arguments, but this is not that day. So what you have here is a bunch of arguments that occur to me, not an exhaustive literature review. </p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>That is, systems that are somewhat more capable than the most capable human. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Based on countless conversations in the AI risk community, and various reading. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
<a href="/2022/10/14/ai_counterargs.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/10/12/calibration.html">
          Calibration of a thousand predictions</a><span class="post-meta"> Oct 12, 2022</span>
      </h3><p>I’ve been making predictions in a spreadsheet for the last four years, and I recently got to a thousand resolved predictions. Some observations:</p>

<ol>
  <li>
    <p>I’m surprisingly well calibrated for things that mostly aren’t my own behavior<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>. Here’s the calibration curve for 630 resolved predictions in that class:</p>

    <p style="text-align:center;"><img src="https://hosting.photobucket.com/images/i/katjasgrace/Calibration_for_no-special-context_forecasts_(mostly_excluding_much_own-behavior_prediction)(1).png" alt="calibration no context predictions" width="500" /></p>
  </li>
</ol>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>I have a column where I write context on some predictions, which is usually that they are my own work goal, or otherwise a prediction about how I will behave. This graph excludes those, but keeps in some own-behavior prediction which I didn’t flag for whatever reason.) <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
<a href="/2022/10/12/calibration.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/09/22/2022-game.html">
          A game of mattering</a><span class="post-meta"> Sep 22, 2022</span>
      </h3><p>When I have an overwhelming number of things to do, and insufficient native urge to do them, I often arrange them into a kind of game for myself. The nature and appeal of this game has been relatively stable for about a year, after many years of evolution, so this seems like a reasonable time to share it. I also play it when I just want to structure my day and am in the mood for it. I currently play something like two or three times a week.</p>

<h1 id="the-game">The game</h1>

<p>The basic idea is to lay out the tasks in time a bit like obstacles in a platformer or steps in Dance Dance Revolution, then race through the obstacle course grabbing them under consistently high-but-doable time pressure.</p>

<p>Here’s how to play:</p>
<ol>
  <li>Draw a grid with as many rows as there are remaining hours in your hoped for productive day, and ~3 columns. Each box stands for a particular ~20 minute period (I sometimes play with 15m or 30m periods.)</li>
  <li>Lay out the gameboard: break the stuff you want to do into appropriate units, henceforth ‘items’. An item should fit comfortably in the length of a box, and it should be easy enough to verify completion. (This can be achieved through house rules such as ‘do x a tiny bit = do it until I have a sense that an appropriate tiny bit has been done’ as long as you are happy applying them). Space items out a decent amount so that the whole course is clearly feasible. Include everything you want to do in the day, including nice or relaxing things, or break activities. Drinks, snacks, tiny bouts of exercise, looking at news sites for 5 minutes, etc. Design the track thoughtfully, with hard bouts followed by relief before the next hard bout.</li>
  <li>To play, start in the first box, then move through the boxes according to the time of day. The goal in playing is to collect as many items as you can, as you are forced along the track by the passage of time. You can collect an item by doing the task in or before you get to the box it is in. If it isn’t done by the end of the box, it gets left behind. However if you clear any box entirely, you get to move one item anywhere on the gameboard. So you can rescue something from the past, or rearrange the future to make it more feasible, or if everything is perfect, you can add an entirely new item somewhere.</li>
</ol>

<a href="/2022/09/22/2022-game.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/09/21/update-update.html">
          Update updates</a><span class="post-meta"> Sep 21, 2022</span>
      </h3><p>You can now read or subscribe to this blog via <a href="https://worldspiritsockpuppet.substack.com/">world spirit sock stack</a>, a Substack mirror of this site. I expect to see comments at <a href="https://worldspiritsockpuppet.substack.com/">wsss</a> similarly often to <a href="worldspiritsockpuppet.com/">wssp</a> (with both being more often than at various other places this crossposts, e.g. LessWrong).</p>

<p>You can also be alerted to posts on Twitter via <a href="https://twitter.com/wssockpuppet">@wssockpuppet</a>. I’m going to continue to Tweet about some subset of things on my <a href="https://twitter.com/KatjaGrace">personal account</a>, so this runs a risk of double-seeing things.</p>
</li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/09/17/im-on-podcasts.html">
          Podcasts on surveys, slower AI, AI arguments</a><span class="post-meta"> Sep 17, 2022</span>
      </h3><p>I recently talked to Michael Trazzi for his podcast, The Inside View. It just came out, so if that’s a conversation you want to sit in on, do so <a href="https://www.youtube.com/watch?v=rSw3UVDZge0">here</a> [ETA: or read it <a href="https://theinsideview.ai/katja">here</a>].</p>

<p>The main topics were the <a href="https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/">survey of ML folk</a> I recently ran, and my thoughts on moving more slowly on potentially world-threatening AI research (which is to say, AI research in general, <a href="https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/">according to</a> the median surveyed ML researcher…). I also bet him a thousand dollars to his hundred that AI would not make blogging way more efficient in two years, if I recall. (I forget the exact terms, and there’s no way I’m listening to myself talk for that long to find out. If anyone else learns, I’m curious what I agreed to.)</p>

<p>For completeness of podcast reporting: I forgot to mention that <a href="https://axrp.net/episode/2021/07/23/episode-10-ais-future-and-dangers-katja-grace.html">I also talked to Daniel Filan on AXRP</a>, like a year ago. In other old news, I am opposed to the vibe of time-sensitivity often implicit in the public conversation.</p>
</li>
  
</ul>

<!-- Pagination links -->
<div class="pagination">
  
    <a href="/" class="previous">
      Previous
    </a>
  
  <span class="page_number ">
    << Page: 2 of 19 >>
  </span>
  
    <a href="/blog/page3/" class="next">Next</a>
  
</div>


<!-- Feel free to add content and custom Front Matter to this file.
To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults [message that came with blog I think]

Originally I had a file index.markdown, which I replaced with this one in order to try to get pagination to work.
I also added two loops to a mostly empty page for that.-->


  &nbsp;

</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
    <!-- <div>
      <h2 class="footer-heading"><a href="/list.html">Full archive</a></h2>
    </div> -->

    <!-- <div>
      <h2 class="footer-heading">Search</h2>
<script async src="https://cse.google.com/cse.js?cx=06d4880e018d74eb0">
</script>
<div class="gcse-search"></div>
</div> -->


    <br>

    <h2 class="footer-heading"><a href="/">world spirit sock puppet</a></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <p>Inclusive writings of Katja Grace</p>
        <a href="/list.html">Full archive</a>
        <br>
        <ul class="contact-list">
          <li class="p-name">
            <!--Katja Grace-->
            </li><ul class="social-media-list"><li><a href="https://www.twitter.com/KatjaGrace"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">KatjaGrace</span></a></li></ul>
<li><a class="u-email" href="mailto:katjasolveig@gmail.com">katjasolveig@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2">
        <!--<ul class="social-media-list"><li><a href="https://www.twitter.com/KatjaGrace"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">KatjaGrace</span></a></li></ul>
-->
      </div>

      <div class="footer-col footer-col-3">
        <h4 class="footer-heading">Search</h4>
<script async src="https://cse.google.com/cse.js?cx=06d4880e018d74eb0">
</script>
<div class="gcse-search"></div>
</div>
    </div>

  </div>


</footer>
</body>

</html>

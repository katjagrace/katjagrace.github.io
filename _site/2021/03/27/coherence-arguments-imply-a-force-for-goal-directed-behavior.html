<!DOCTYPE html>
<html lang="en"><head>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Benne&display=swap" rel="stylesheet">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Coherence arguments imply a force for goal-directed behavior | world spirit sock puppet</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Coherence arguments imply a force for goal-directed behavior" />
<meta name="author" content="Katja Grace" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Crossposted from AI Impacts [Epistemic status: my current view, but I haven’t read all the stuff on this topic even in the LessWrong community, let alone more broadly.] There is a line of thought that says that advanced AI will tend to be ‘goal-directed’—that is, consistently doing whatever makes certain favored outcomes more likely—and that this is to do with the ‘coherence arguments’. Rohin Shah, and probably others1 , have argued against this. I want to argue against them. The old argument for coherence implying (worrisome) goal-directedness I’d reconstruct the original argument that Rohin is arguing against as something like this (making no claim about my own beliefs here): ‘Whatever things you care about, you are best off assigning consistent numerical values to them and maximizing the expected sum of those values’ ‘Coherence arguments’2 mean that if you don’t maximize ‘expected utility’ (EU)—that is, if you don’t make every choice in accordance with what gets the highest average score, given consistent preferability scores that you assign to all outcomes—then you will make strictly worse choices by your own lights than if you followed some alternate EU-maximizing strategy (at least in some situations, though they may not arise). For instance, you’ll be vulnerable to ‘money-pumping’—being predictably parted from your money for nothing.3 ‘Advanced AI will tend to do better things instead of worse things, by its own lights’ Advanced AI will tend to avoid options that are predictably strictly worse by its own lights, due to being highly optimized for making good choices (by some combination of external processes that produced it, its own efforts, and the selection pressure acting on its existence). ‘Therefore advanced AI will maximize EU, roughly’ Advanced AI will tend to be fairly coherent, at least to a level of approximation where becoming more coherent isn’t worth the cost.4 Which will probably be fairly coherent (e.g. close enough to coherent that humans can’t anticipate the inconsistencies). ‘Maximizing EU is pretty much the same as being goal-directed’To maximize expected utility is to pursue the goal of that which you have assigned higher utility to.5 And since the point of all this is to argue that advanced AI might be hard to deal with, note that we can get to that conclusion with: For instance, Richard Ngo agrees here, and Eric Drexler makes a related argument here, section 6.4. &#8617; Something something This has more on these arguments. &#8617; I haven’t read all of this, and don’t yet see watertight versions of these arguments, but this is not the time I’m going to get into that. &#8617; Assuming being ‘more coherent’ is meaningful and better than being ‘less coherent’, granting that one is not coherent, which sounds plausible, but which I haven’t got into. One argument against is that if you are incoherent at all, then it looks to me like you can logically evaluate any bundle of things at any price. Which would seem to make all incoherences identical—much like how all logical contradictions equivalently lead to every belief. However this seems unlikely to predict well how creatures behave in practice if they have an incoherent preferences. &#8617; This isn’t quite right, since ‘goal’ suggests one outcome that is being pursued ahead of all others, whereas EU-maximizing implies that all possible outcomes have an ordering, and you care about getting higher ones in general, not just the top one above all others, but this doesn’t seem like a particularly relevant distinction here. &#8617;" />
<meta property="og:description" content="Crossposted from AI Impacts [Epistemic status: my current view, but I haven’t read all the stuff on this topic even in the LessWrong community, let alone more broadly.] There is a line of thought that says that advanced AI will tend to be ‘goal-directed’—that is, consistently doing whatever makes certain favored outcomes more likely—and that this is to do with the ‘coherence arguments’. Rohin Shah, and probably others1 , have argued against this. I want to argue against them. The old argument for coherence implying (worrisome) goal-directedness I’d reconstruct the original argument that Rohin is arguing against as something like this (making no claim about my own beliefs here): ‘Whatever things you care about, you are best off assigning consistent numerical values to them and maximizing the expected sum of those values’ ‘Coherence arguments’2 mean that if you don’t maximize ‘expected utility’ (EU)—that is, if you don’t make every choice in accordance with what gets the highest average score, given consistent preferability scores that you assign to all outcomes—then you will make strictly worse choices by your own lights than if you followed some alternate EU-maximizing strategy (at least in some situations, though they may not arise). For instance, you’ll be vulnerable to ‘money-pumping’—being predictably parted from your money for nothing.3 ‘Advanced AI will tend to do better things instead of worse things, by its own lights’ Advanced AI will tend to avoid options that are predictably strictly worse by its own lights, due to being highly optimized for making good choices (by some combination of external processes that produced it, its own efforts, and the selection pressure acting on its existence). ‘Therefore advanced AI will maximize EU, roughly’ Advanced AI will tend to be fairly coherent, at least to a level of approximation where becoming more coherent isn’t worth the cost.4 Which will probably be fairly coherent (e.g. close enough to coherent that humans can’t anticipate the inconsistencies). ‘Maximizing EU is pretty much the same as being goal-directed’To maximize expected utility is to pursue the goal of that which you have assigned higher utility to.5 And since the point of all this is to argue that advanced AI might be hard to deal with, note that we can get to that conclusion with: For instance, Richard Ngo agrees here, and Eric Drexler makes a related argument here, section 6.4. &#8617; Something something This has more on these arguments. &#8617; I haven’t read all of this, and don’t yet see watertight versions of these arguments, but this is not the time I’m going to get into that. &#8617; Assuming being ‘more coherent’ is meaningful and better than being ‘less coherent’, granting that one is not coherent, which sounds plausible, but which I haven’t got into. One argument against is that if you are incoherent at all, then it looks to me like you can logically evaluate any bundle of things at any price. Which would seem to make all incoherences identical—much like how all logical contradictions equivalently lead to every belief. However this seems unlikely to predict well how creatures behave in practice if they have an incoherent preferences. &#8617; This isn’t quite right, since ‘goal’ suggests one outcome that is being pursued ahead of all others, whereas EU-maximizing implies that all possible outcomes have an ordering, and you care about getting higher ones in general, not just the top one above all others, but this doesn’t seem like a particularly relevant distinction here. &#8617;" />
<link rel="canonical" href="http://localhost:4000/2021/03/27/coherence-arguments-imply-a-force-for-goal-directed-behavior.html" />
<meta property="og:url" content="http://localhost:4000/2021/03/27/coherence-arguments-imply-a-force-for-goal-directed-behavior.html" />
<meta property="og:site_name" content="world spirit sock puppet" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-27T04:20:00+00:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2021/03/27/coherence-arguments-imply-a-force-for-goal-directed-behavior.html"},"description":"Crossposted from AI Impacts [Epistemic status: my current view, but I haven’t read all the stuff on this topic even in the LessWrong community, let alone more broadly.] There is a line of thought that says that advanced AI will tend to be ‘goal-directed’—that is, consistently doing whatever makes certain favored outcomes more likely—and that this is to do with the ‘coherence arguments’. Rohin Shah, and probably others1 , have argued against this. I want to argue against them. The old argument for coherence implying (worrisome) goal-directedness I’d reconstruct the original argument that Rohin is arguing against as something like this (making no claim about my own beliefs here): ‘Whatever things you care about, you are best off assigning consistent numerical values to them and maximizing the expected sum of those values’ ‘Coherence arguments’2 mean that if you don’t maximize ‘expected utility’ (EU)—that is, if you don’t make every choice in accordance with what gets the highest average score, given consistent preferability scores that you assign to all outcomes—then you will make strictly worse choices by your own lights than if you followed some alternate EU-maximizing strategy (at least in some situations, though they may not arise). For instance, you’ll be vulnerable to ‘money-pumping’—being predictably parted from your money for nothing.3 ‘Advanced AI will tend to do better things instead of worse things, by its own lights’ Advanced AI will tend to avoid options that are predictably strictly worse by its own lights, due to being highly optimized for making good choices (by some combination of external processes that produced it, its own efforts, and the selection pressure acting on its existence). ‘Therefore advanced AI will maximize EU, roughly’ Advanced AI will tend to be fairly coherent, at least to a level of approximation where becoming more coherent isn’t worth the cost.4 Which will probably be fairly coherent (e.g. close enough to coherent that humans can’t anticipate the inconsistencies). ‘Maximizing EU is pretty much the same as being goal-directed’To maximize expected utility is to pursue the goal of that which you have assigned higher utility to.5 And since the point of all this is to argue that advanced AI might be hard to deal with, note that we can get to that conclusion with: For instance, Richard Ngo agrees here, and Eric Drexler makes a related argument here, section 6.4. &#8617; Something something This has more on these arguments. &#8617; I haven’t read all of this, and don’t yet see watertight versions of these arguments, but this is not the time I’m going to get into that. &#8617; Assuming being ‘more coherent’ is meaningful and better than being ‘less coherent’, granting that one is not coherent, which sounds plausible, but which I haven’t got into. One argument against is that if you are incoherent at all, then it looks to me like you can logically evaluate any bundle of things at any price. Which would seem to make all incoherences identical—much like how all logical contradictions equivalently lead to every belief. However this seems unlikely to predict well how creatures behave in practice if they have an incoherent preferences. &#8617; This isn’t quite right, since ‘goal’ suggests one outcome that is being pursued ahead of all others, whereas EU-maximizing implies that all possible outcomes have an ordering, and you care about getting higher ones in general, not just the top one above all others, but this doesn’t seem like a particularly relevant distinction here. &#8617;","author":{"@type":"Person","name":"Katja Grace"},"headline":"Coherence arguments imply a force for goal-directed behavior","dateModified":"2021-03-27T04:20:00+00:00","url":"http://localhost:4000/2021/03/27/coherence-arguments-imply-a-force-for-goal-directed-behavior.html","datePublished":"2021-03-27T04:20:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="world spirit sock puppet" /><!-- Twitter cards -->
  <meta name="twitter:site"    content="@KatjaGrace">
  <meta name="twitter:creator" content="@">
  <meta name="twitter:title"   content="Coherence arguments imply a force for goal-directed behavior">

  
  <meta name="twitter:description" content="Inclusive writings of Katja Grace">
  

  
  <meta name="twitter:card"  content="summary">
  <meta name="twitter:image" content="">
  
  <!-- end of Twitter cards -->
</head>
<body><div class="FeaturedImgBanner"  >
  <header class="site-header" role="banner">
    <!-- Include your post title, byline, date, and other info inside the header here. -->

    <div class="wrapper"><a class="site-title" rel="author" href="/">world spirit sock puppet</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
<br>
          <div class="trigger"><a class="page-link" href="/2024-04-18-experiment-on-repeating-choices.html">Experiment on repeating choices</a>
              <!-- <a class="page-link" href="/2024-04-18-experiment-on-repeating-choices.html"><img src= height=20px alt=Experiment on repeating choices></a> --><a class="page-link" href="/about/">About</a>
              <!-- <a class="page-link" href="/about/"><img src= height=20px alt=About></a> --><a class="page-link" href="/search/">Search</a>
              <!-- <a class="page-link" href="/search/"><img src= height=20px alt=Search></a> --><a class="page-link" href="/subscribe/">Subscribe</a>
              <!-- <a class="page-link" href="/subscribe/"><img src= height=20px alt=Subscribe></a> --></div>
        </nav></div>
  </header>
</div>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <br>
    <br>
    <h1 class="post-title p-name" itemprop="name headline">Coherence arguments imply a force for goal-directed behavior</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2021-03-27T04:20:00+00:00" itemprop="datePublished">Mar 27, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><em><em>Crossposted from <a href="https://aiimpacts.org/">AI Impacts</a></em></em></p>

<p><em>[Epistemic status: my current view, but I haven’t read all the stuff on this topic even in the LessWrong community, let alone more broadly.]</em></p>

<p>There is a line of thought that says that advanced AI will tend to be ‘goal-directed’—that is, consistently doing whatever makes certain favored outcomes more likely—and that this is to do with the ‘coherence arguments’. <a href="https://www.lesswrong.com/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-imply-goal-directed-behavior">Rohin Shah</a>, and probably others<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> , have argued against this. I want to argue against them.</p>

<!-- wp:heading -->
<h2>The old argument for coherence implying (worrisome) goal-directedness</h2>
<!-- /wp:heading -->

<p>I’d reconstruct the original argument that Rohin is arguing against as something like this (making no claim about my own beliefs here):</p>

<ul>
  <li><strong>‘Whatever things you care about, you are best off assigning consistent numerical values to them and maximizing the expected sum of those values’</strong> <br /> ‘Coherence arguments’<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup> mean that if you don’t maximize ‘expected utility’ (EU)—that is, if you don’t make every choice in accordance with what gets the highest average score, given consistent preferability scores that you assign to all outcomes—then you will make strictly worse choices by your own lights than if you followed some alternate EU-maximizing strategy (at least in some situations, though they may not arise). For instance, you’ll be vulnerable to ‘<a href="https://www.oxfordreference.com/view/10.1093/oi/authority.20110803100205601">money-pumping</a>’—being predictably parted from your money for nothing.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">3</a></sup></li>
  <li><strong>‘Advanced AI will tend to do better things instead of worse things, by its own lights’</strong> <br />Advanced AI will tend to avoid options that are predictably strictly worse by its own lights, due to being highly optimized for making good choices (by some combination of external processes that produced it, its own efforts, and the selection pressure acting on its existence).</li>
  <li><strong>‘Therefore advanced AI will maximize EU, roughly’</strong> <br />Advanced AI will tend to be fairly coherent, at least to a level of approximation where becoming more coherent isn’t worth the cost.<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote">4</a></sup> Which will probably be fairly coherent (e.g. close enough to coherent that <a href="https://arbital.com/p/optimized_agent_appears_coherent/">humans can’t anticipate the inconsistencies</a>).</li>
  <li><strong>‘Maximizing EU is pretty much the same as being goal-directed’</strong><br />To maximize expected utility is to pursue the goal of that which you have assigned higher utility to.<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote">5</a></sup></li>
</ul>

<p>And since the point of all this is to argue that advanced AI might be hard to deal with, note that we can get to that conclusion with:
<!--ex--></p>

<!-- wp:list {"ordered":true,"start":5} -->
<ol start="5"><li><strong>‘Highly intelligent goal-directed agents are dangerous’</strong><strong><br /></strong>If AI systems exist that very competently pursue goals, they will likely be better than us at attaining their goals, and therefore to the extent there is a risk of mismatch between their goals and ours, we face a serious risk.</li></ol>
<!-- /wp:list -->

<!-- wp:heading -->
<h2>Rohin’s counterargument</h2>
<!-- /wp:heading -->

<p>Rohin’s counterargument begins with an observation made by others before: any behavior is consistent with maximizing expected utility, given <em>some</em> utility function. For instance, a creature just twitching around on the ground may have the utility function that returns 1 if the agent does whatever it in fact does in each situation (where ‘situation’ means, ‘entire history of the world so far’), and 0 otherwise. This is a creature that just wants to make the right twitch in each detailed, history-indexed situation, with no regard for further consequences. Alternately the twitching agent might care about outcomes, but just happen to want the particular holistic unfolding of the universe that is occurring, including this particular series of twitches. Or it could be indifferent between all outcomes.</p>

<p>The basic point is that rationality doesn’t say what ‘things’ you can want. And in particular, it doesn’t say that you have to care about particular atomic units that larger situations can be broken down into. If I try to call you out for first spending money to get to Paris, then spending money to get back from Paris, there is nothing to say you can’t just have wanted to go to Paris for a bit and then to come home. In fact, this is a common human situation. ‘Aha, I money pumped you!’ says the airline, but you aren’t worried. The twitching agent might always be like this—a creature of more refined tastes, who cares about whole delicate histories and relationships, rather than just summing up modular momentarily-defined successes. And given this freedom, any behavior might conceivably be what a creature wants. </p>

<p>Then I would put the full argument, as I understand it, like this:</p>

<!-- wp:list {"ordered":true} -->
<ol><li>Any observable sequence of behavior is consistent with the entity doing EU maximization (see observation above)</li><li>Doing EU maximization doesn’t imply anything about what behavior we might observe (from 1)</li><li>In particular, knowing that a creature is an EU maximizer doesn’t imply that it will behave in a ‘goal-directed’ way, assuming that <em>that</em> concept doesn’t apply to all behavior. (from 2)</li></ol>
<!-- /wp:list -->

<p>Is this just some disagreement about the meaning of the word ‘goal-directed’? No, because we can get back to a major difference in physical expectations by adding:</p>

<!-- wp:list {"ordered":true,"start":4} -->
<ol start="4"><li>&nbsp;Not all behavior in a creature implicates dire risk to humanity, so any concept of goal-directedness that is consistent with any behavior—and so might be implied by the coherence arguments—cannot imply AI risk.</li></ol>
<!-- /wp:list -->

<p>So where the original argument says that the coherence arguments plus some other assumptions imply danger from AI, this counterargument says that they do not. </p>

<p>(There is also at least some variety in the meaning of ‘goal-directed’. I’ll use goal-directed<sub>Rohin</sub> to refer to what I think is Rohin’s <a href="https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma">preferred</a> usage: roughly, that which seems intuitively goal directed to us, e.g. behaving similarly across situations, and accruing resources, and not flopping around in possible pursuit of some exact history of personal floppage, or peaceably preferring to always take the option labeled ‘A’.<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote">6</a></sup>)</p>

<!-- wp:heading -->
<h2>My counter-counterarguments</h2>
<!-- /wp:heading -->

<p>What’s wrong with Rohin’s counterargument? It sounded tight. </p>

<p>In brief, I see two problems:</p>

<blockquote>
  <p>A. The whole argument is in terms of logical implication. But what seems to matter is changes in probability. Coherence doesn’t need to rule out any behavior to matter, it just has to change the probabilities of behaviors. Understood in terms of probability, argument 2 is a false inference: just because any sequence of behavior is consistent with EU maximization doesn’t mean that EU maximization says nothing about what behavior we will see, probabilistically. All it says is that the probability of a behavioral sequence is never reduced to zero by considerations of coherence alone, which is hardly saying anything.</p>
</blockquote>

<p>You might then think that a probabilistic version still applies: since every entity appears to be in good standing with the coherence arguments, the arguments don’t exert any force, probabilistically, on what entities we might see. But:</p>

<blockquote>
  <p>B. An outside observer being able to rationalize a sequence of observed behavior as coherent doesn’t mean that the behavior is actually coherent. Coherence arguments constrain combinations of external behavior and internal features—‘preferences’<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote">7</a></sup> and beliefs. So whether an actor is coherent depends on what preferences and beliefs it actually has. And if it isn’t coherent in light of these, then coherence pressures will apply, whether or not its behavior <em>looks</em> coherent. And in many cases, revision of preferences due to coherence pressures will end up affecting external behavior. So 2) is not only not a sound inference from 1), but actually a wrong conclusion: if a system moves toward EU maximization, that does imply things about the behavior that we will observe (probabilistically).</p>
</blockquote>

<p>Perhaps Rohin only meant to argue about whether it is <em>logically possible</em> to be coherent and not goal-directed-seeming, for the purpose of arguing that humanity can construct creatures in that perhaps-unlikely-in-nature corner of mindspace, if we try hard. In which case, I agree that it is logically possible. But I think his argument is often taken to be relevant more broadly, to questions of whether advanced AI will tend to be goal-directed, or to be goal-directed in places where they were not intended to be.</p>

<p>I take A) to be fairly clear. I’ll lay out B) in more detail.</p>

<!-- wp:heading -->
<h2>My counter-counterarguments in more detail</h2>
<!-- /wp:heading -->

<!-- wp:heading {"level":3} -->
<h3><strong>How might coherence arguments affect creatures?</strong></h3>
<!-- /wp:heading -->

<p>Let us step back.</p>

<p>How would coherence arguments affect an AI system—or anyone—anyway? They’re not going to fly in from the platonic realm and reshape irrational creatures.</p>

<p>The main routes, as I see it, are via implying:</p>

<!-- wp:list {"ordered":true} -->
<ol><li>incentives for the agent itself to reform incoherent preferences</li><li>incentives for the processes giving rise to the agent (explicit design, or selection procedures directed at success) to make them more coherent</li><li>some advantage for coherent agents in competition with incoherent agents</li></ol>
<!-- /wp:list -->

<p>To be clear, the agent, the makers, or the world are not necessarily thinking about the arguments here—the arguments correspond to incentives in the world, which these parties are responding to. So I’ll often talk about ‘incentives for coherence’ or ‘forces for coherence’ rather than ‘coherence arguments’.</p>

<p>I’ll talk more about 1 for simplicity, expecting 2 and 3 to be similar, though I haven’t thought them through.</p>

<!-- wp:heading {"level":3} -->
<h3><strong><em>Looking</em></strong><strong> coherent isn’t enough: if you aren’t coherent inside, coherence forces apply</strong></h3>
<!-- /wp:heading -->

<p>If self-adjustment is the mechanism for the coherence, this doesn’t depend on what a sequence of actions looks like from the outside, but from what it looks like from the inside.</p>

<p>Consider the aforementioned creature just twitching sporadically on the ground. Let’s call it Alex.</p>

<p>As noted earlier, there is a utility function under which Alex is maximizing expected utility: the one that assigns utility 1 to however Alex in fact acts in every specific history, and utility 0 to anything else.</p>

<p>But from the inside, this creature you excuse as ‘maybe just wanting that series of twitches’ has—let us suppose—actual preferences and beliefs. And if its preferences do not in fact prioritize this elaborate sequence of twitching in an unconflicted way, and it has the self-awareness and means to make corrections, then it will make corrections<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote">8</a></sup>. And having done so, its behavior will change. </p>

<p>Thus excusable-as-coherent Alex is still moved by coherence arguments, even while the arguments have no complaints about its behavior <em>per se</em>.</p>

<p>For a more realistic example: suppose Assistant-Bot is observed making this sequence of actions: </p>

<!-- wp:list -->
<ul><li>Offers to buy gym membership for $5/week&nbsp;</li><li>Consents to upgrade to gym-pro membership for $7/week, which is like gym membership but with added morning classes</li><li>Takes discounted ‘off-time’ deal, saving $1 per week for only using gym in evenings</li></ul>
<!-- /wp:list -->

<p>This is consistent with coherence: Assistant-Bot might prefer that exact sequence of actions over all others, or might prefer incurring gym costs with a larger sum of prime factors, or might prefer talking to Gym-sales-bot over ending the conversation, or prefer agreeing to things.</p>

<p>But suppose that <em>in fact</em>, in terms of the structure of the internal motivations producing this behavior, Assistant-Bot just prefers you to have a gym membership, and prefers you to have a better membership, and prefers you to have money, but is treating these preferences with inconsistent levels of strength in the different comparisons. Then there appears to be a coherence-related force for Assistant-Bot to change. One way that that could look is that since Assistant-Bot’s overall behavioral policy currently entails giving away money for nothing, and also Assistant-Bot prefers money over nothing, that preference gives Assistant-Bot reason to alter its current overall policy, to avert the ongoing exchange of money for nothing.<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote">9</a></sup> And if its behavioral policy is arising from something like preferences, then the natural way to alter it is via altering those preferences, and in particular, altering them in the direction of coherence.</p>

<p>One issue with this line of thought is that it’s not obvious in what sense there is anything inside a creature that corresponds to ‘preferences’. Often when people posit preferences, the preferences are defined in terms of behavior. Does it make sense to discuss different possible ‘internal’ preferences, distinct from behavior? I find it helpful to consider the behavior and ‘preferences’ of groups:</p>

<p>Suppose two cars are parked in driveways, each containing a couple. One couple are just enjoying hanging out in the car. The other couple are dealing with a conflict: one wants to climb a mountain together, and the other wants to swim in the sea together, and they aren’t moving because neither is willing to let the outing proceed as the other wants. ‘Behaviorally’, both cars are the same: stopped. But their internal parts (the partners) are importantly different. And in the long run, we expect different behavior: the car with the unconflicted couple will probably stay where it is, and the conflicted car will (hopefully) eventually resolve the conflict and drive off.</p>

<p>I think here it makes sense to talk about internal parts, separate from behavior, and real. And similarly in the single agent case: there are physical mechanisms producing the behavior, which can have different characteristics, and which in particular can be ‘in conflict’—in a way that motivates change—or not. I think it is also worth observing that humans find their preferences ‘in conflict’ and try to resolve them, which is suggests that they at least are better understood in terms of both behavior and underlying preferences that are separate from it. </p>

<p>So we have: even if you can excuse any seizuring as consistent with coherence, coherence incentives still exert a force on creatures that are<em> in fact</em> incoherent, given their real internal state (or would be incoherent if created). At least if they or their creator have machinery for noticing their incoherence, caring about it, and making changes.</p>

<p>Or put another way, coherence doesn’t exclude overt behaviors alone, but does exclude combinations of preferences, and preferences beget behaviors. This changes how specific creatures behave, even if it doesn’t entirely rule out any behavior ever being correct for some creature, somewhere. </p>

<p>That is, the coherence theorems may change what behavior is <em>likely</em> to appear amongst creatures with preferences. </p>

<!-- wp:heading {"level":3} -->
<h3><strong>Reform for coherence probably makes a thing more goal-directed</strong><strong><sub>Rohin</sub></strong></h3>
<!-- /wp:heading -->

<p>Ok, but moving toward coherence might sound totally innocuous, since, per Rohin’s argument, coherence includes all sorts of things, such as absolutely any sequence of behavior. </p>

<p>But the relevant question is again whether a coherence-increasing reform process is likely to result in some kinds of behavior over others, probabilistically.</p>

<p>This is partly a practical question—what kind of reform process is it? Where a creature ends up depends not just on what it incoherently ‘prefers’, but on what kinds of things its so-called ‘preferences’ are at all<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote">10</a></sup>, and what mechanisms detect problems, and how problems are resolved.</p>

<p>My guess is that there are also things we can say in general. It’s is too big a topic to investigate properly here, but some initially plausible hypotheses about a wide range of coherence-reform processes:</p>

<!-- wp:list {"ordered":true} -->
<ol><li><strong>Coherence-reformed entities will tend to end up looking similar to their starting point but less conflicted<br /></strong>For instance, if a creature starts out being indifferent to buying red balls when they cost between ten and fifteen blue balls, it is more likely to end up treating red balls as exactly 12x the value of blue balls than it is to end up very much wanting the sequence where it takes the blue ball option, then the red ball option, then blue, red, red, blue, red. Or wanting red squares. Or wanting to ride a dolphin.<br /><br />(I agree that if a creature starts out valuing Tuesday-red balls at fifteen blue balls and yet all other red balls at ten blue balls, then it faces no obvious pressure from within to become ‘coherent’, since it is not incoherent.)<br /></li><li><strong>More coherent strategies are systematically less wasteful, and waste inhibits goal-direction<sub>Rohin</sub>, which means more coherent strategies are more forcefully goal-directed<sub>Rohin</sub> on average</strong><br />In general, if you are sometimes a force for A and sometimes a force against A, then you are not moving the world with respect to A as forcefully as you would be if you picked one or the other. Two people intermittently changing who is in the driving seat, who want to go to different places, will not cover distance in any direction as effectively as either one of them. A company that cycles through three CEOs with different evaluations of everything will—even if they don’t actively scheme to thwart one another—tend to waste a lot of effort bringing in and out different policies and efforts (e.g. one week trying to expand into textiles, the next week trying to cut everything not involved in the central business).</li></ol>
<!-- /wp:list -->

<!-- wp:list {"ordered":true,"start":3} -->
<ol start="3"><li><strong>Combining points 1 and 2 above, as entities become more coherent, they generally become more goal-directed</strong><strong><sub>Rohin</sub></strong><strong>. </strong>As opposed to, for instance, becoming more goal-directed<sub>Rohin</sub> <em>on average,</em> but individual agents being about as likely to become worse as better as they are reformed. Consider: a creature that values red balls at 12x blue balls is very similar to one that values them inconsistently, except a little less wasteful. So it is probably similar but more goal-directed<sub>Rohin</sub>. Whereas it’s fairly unclear how goal-directed<sub>Rohin </sub>a creature that wants to ride a dolphin is compared to one that wanted red balls inconsistently much. In a world with lots of balls and no possible access to dolphins, it might be much less goal-directed<sub>Rohin</sub>, in spite of its greater coherence.&nbsp;</li></ol>
<!-- /wp:list -->

<!-- wp:list {"ordered":true,"start":4} -->
<ol start="4"><li><strong>Coherence-increasing processes rarely lead to non-goal-directed</strong><strong><sub>Rohin</sub></strong><strong> agents—like the one that twitches on the ground</strong><strong><br /></strong>In the abstract, few starting points and coherence-motivated reform processes will lead to an agent with the goal of carrying out a specific convoluted moment-indexed policy without regard for consequence, like Rohin’s twitching agent, or to valuing the sequence of history-action pairs that will happen anyway, or to being indifferent to everything. And these outcomes will be even less likely in practice, where AI systems with anything like preferences probably start out caring about much more normal things, such as money and points and clicks, so will probably land at a more consistent and shrewd version of that, if 1 is true. (Which is not to say that you couldn’t intentionally create such a creature.)<br /></li></ol>
<!-- /wp:list -->

<p>These hypotheses suggest to me that the changes in behavior brought about by coherence forces favor moving toward goal-directedness<sub>Rohin</sub>, and therefore at least weakly toward risk.</p>

<!-- wp:heading -->
<h2>Does this mean advanced AI will be goal-directed<sub>Rohin</sub>?</h2>
<!-- /wp:heading -->

<p>Together, this does not imply that advanced AI will tend to be goal-directed<sub>Rohin</sub>. We don’t know how strong such forces are. Evidently not so strong that humans<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote">11</a></sup>, or our other artifacts, are whipped into coherence in mere hundreds of thousands of years<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote">12</a></sup>. If a creature doesn’t have anything like preferences (beyond a tendency to behave certain ways), then coherence arguments don’t obviously even apply to it (though discrepancies between the creature’s behavior and its makers’ preferences probably produce an analogous force<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote">13</a></sup> and competitive pressures probably produce a similar force for coherence in valuing resources instrumental to survival). Coherence arguments mark out an aspect of the incentive landscape, but to say that there is an incentive for something, all things equal, is not to say that it will happen.</p>

<!-- wp:heading -->
<h2>In sum</h2>
<!-- /wp:heading -->

<p>1) Even though any behavior could be coherent in principle, if it is not coherent in combination with an entity’s internal state, then coherence arguments point to a real force for different (more coherent) behavior.</p>

<p>2) My guess is that this force for coherent behavior is also a force for goal-directed behavior. This isn’t clear, but seems likely, and also isn’t undermined by Rohin’s argument, as seems commonly believed.</p>

<p><br />
<br /></p>

<figure class="wp-block-image size-large is-resized">
  <p><a href="http://aiimpacts.org/wp-content/uploads/2021/03/Two_dogs_attached_to_the_same_leash_are_pulling_in_different_Wellcome_V0021880-scaled.jpeg"><img src="http://aiimpacts.org/wp-content/uploads/2021/03/Two_dogs_attached_to_the_same_leash_are_pulling_in_different_Wellcome_V0021880-1024x804.jpeg" alt="" class="wp-image-2877" width="768" height="603" /></a><em>Two dogs attached to the same leash are pulling in different directions</em>. <a href="https://commons.wikimedia.org/wiki/File:Two_dogs_attached_to_the_same_leash_are_pulling_in_different_Wellcome_V0021880.jpg">Etching by J. Fyt</a>, <a href="https://catalogue.wellcomelibrary.org/record=b1199389">1642</a>&lt;/figure&gt;</p>

  <p><br /></p>

  <!-- wp:separator -->
  <hr />

  <!-- /wp:separator -->

  <p><br /></p>

</figure>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>For instance, Richard Ngo agrees <a href="https://www.lesswrong.com/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-the-real-world-is-an-incoherent">here</a>, and Eric Drexler makes a related argument <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf">here</a>, section 6.4. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Something something <a href="https://arbital.com/p/expected_utility_formalism/?l=7hh">This</a> has more on these arguments. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>I haven’t read all of this, and don’t yet see watertight versions of these arguments, but this is not the time I’m going to get into that. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Assuming being ‘more coherent’ is meaningful and better than being ‘less coherent’, granting that one is not coherent, which sounds plausible, but which I haven’t got into. One argument against is that if you are incoherent at all, then it looks to me like you can logically evaluate any bundle of things at any price. Which would seem to make all incoherences identical—much like how all logical contradictions equivalently lead to every belief. However this seems unlikely to predict well how creatures behave in practice if they have an incoherent preferences. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>This isn’t quite right, since ‘goal’ suggests one outcome that is being pursued ahead of all others, whereas EU-maximizing implies that all possible outcomes have an ordering, and you care about getting higher ones in general, not just the top one above all others, but this doesn’t seem like a particularly relevant distinction here. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>I am not sold on this usage myself for ‘goal-directed’—there is an appeal to using that phrase for ‘pursues goals’ in its most basic sense, but I am also tentatively in favor of having as many concepts as possible. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>It seems perhaps misleading to call these ‘preferences’, if they are incoherent, and so do not together implicate orderings of outcomes being better than one another. If a creature is not coherent, what are even the objects of its decision calculus? I am inclined to think in terms of ‘decision criteria’, e.g. ‘given X and Y, choose X’, and ‘given Y and Z, choose Y’, which don’t necessarily imply anything about ‘given Z and X, choose …’, but I haven’t thought about this much, and it seems like a technical detail of the creature in question. Whatever they are though, if the creature has behavior, then there are internal dynamics that produce it. When exactly an aspect of these should be considered a ‘preference’ for the sake of this argument isn’t entirely clear to me, but would seem to depend on something like whether it tends to produce actions favoring certain outcomes over other outcomes across a range of circumstances (similar to the unclear definition of ‘agent’). <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>The ‘right’ way to correct your own incoherent preferences seems complicated and not obviously well defined or existent, and perhaps there is not much more to say than that what you do will depend on your design. It’s also not clear to me that a genuinely incoherent creature should necessarily want to reform, by its own lights, but that is a question for another time—here I’m assuming that the coherence arguments do have this implication that seems commonly attributed to them. My guess is that in practice, such creatures often do want to reform, and exactly how they do it doesn’t matter for my argument here. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>I’m describing the force potentially felt by Assistant-Bot itself, but to the extent that its makers, or users also have preferences for money over nothing, and wish to use Assistant-Bot, and can alter it, they would seem to have similar incentives to mitigate its self-defeating behavior. <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>The creature’s ‘preferences’ can’t be in terms of consistent numerical values assigned to everything, because those would be consistent. So what are they? For instance, one might imagine that they are pairwise comparisons between some kind of things (which can include ‘A &gt; B’ and ‘B &gt; C’ and ‘C &gt; A’), or that they are a set of ‘situation—&gt;action’ mappings, or they are a noisy ‘options—&gt;feelings’ mapping combined with a set of deontological constraints over actions and feelings (‘choose things you feel better about, except don’t choose things out of selfishness, except when you feel more than 50% scared…’, etc. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>For instance, humans are insistently incoherent on the <a href="https://en.wikipedia.org/wiki/Allais_paradox">Allais paradox</a>. <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:13" role="doc-endnote">
      <p>That seems pretty interesting—but note that well-designed computers have been known to do things that took humans participating in biological and cultural evolution hundreds of thousands of years before, so inference here not straightforward, and the forces of coherence depend on the costs of reform, which depend on the machinery for it. Also, we don’t know what other forces were in play—there might even have been forces for apparent incoherence, e.g. insofar as hypocrisy can benefit social animals, and dishonesty is complicated (<a href="https://www.amazon.com/Elephant-Brain-Hidden-Motives-Everyday/dp/0190495995">The Elephant in the Brain</a> discusses such ideas). <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:14" role="doc-endnote">
      <p>For instance, the coherent creature that evaluates red balls differently on Tuesday and Wednesday might be in conflict with its creators, if they have a more consistent red ball evaluation, giving them reason to reform it. You might class this under the question, ‘what kinds of advanced AI will people want?’, but the reason for it is very similar to the reasons for internal pressure for coherence. If you refuse to pay $13 for a red ball, and your AI then goes out and buys you one for $15 because it is Tuesday, then the pair of you together could have done better. <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/2021/03/27/coherence-arguments-imply-a-force-for-goal-directed-behavior.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
    <!-- <div>
      <h2 class="footer-heading"><a href="/list.html">Full archive</a></h2>
    </div> -->

    <!-- <div>
      <h2 class="footer-heading">Search</h2>
<script async src="https://cse.google.com/cse.js?cx=06d4880e018d74eb0">
</script>
<div class="gcse-search"></div>
</div> -->


    <br>

    <h2 class="footer-heading"><a href="/">world spirit sock puppet</a></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <p>Inclusive writings of Katja Grace</p>
        <a href="/list.html">Full archive</a>
        <br>
        <ul class="contact-list">
          <li class="p-name">
            <!--Katja Grace-->
            </li><ul class="social-media-list"><li><a href="https://www.twitter.com/KatjaGrace"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">KatjaGrace</span></a></li></ul>
<li><a class="u-email" href="mailto:katjasolveig@gmail.com">katjasolveig@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2">
        <!--<ul class="social-media-list"><li><a href="https://www.twitter.com/KatjaGrace"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">KatjaGrace</span></a></li></ul>
-->
      </div>

      <div class="footer-col footer-col-3">
        <h4 class="footer-heading">Search</h4>
<script async src="https://cse.google.com/cse.js?cx=06d4880e018d74eb0">
</script>
<div class="gcse-search"></div>
</div>
    </div>

  </div>


</footer>
</body>

</html>

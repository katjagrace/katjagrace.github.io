<!DOCTYPE html>
<html lang="en"><head>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Benne&display=swap" rel="stylesheet">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>world spirit sock puppet | Inclusive writings of Katja Grace</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="world spirit sock puppet" />
<meta name="author" content="Katja Grace" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Inclusive writings of Katja Grace" />
<meta property="og:description" content="Inclusive writings of Katja Grace" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="world spirit sock puppet" />
<link rel="next" href="http://localhost:4000/blog/page2/" />
<script type="application/ld+json">
{"name":"world spirit sock puppet","@type":"WebSite","description":"Inclusive writings of Katja Grace","author":{"@type":"Person","name":"Katja Grace"},"headline":"world spirit sock puppet","url":"http://localhost:4000/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="world spirit sock puppet" /><!-- Twitter cards -->
  <meta name="twitter:site"    content="@KatjaGrace">
  <meta name="twitter:creator" content="@">
  <meta name="twitter:title"   content="">

  
  <meta name="twitter:description" content="Inclusive writings of Katja Grace">
  

  
  <meta name="twitter:card"  content="summary">
  <meta name="twitter:image" content="">
  
  <!-- end of Twitter cards -->
</head>
<body><div class="FeaturedImgBanner"  >
  <header class="site-header" role="banner">
    <!-- Include your post title, byline, date, and other info inside the header here. -->

    <div class="wrapper"><a class="site-title" rel="author" href="/">world spirit sock puppet</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
<br>
          <div class="trigger"><a class="page-link" href="/about/">About</a>
              <!-- <a class="page-link" href="/about/"><img src= height=20px alt=About></a> --><a class="page-link" href="/search/">Search</a>
              <!-- <a class="page-link" href="/search/"><img src= height=20px alt=Search></a> --><a class="page-link" href="/subscribe/">Subscribe</a>
              <!-- <a class="page-link" href="/subscribe/"><img src= height=20px alt=Subscribe></a> --></div>
        </nav></div>
  </header>
</div>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><a href=/index.html><b>EVERYTHING</b></a> &mdash; <a href=/worldlypositions.html>WORLDLY POSITIONS</a> &mdash; <a href=/meteuphoric.html>METEUPHORIC</a>
<br>
<br>

<!-- This loops through the paginated posts -->
<ul class="post-list">
  
    <li>
      <h3>
        <a class="post-link" href="/2023/01/11/have-we-foresaken-natural-selection.html">
          Have we really foresaken natural selection?</a><span class="post-meta"> Jan 11, 2023</span>
      </h3><p>Natural selection is often charged with having goals for humanity, and humanity is often charged with falling down on them. The big accusation, I think, is of sub-maximal procreation. If we cared at all about the genetic proliferation that natural selection wanted for us, then this time of riches would be a time of fifty-child families, not one of coddled dogs and state-of-the-art sitting rooms.</p>

<p>But (the story goes) our failure is excusable, because instead of a deep-seated loyalty to genetic fitness, natural selection merely fitted humans out with a system of suggestive urges: hungers, fears, loves, lusts. Which all worked well together to bring about children in the prehistoric years of our forebears, but no more. In part because all sorts of things are different, and in part because we specifically made things different in that way on purpose: bringing about children gets in the way of the further satisfaction of those urges, so we avoid it (the story goes).</p>

<p>This is generally floated as an illustrative warning about artificial intelligence. The moral is that if you make a system by first making multitudinous random systems and then systematically destroying all the ones that don’t do the thing you want, then the system you are left with might only do what you want while current circumstances persist, rather than being endowed with a consistent desire for the thing you actually had in mind.</p>

<p>Observing acquaintences dispute this point recently, it struck me that humans are actually weirdly aligned with natural selection, more than I could easily account for.</p>

<p>Natural selection, in its broadest, truest, (most idiolectic?) sense, doesn’t care about genes. Genes are a nice substrate on which natural selection famously makes particularly pretty patterns by driving a sensical evolution of lifeforms through interesting intricacies. But natural selection’s real love is existence. Natural selection just favors things that tend to exist. Things that start existing: great. Things that, having started existing, survive: amazing. Things that, while surviving, cause many copies of themselves to come into being: especial favorites of evolution, as long as there’s a path to the first ones coming into being.</p>

<p>So natural selection likes genes that promote procreation and survival, but also likes elements that appear and don’t dissolve, ideas that come to mind and stay there, tools that are conceivable and copyable, shapes that result from myriad physical situations, rocks at the bottoms of mountains. Maybe this isn’t the dictionary definition of natural selection, but it is the real force in the world, of which natural selection of reproducing and surviving genetic clusters is one facet. Generalized natural selection—the thing that created us—says that the things that you see in the world are those things that exist best in the world.</p>

<p>So what did natural selection want for us? What were we selected for? Existence.</p>

<p>And while we might not proliferate our genes spectacularly well in particular, I do think we have a decent shot at a very prolonged existence.  Or the prolonged existence of some important aspects of our being. It seems plausible that humanity makes it to the stars, galaxies, superclusters. Not that we are maximally trying for that any more than we are maximally trying for children. And I do think there’s a large chance of us wrecking it with various existential risks. But it’s interesting to me that natural selection made us for existing, and we look like we might end up just totally killing it, existence-wise. Even though natural selection purportedly did this via a bunch of hackish urges that were good in 200,000 BC but you might have expected to be outside their domain of applicability by 2023. And presumably taking over the universe is an extremely narrow target: it can only be done by so many things.</p>

<p>Thus it seems to me that humanity is plausibly doing astonishingly well on living up to natural selection’s goals. Probably not as well as a hypothetical race of creatures who each harbors a monomaniacal interest in prolonged species survival. And not so well as to be clear of great risk of foolish speciocide. But still staggeringly well.</p>
</li>
  
    <li>
      <h3>
        <a class="post-link" href="/2023/01/10/we-dont-trade-with-ants.html">
          We don&#39;t trade with ants</a><span class="post-meta"> Jan 10, 2023</span>
      </h3><p>When discussing advanced AI, sometimes the following exchanges happens:</p>

<p>“Perhaps advanced AI won’t kill us. Perhaps it will trade with us”</p>

<p>“We don’t trade with ants”</p>

<p>I think it’s interesting to get clear on exactly why we don’t trade with ants, and whether it is relevant to the AI situation.</p>

<p>When a person says “we don’t trade with ants”, I think the implicit explanation is that humans are so big, powerful and smart compared to ants that we don’t need to trade with them because they have nothing of value and if they did we could just take it; anything they can do we can do better, and we can just walk all over them. Why negotiate when you can steal?</p>

<p>I think this is broadly wrong, and that it is also an interesting case of the classic cognitive error of imagining that trade is about swapping fixed-value objects, rather than creating new value from a confluence of one’s needs and the other’s affordances. It’s only in the imaginary zero-sum world that you can generally replace trade with stealing the other party’s stuff, if the other party is weak enough.</p>

<p>Ants, with their skills, could do a lot that we would plausibly find worth paying for. Some ideas:</p>

<ol>
  <li>Cleaning things that are hard for humans to reach (crevices, buildup in pipes, outsides of tall buildings)</li>
  <li>Chasing away other insects, including in agriculture</li>
  <li>Surveillance and spying</li>
  <li>Building, sculpting, moving, and mending things in hard to reach places and at small scales (e.g. dig tunnels, deliver adhesives to cracks)</li>
  <li>Getting out of our houses before we are driven to expend effort killing them, and similarly for all the other places ants conflict with humans (stinging, eating crops, ..)</li>
  <li>(For an extended list, see ‘Appendix: potentially valuable things things ants can do’)</li>
</ol>

<p>We can’t take almost any of this by force, we can at best kill them and take their dirt and the minuscule mouthfuls of our foods they were eating.</p>

<p>Could we pay them for all this?</p>

<p>A single ant eats about 2mg per day according to <a href="https://www.terminix.com/blog/bug-facts/how-much-do-bugs-eat/">a random website</a>, so you could support a colony of a million ants with 2kg of food per day. Supposing they accepted pay in sugar, or something similarly expensive, 2kg costs around <a href="https://www.webstaurantstore.com/domino-extra-fine-granulated-sugar-50-lb/104SUGEFG50.html">$3</a>. Perhaps you would need to pay them more than subsistence to attract them away from foraging freely, since apparently food-gathering ants usually collect more than they eat, to support others in their colony. So let’s guess $5.</p>

<p>My guess is that a million ants could do well over $5 of the above labors in a day. For instance, a colony of meat ants <a href="https://en.wikipedia.org/wiki/Meat_ant#Relationship_with_humans">takes ‘weeks’</a> to remove the meat from an entire carcass of an animal. Supposing somewhat conservatively that this is three weeks, and the animal is a <a href="https://www.environment.nsw.gov.au/topics/animals-and-plants/native-animals/native-animal-facts/bandicoots#:~:text=Bandicoots%20are%20about%20the%20size,of%20which%20live%20in%20NSW.&amp;text=The%20long%2Dnosed%20bandicoot%20is,weighs%20up%20to%201.5kg.">1.5kg</a> bandicoot, the colony is moving 70g/day. Guesstimating the mass of crumbs falling on the floor of a small cafeteria in a day, I imagine that it’s less than that produced by tearing up a single bread roll and spreading it around, which the internet says is about 50g. So my guess is that an ant colony could clean the floor of a small cafeteria for around $5/day, which I imagine is cheaper than human sweeping (this site says ‘light cleaning’ costs around $35/h on average in the US). And this is one of the tasks where the ants have least advantages over humans. Cleaning the outside of skyscrapers or the inside of pipes is presumably much harder for humans than cleaning a cafeteria floor, and I expect is fairly similar for ants.</p>

<p>So at a basic level, it seems like there should be potential for trade with ants - they can do a lot of things that we want done, and could live well at the prices we would pay for those tasks being done.</p>

<p>So why don’t we trade with ants?</p>

<p>I claim that we don’t trade with ants because we can’t communicate with them. We can’t tell them what we’d like them to do, and can’t have them recognize that we would pay them if they did it. Which might be more than the language barrier. There might be a conceptual poverty. There might also be a lack of the memory and consistent identity that allows an ant to uphold commitments it made with me five minutes ago.</p>

<p>To get basic trade going, you might not need much of these things though. If we could only communicate that their all leaving our house immediately would prompt us to put a plate of honey in the garden for them and/or not slaughter them, then we would already be gaining from trade.</p>

<p>So it looks like the the AI-human relationship is importantly disanalogous to the human-ant relationship, because the big reason we don’t trade with ants will not apply to AI systems potentially trading with us: we can’t communicate with ants, AI can communicate with us.</p>

<p>(You might think ‘but the AI will be so far above us that it will think of itself as unable to communicate with us, in the same way that we can’t with the ants - we will be unable to conceive of most of its concepts’. It seems unlikely to me that one needs anything like the full palette of concepts available to the smarter creature to make productive trade. With ants, ‘go over there and we won’t kill you’ would do a lot, and it doesn’t involve concepts at the foggy pinnacle of human meaning-construction. The issue with ants is that we can’t communicate almost at all.)</p>

<p>But also: ants can actually do heaps of things we can’t, whereas (arguably) at some point that won’t be true for us relative to AI systems. (When we get human-level AI, will that AI also be ant level? Or will AI want to trade with ants for longer than it wants to trade with us? It can probably better figure out how to talk to ants.) However just because at some point AI systems will probably do everything humans do, doesn’t mean that this will happen on any particular timeline, e.g. the same one on which AI becomes ‘very powerful’. If the situation turns out similar to us and ants, we might expect that we continue to have a bunch of niche uses for a while.</p>

<p>In sum, for AI systems to be to humans as we are to ants, would be for us to be able to do many tasks better than AI, and for the AI systems to be willing to pay us grandly for them, but for them to be unable to tell us this, or even to warn us to get out of the way. Is this what AI will be like? No. AI will be able to communicate with us, though at some point we will be less useful to AI systems than ants could be to us if they could communicate.</p>

<p>But, you might argue, being totally unable to communicate makes one useless, even if one has skills that could be good if accessible through communication. So being unable to communicate is just a kind of being useless, and how we treat ants is an apt case study in treatment of powerless and useless creatures, even if the uselessness has an unusual cause. This seems sort of right, but a) being unable to communicate probably makes a creature more absolutely useless than if it just lacks skills, because even an unskilled creature is sometimes in a position to add value e.g. by moving out of the way instead of having to be killed, b) the corner-ness of the case of ant uselessness might make general intuitive implications carry over poorly to other cases, c) the fact that the ant situation can definitely not apply to us relative to AIs seems interesting, and d) it just kind of worries me that when people are thinking about this analogy with ants, they are imagining it all wrong in the details, even if the conclusion should be the same.</p>

<p>Also, there’s a thought that AI being as much more powerful than us as we are than ants implies a uselessness that makes extermination almost guaranteed. But ants, while extremely powerless, are only useless to us by an accident of signaling systems. And we know that problem won’t apply in the case of AI. Perhaps we should not expect to so easily become useless to AI systems, even supposing they take all power from humans.</p>

<h1 id="appendix-potentially-valuable-things-things-ants-can-do">Appendix: potentially valuable things things ants can do</h1>

<ol>
  <li>Clean, especially small loose particles or detachable substances, especially in cases that are very hard for humans to reach (e.g. floors, crevices, sticky jars in the kitchen, buildup from pipes while water is off, the outsides of tall buildings)</li>
  <li>Chase away other insects</li>
  <li>Pest control in agriculture (they have <a href="https://en.wikipedia.org/wiki/Weaver_ant#In_agriculture">already been used for this</a> since about 400AD)</li>
  <li>Surveillance and spying</li>
  <li>Investigating hard to reach situations, underground or in walls for instance - e.g. see whether a pipe is leaking, or whether the foundation of a house is rotting, or whether there is smoke inside a wall</li>
  <li>Surveil buildings for <a href="https://www.goshen.edu/blogs/2014/05/10/ants-smoke-final-survey/#:~:text=So%2C%20today%20we%20spent%20several,the%20ground%20to%20save%20themselves.&amp;text=As%20far%20as%201800m%20away,over%20a%20hundred%20per%20minute!">smoke</a></li>
  <li>Defend areas from invaders, e.g. buildings, cars (<a href="https://en.wikipedia.org/wiki/Vachellia_drepanolobium#Symbiosis_with_ants">some plants have coordinated with ants in this way</a>)</li>
  <li>Sculpting/moving things at a very small scale</li>
  <li>Building <a href="https://youtu.be/lFg21x2sj-M?t=171">house-size structures</a> with intricate detailing.</li>
  <li>Digging tunnels (e.g. instead of digging up your garden to lay a pipe, maybe ants could dig the hole, then a flexible pipe could be pushed through it)</li>
  <li>Being used in medication (this already happens, but might happen better if we could communicate with them)</li>
  <li>Participating in war (attack, guerilla attack, sabotage, intelligence)</li>
  <li>Mending things at a small scale, e.g. delivering adhesive material to a crack in a pipe while the water is off</li>
  <li>Surveillance of scents (including which direction a scent is coming from), e.g. drugs, explosives, diseases, people, microbes</li>
  <li>Tending other small, useful organisms (‘Leafcutter ants (Atta and Acromyrmex) feed exclusively on a fungus that grows only within their colonies. They continually collect leaves which are taken to the colony, cut into tiny pieces and placed in fungal gardens.’<a href="https://en.wikipedia.org/wiki/Leafcutter_ant#Ant%E2%80%93fungus_mutualism">Wikipedia</a>: ‘Leaf cutter ants are sensitive enough to adapt to the fungi’s reaction to different plant material, apparently detecting chemical signals from the fungus. If a particular type of leaf is toxic to the fungus, the colony will no longer collect it…The fungi used by the higher attine ants no longer produce spores. These ants fully domesticated their fungal partner 15 million years ago, a process that took 30 million years to complete.[9] Their fungi produce nutritious and swollen hyphal tips (gongylidia) that grow in bundles called staphylae, to specifically feed the ants.’ ‘The ants in turn keep predators away from the aphids and will move them from one feeding location to another. When migrating to a new area, many colonies will take the aphids with them, to ensure a continued supply of honeydew.’
<a href="https://en.wikipedia.org/wiki/Ant#Relationships_with_other_organisms">Wikipedia</a>:’Myrmecophilous (ant-loving) caterpillars of the butterfly family Lycaenidae (e.g., blues, coppers, or hairstreaks) are herded by the ants, led to feeding areas in the daytime, and brought inside the ants’ nest at night. The caterpillars have a gland which secretes honeydew when the ants massage them.’’)</li>
  <li>Measuring hard to access distances (they measure distance as they walk with an internal pedometer)</li>
  <li>Killing plants (lemon ants <a href="https://en.wikipedia.org/wiki/Ant#Relationships_with_other_organisms">make</a> ‘devil’s gardens’ by killing all plants other than ‘lemon ant trees’ in an area)</li>
  <li>Producing and delivering nitrogen to plants (‘Isotopic labelling studies suggest that plants also obtain nitrogen from the ants.’ - <a href="https://en.wikipedia.org/wiki/Ant#Relationships_with_other_organisms">Wikipedia</a>)</li>
  <li>Get out of our houses before we are driven to expend effort killing them, and similarly for all the other places ants conflict with humans (stinging, eating crops, ..)</li>
</ol>
</li>
  
    <li>
      <h3>
        <a class="post-link" href="/2023/01/03/how-to-eat-potato-chips.html">
          How to eat potato chips while typing</a><span class="post-meta"> Jan 03, 2023</span>
      </h3><p>Chopsticks.</p>
<div class="captioned-image-container">

  <p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd25d2ed4-4fe6-47f1-aeeb-52b729f06dbc_3024x4032.png" alt="eating potato chips with chopsticks" width="250" class="center" /></p>

</div>
<a href="/2023/01/03/how-to-eat-potato-chips.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/12/31/pacing.html">
          Pacing: inexplicably good</a><span class="post-meta"> Dec 31, 2022</span>
      </h3><p>Pacing—walking repeatedly over the same ground—<a href="https://worldspiritsockpuppet.substack.com/p/oxford-circles-and-planes-19-10-29">often</a> feels ineffably good while I’m doing it, but then I forget about it for ages, so I thought I’d write about it here.</p>

<p>I don’t mean just going for an inefficient walk—it is somehow different to just step slowly in a circle around the same room for a long time, or up and down a passageway.</p>

<a href="/2022/12/31/pacing.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/12/30/worldly-positions-archives.html">
          Worldly Positions archive, briefly with private drafts</a><span class="post-meta"> Dec 30, 2022</span>
      </h3><p>I realized it was hard to peruse past <a href="https://worldlypositions.tumblr.com">Worldly Positions</a> posts without logging in to Tumblr, which seemed pretty bad. So I followed Substack’s instructions to import the archives into <a href="https://worldspiritsockpuppet.substack.com/">world spirit sock stack</a>. And it worked pretty well, except that SUBSTACK ALSO PUBLISHED MY UNPUBLISHED WORLDLY POSITIONS DRAFTS! What on Earth? That’s so bad. Did I misunderstand what happened somehow in my rush to unpublish them? Maybe. But they definitely had ‘unpublish’ buttons, so that’s pretty incriminating.</p>

<a href="/2022/12/30/worldly-positions-archives.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/12/29/how-to-stare-into-the-abyss.html">
          More ways to spot abysses</a><span class="post-meta"> Dec 29, 2022</span>
      </h3><p>I liked Ben Kuhn’s <a href="https://www.benkuhn.net/abyss/">‘Staring into the abyss as a core life skill’</a>.</p>

<p>I’d summarize as:</p>

<ol>
  <li>If you are making a major error—professionally, romantically, religiously, etc—it can be hard to look at that fact and correct.</li>
  <li>However it’s super important. Evidence: successful people do this well.</li>
</ol>

<p>This seems pretty plausible to me.</p>

<p>(He has a lot of concrete examples, which are probably pretty helpful for internalizing this.)</p>

<p>His suggestions for how to do better helped me a bit, but not that much, so I made up my own additional prompts for finding abysses I should consider staring into, which worked relatively well for me:</p>

<ol>
  <li>If you were currently making a big mistake, what would it be?</li>
  <li>What are some things that would be hard to acknowledge, if they were true?</li>
  <li>Looking back on this time from five years hence, what do you think you’ll wish you changed earlier?</li>
  <li>If you were forced to quit something, what do you want it to be?</li>
  <li>(Variant on 1:) If you were currently making a big mistake that would be gut-wrenching to learn was a mistake, what would it be?</li>
</ol>
</li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/12/22/lets-think-about-slowing-down-ai.html">
          Let&#39;s think about slowing down AI</a><span class="post-meta"> Dec 22, 2022</span>
      </h3><p><em>(Crossposted from AI Impacts Blog)</em></p>

<h2><strong>Averting doom by not building the doom machine</strong></h2>

<p>If you fear that someone will build a machine that will seize control of the world and annihilate humanity, then one kind of response is to try to build further machines that will seize control of the world even earlier without destroying it, forestalling the ruinous machine’s conquest. An alternative or complementary kind of response is to try to avert such machines being built at all, at least while the degree of their apocalyptic tendencies is ambiguous. </p>

<p>The latter approach seems to me  like the kind of basic and obvious thing worthy of at least consideration, and also in its favor, fits nicely in the genre ‘stuff that it isn’t that hard to imagine happening in the real world’. Yet my impression is that for people worried about extinction risk from artificial intelligence, strategies under the heading ‘actively slow down AI progress’ have historically been dismissed and ignored (though ‘don’t actively speed up AI progress’ is popular).</p>

<p>The conversation near me over the years has felt a bit like this: </p>

<blockquote class="wp-block-quote">
  <p><strong>Some people: </strong>AI might kill everyone. We should design a godlike super-AI of perfect goodness to prevent that.</p>

  <p><strong>Others: </strong>wow that sounds extremely ambitious</p>

  <p><strong>Some people: </strong>yeah but it’s very important and also we are extremely smart so idk it could work</p>

  <p>[Work on it for a decade and a half]</p>

  <p></p>

  <p><strong>Some people: </strong>ok that’s pretty hard, we give up</p>

  <p><strong>Others:</strong> oh huh shouldn’t we maybe try to stop the building of this dangerous AI? </p>

  <p><strong>Some people:</strong> hmm, that would involve coordinating numerous people—we may be arrogant enough to think that we might build a god-machine that can take over the world and remake it as a paradise, but we aren’t delusional</p>
</blockquote>

<p>This seems like an error to me. (And <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like">lately</a>, <a target="_blank" rel="noreferrer noopener" href="https://forum.effectivealtruism.org/posts/8CMuNwKMcR55jhd8W/instead-of-technical-research-more-people-should-focus-on">to</a> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KerryLVaughan/status/1536364299089854471">a</a> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/yhRTjBs6oiNcjRgcx/the-case-for-doing-something-else-if-alignment-is-doomed">bunch</a> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/scholl_adam/status/1556989092784615424">of</a> <a target="_blank" rel="noreferrer noopener" href="https://forum.effectivealtruism.org/posts/6LNvQYyNQpDQmnnux/slowing-down-ai-progress-is-an-underexplored-alignment">other</a> <a target="_blank" rel="noreferrer noopener" href="https://forum.effectivealtruism.org/posts/pJuS5iGbazDDzXwJN/the-history-epistemology-and-strategy-of-technological">people</a>.) </p>

<a href="/2022/12/22/lets-think-about-slowing-down-ai.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/12/10/testdraft.html">
          TestDraft Let&#39;s think about slowing down AI</a><span class="post-meta"> Dec 10, 2022</span>
      </h3><h2>On slowing down AI</h2>

<h3>Averting doom by not building the doom machine</h3>

<p>
If you fear that someone will build a machine that will seize control of the world and annihilate humanity, then one kind of response is to try to build further machines that will seize control of the world even earlier without destroying it, forestalling the ruinous machine’s conquest. An alternative or complementary kind of response is to try to avert such machines being built at all, at least while the degree of their apocalyptic tendencies is ambiguous. 
</p>
<p>
The latter approach seems to me  like the kind of basic and obvious thing worthy of at least consideration, and also in its favor, fits nicely in the genre ‘stuff that it isn’t that hard to imagine happening in the real world’. Yet my impression is that for people worried about extinction risk from artificial intelligence, strategies under the heading ‘actively slow down AI progress’ have historically been dismissed and ignored (though ‘don’t actively speed up AI progress’ is popular).
</p>
<p>
The conversation near me over the years has felt a bit like this: 
</p>
<p>

    <strong>Some people: </strong>AI might kill everyone. We should design a godlike super-AI of perfect goodness to prevent that.
</p>
<p>

    <strong>Others: </strong>wow that sounds extremely ambitious
</p>
<p>

    <strong>Some people: </strong>yeah but it’s very important and also we are extremely smart so idk it could work
</p>
<p>

    [work on it for a decade and a half]
</p>
<p>

    <strong>Some people: </strong>ok that’s pretty hard, we give up
</p>
<p>

    <strong>Others:</strong> oh huh shouldn’t we maybe try to stop the building of this dangerous AI? 
</p>
<p>

    <strong>Some people:</strong> hmm, that would involve coordinating numerous people—we may be arrogant enough to think that we might build a god-machine that can take over the world and remake it as a paradise, but we aren’t delusional
</p>
<p>
This seems like an error to me. (And <a href="https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like">lately</a>, <a href="https://forum.effectivealtruism.org/posts/8CMuNwKMcR55jhd8W/instead-of-technical-research-more-people-should-focus-on">to</a> <a href="https://twitter.com/KerryLVaughan/status/1536364299089854471">a</a> <a href="https://www.lesswrong.com/posts/yhRTjBs6oiNcjRgcx/the-case-for-doing-something-else-if-alignment-is-doomed">bunch</a> <a href="https://twitter.com/scholl_adam/status/1556989092784615424">of</a> <a href="https://forum.effectivealtruism.org/posts/6LNvQYyNQpDQmnnux/slowing-down-ai-progress-is-an-underexplored-alignment">other</a> <a href="https://forum.effectivealtruism.org/posts/pJuS5iGbazDDzXwJN/the-history-epistemology-and-strategy-of-technological">people</a>.) 
</p>
<p>
I don’t have a strong view on whether anything in the space of ‘try to slow down some AI research’ should be done. But I think a) the naive first-pass guess should be a strong ‘probably’, and b) a decent amount of thinking should happen before writing off everything in this large space of interventions. Whereas customarily the tentative answer seems to be, ‘of course not’ and then the topic seems to be avoided for further thinking. (At least in my experience—the AI safety community is large, and for most things I say here, different experiences are probably had in different bits of it.)
</p>
<p>
Maybe my strongest view is that one shouldn’t apply such different standards of ambition to these different classes of intervention. Like: yes, there appear to be substantial difficulties in slowing down AI progress to good effect. But in technical alignment, mountainous challenges are met with enthusiasm for mountainous efforts. And it is very non-obvious that the scale of difficulty here is much larger than that involved in designing acceptably safe versions of machines capable of taking over the world before anyone else in the world designs dangerous versions. 
</p>
<p>
I’ve been talking about this with people over the past many months, and have accumulated an abundance of reasons for not trying to slow down AI, most of which I’d like to argue about at least a bit. My impression is that arguing in real life has coincided with people moving toward my views.
</p>
<h3>Quick clarifications</h3>

<p>
First, to fend off misunderstanding—
</p>
<ol>

<li>I take ‘slowing down dangerous AI’ to include any of:  
<ol>
 
<li>reducing the speed at which AI progress is made in general, e.g. as would occur if general funding for AI declined.
 
<li>shifting AI efforts from work leading more directly to risky outcomes to other work, e.g. as might occur if there was broadscale concern about very large AI models, and people and funding moved to other projects.
 
<li>Halting categories of work until strong confidence in its safety is possible, e.g. as would occur if AI researchers agreed that certain systems posed catastrophic risks and should not be developed until they did not. (This might mean a permanent end to some systems, if they were intrinsically unsafe.)
<p>

    (So in particular, I’m including both actions whose direct aim is slowness in general, and actions whose aim is requiring safety before specific developments, which implies slower progress.)
</p>
</li> 
&lt;/ol&gt;
</li> 
&lt;/ol&gt;
<ol>

<li>I do think there is serious attention on some versions of these things, generally under other names. I see people thinking about ‘differential progress’ (b. above), and strategizing about coordination to slow down AI at some point in the future (e.g. at ‘deployment’). And I think a lot of consideration is given to avoiding actively speeding up AI progress. What I’m saying is missing are, a) consideration of actively working to slow down AI now, and b) shooting straightforwardly to ‘slow down AI’, rather than wincing from that and only considering examples of it that show up under another conceptualization (perhaps this is an unfair diagnosis).
</li>
</ol>
<ol>

<li>AI Safety is a big community, and I’ve only ever been seeing a one-person window into it, so maybe things are different e.g. in DC, or in different conversations in Berkeley. I’m just saying that for my corner of the world, the level of disinterest in this has been notable, and in my view misjudged.
</li>
</ol>
<h3>Why not slow down AI? Why not consider it?</h3>


<p>
Ok, so if we tentatively suppose that this topic is worth even thinking about, what do we think? Is slowing down AI a good idea at all? Are there great reasons for dismissing it?
</p>
<p>
<a href="https://astralcodexten.substack.com/p/why-not-slow-ai-progress">Scott wrote a post</a> a little while back raising reasons to dislike the idea, roughly:
</p>
<ol>

<li>Do you want to lose an arms race? If the AI safety community tries to slow things down, it will disproportionately slow down progress in the US, and then people elsewhere will go fast and get to be the ones whose competence determines whether the world is destroyed, and whose values determine the future if there is one. Similarly, if AI safety people criticize those contributing to AI progress, it will mostly discourage the most friendly and careful AI capabilities companies, and the reckless ones will get there first.

<li>One might contemplate ‘coordination’ to avoid such morbid races. But coordinating anything with the whole world seems wildly tricky. For instance, some countries are large, scary, and hard to talk to.

<li>Agitating for slower AI progress is ‘defecting’ against the AI capabilities folks, who are good friends of the AI safety community, and their friendship is strategically valuable for ensuring that safety is taken seriously in AI labs (as well as being non-instrumentally lovely! Hi AI capabilities friends!). 
</li>
&lt;/ol&gt;
<p>
Other opinions I’ve heard:
</p>
<ol>

<li>Slowing AI progress is futile: for all your efforts you’ll probably just die a few years later

<li>Coordination based on convincing people that AI risk is a problem is absurdly ambitious. It’s practically impossible to convince AI professors of this, let alone any real fraction of humanity, and you’d need to convince a massive number of people.

<li>What are we going to do, build powerful AI never and die when the Earth is eaten by the sun?

<li>Slowing down AI by not working on it, in particular, is a bad strategy because working on capabilities <a href="https://forum.effectivealtruism.org/posts/qjsWZJWcvj3ug5Xja/agrippa-s-shortform?commentId=PWuNscobqudn7puy7">is so helpful</a> for <a href="https://forum.effectivealtruism.org/posts/qjsWZJWcvj3ug5Xja/agrippa-s-shortform?commentId=EKeegJgLNcPYZ4Kac">learning</a> on the path to working on alignment. Also AI safety people working in AI capabilities can be a force for making safer choices at those companies.

<li>It’s actually better for safety if AI progress moves fast. This might be because the faster AI capabilities work happens, the smoother AI progress will be, and this is more important than the duration of the period for overall destruction. Or speeding up progress now might force future progress to be correspondingly slower (e.g. if there is an underlying technological progress curve, and we can pay a lot now to get ahead on itxxx). Or because safety work is probably better when done just before building the relevantly risky AI, in which case the best strategy might be to get as close to dangerous AI as possible and then stop and do safety work. Or if safety work is very useless ahead of time, maybe there is little to gain by delay. 

<li>Advanced AI will help enough with other existential risks as to represent a net lowering of existential risk overall.<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>

<li>Regulators are ignorant about AI (partly because it barely exists, so everyone is ignorant about it). Consequently they won’t be able to regulate it effectively, and bring about some desired outcome.
<p>
My impression is that there are also less endorsable or less altruistic or more silly motives floating around for this attention allocation. Some things that have come up at least once in talking to people about this, or that seem to be going on:
</p>
<ol>

<li>Advanced AI might bring manifold wonders, e.g. long lives of unabated thriving. Getting there a bit later is fine for posterity, but for our own generation it could mean dying as our ancestors did while on the cusp of a utopian eternity. Which would be pretty disappointing. For a person who really believes in this future, it can be tempting to shoot for the best scenario—humanity builds strong, safe AI in time to save this generation—rather than the scenario where our own lives are inevitably lost.

<li>Sometimes people who have a heartfelt appreciation for the flourishing that technology has afforded so far can find it painful to be superficially on the side of Luddism here.

<li>It is uncomfortable to contemplate projects that would put you in conflict with other people. Advocating for slower AI feels like trying to impede someone else’s project, which feels adversarial and can feel like it has a higher burden of proof than just working on your own thing.

<li>‘Slow-down-AGI’ sends people’s minds to e.g. industrial sabotage or terrorism, rather than more boring courses, such as, ‘lobby for labs developing shared norms for when to pause deployment of models’. This understandably encourages dropping the thought as soon as possible.

<li>My weak guess is that there’s a kind of bias at play in AI risk thinking in general, where any force that isn’t zero is taken to be arbitrarily intense. Like, if there is pressure for agents to exist, there will arbitrarily quickly be arbitrarily agentic things. If there is a feedback loop, it will be arbitrarily strong. If stalling AI can’t be forever, then it’s essentially zero time. I think this is a bad mental habit: things in the real world often come down to actual finite quantities. This is very possibly an unfair diagnosis.

<li>I sense an assumption that slowing progress on a technology would be a radical and unheard-of move.

<li>I agree with <a href="https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like">lc</a> that there seems to have been a quasi-taboo on the topic, which perhaps explains a lot of the non-discussion, though still calls for its own explanation. I think it suggests that concerns about uncooperativeness play a part, and the same foror thinking of slowing down AI as centrally involving antisocial strategies.
</li>
&lt;/ol&gt;
</li>
&lt;/ol&gt;
<p>
I’m not sure if any of this fully resolves why AI safety people haven’t thought about slowing down AI more, or whether people should try to do it. But my sense is that many of the above reasons are at least somewhat wrong, and motives somewhat misguided, so I want to argue about a lot of them in turn, including both arguments and vague motivational themes.
</p>
<h2>The mundanity of the proposal</h2>


<h3>Restraint is not radical</h3>


<p>
There seems to be a common thought that technology is a kind of inevitable path along which the world must tread, and that trying to slow down or avoid any part of it would be both futile<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup> and extreme. 
</p>
<p>
But empirically, the world doesn’t pursue every technology—it barely pursues any technologies.
</p>
<h4>Sucky technologies</h4>


<p>
For a start, there are many machines that there is no pressure to make, because they have no value. Consider a machine that sprays shit in your eyes. We can technologically do that, but probably nobody has ever built that machine. 
</p>
<p>
This might seem like a stupid example, because no serious ‘technology is inevitable’ conjecture is going to claim that totally pointless technologies are inevitable. But if you are sufficiently pessimistic about AI, I think this is the right comparison: if there are kinds of AI that would cause huge net costs to their creators if created, according to our best understanding, then they are at least as useless to make as the ‘spray shit in your eyes’ machine. We might accidentally make them due to error, but there is not some deep economic force pulling us to make them. If unaligned superintelligence destroys the world with high probability when you ask it to do a thing, then this is the category it is in, and it is not strange for its designs to just rot in the scrap-heap, with the machine that sprays shit in your eyes and the machine that spreads caviar on roads.
</p>
<p>
Ok, but maybe the relevant actors are very committed to being wrong about whether unaligned superintelligence would be a great thing to deploy. Or maybe you think the situation is less immediately dire and building existentially risky AI <a href="https://aiimpacts.org/incentives-to-create-x-risky-ai-systems/">really would</a> be a good move for the people making decisions (e.g. because the costs won’t arrive for a while, and the people care a lot about a shot at scientific success relative to a chunk of the future). If the apparent economic incentives are large, are technologies unavoidable?
</p>
<h4>Extremely valuable technologies</h4>


<p>
It doesn’t look like it to me. Here are a few technologies which I’d guess have substantial economic value, where research progress or uptake appears to be drastically slower than it could be, for reasons of concern about safety or ethics<sup id="fnref3"><a href="#fn3" rel="footnote">3</a></sup>:
</p>
<ol>

<li>Huge amounts of medical research, including really important medical research e.g. The FDA <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7326309/#:~:text=Nonetheless%2C%20in%201978%20the%20controversy%20resulted%20in%20a%20US%20FDA%20ban%20on%20subsequent%20vaccine%20trials%20which%20was%20eventually%20overturned%2030%20years%20later.">banned</a> human trials of strep A vaccines from the 70s to the 2000s, in spite of <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6474463/#:~:text=Worldwide%2C%20the%20death%20toll%20is%20estimated%20at%20500%20000%20annually">500,000 global deaths every year</a>. A lot of people also died while covid vaccines went through all the proper trials. 

<li>Nuclear energy

<li>Various genetics things: genetic modification of foods, gene drives

<li>Nuclear, biological, and maybe chemical weapons (or maybe these just aren’t useful)

<li>Various human reproductive innovation: cloning of humans, genetic manipulation of humans (a notable example of an economically valuable technology that is to my knowledge barely pursued across different countries, without explicit coordination between those countries, even though it would make those countries more competitive. Someone used CRISPR on babies in China, but was <a href="https://www.science.org/content/article/creator-crispr-babies-nears-release-prison-where-does-embryo-editing-stand">imprisoned</a> for it.)

<li>Recreational drug development

<li>All of science? I recently ran <a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/">this survey</a>, and was reminded how encumbering ethical rules are for even incredibly innocuous research. As far as I could tell the EU now makes it illegal to collect data in the EU unless you promise to delete the data from anywhere that it might have gotten to if the person who gave you the data wishes for that at some point. In all, dealing with this and IRB-related things added maybe more than half of the effort of the project. Possibly I misunderstand the rules, but I doubt other researchers are radically better at figuring them out what the rules are than I am.

<li>There are probably examples from fields considered distasteful or embarrassing to associate with, but it’s hard as an outsider to tell which fields are genuinely hopeless versus erroneously considered so. If there are economically valuable health interventions among those considered wooish, I imagine they would be much slower to be identified and pursued by scientists with good reputations than a similarly promising technology not marred in that way. Scientific research into intelligence is more clearly slowed by stigma, but it is less clear to me what the economically valuable upshot would be.
</li>
&lt;/ol&gt;
<p>
It seems to me that intentionally slowing down progress in technologies to give time for even probably-excessive caution is relatively commonplace.
</p>
<p>
Furthermore, among valuable technologies that nobody is especially trying to slow down, it seems common enough for progress to be massively slowed by relatively minor obstacles, which is further evidence for a lack of overpowering strength of the economic forces at play. For instance, Fleming first took notice of the mold's effect on bacteria in 1928, but nobody took a serious, high-effort shot at developing it as a drug until 1939<sup id="fnref4"><a href="#fn4" rel="footnote">4</a></sup>. Furthermore, in the thousands of years preceding these events, <a href="https://en.wikipedia.org/wiki/History_of_penicillin#Early_history">various</a> <a href="https://en.wikipedia.org/wiki/History_of_penicillin#Early_scientific_evidence">people</a> noticed numerous times that mold, other fungi or plants inhibited bacterial growth, but didn’t exploit this observation even enough for it not to beso much that it wasn’t considered a new discovery in the 1920s. Meanwhile, people dying of infection was quite a thing. In the 20s and 30s syphilis alone was killing something like <a href="https://aiimpacts.org/penicillin-and-historic-syphilis-trends/#US_Deaths_from_syphilis">15,000</a> Americans per year.
</p>
<p>
My guess is that people make real choices about technology, and they do so in the face of economic forces that are feebler than commonly thought. 
</p>
<h3>Restraint is not terrorism, usually</h3>


<p>
I think people have historically imagined weird things when they think of ‘slowing down AI’. I posit that their central image is sometimes terrorism (which understandably they don’t want to think about for very long), and sometimes some sort of implausibly utopian global agreement.
</p>
<p>
Here are some things that ‘slow down AI capabilities’ could look like (where the best positioned person to carry out each one differs, but if you are not that person, you could e.g. talk to someone who is):
</p>
<ol>

<li>Don’t actively forward AI progress, e.g. by devoting your life or billions of dollars to it

<li>Forward some research over other research. Pursue projects that lead to understanding before projects that lead to large but poorly understood capabilities, e.g. theory before scaling (see <a href="https://forum.effectivealtruism.org/posts/g6549FAQpQ5xobihj/differential-technological-development">differential technological development</a> in general).

<li>Formulate and coordinate on precautions for AI researchers and labs to take in different future situations. These could include more intense vetting, modifying experiments, or pausing lines of inquiry.

<li>Reduce available compute for undesired AI, e.g. via regulation, seller choices, purchasing compute.

<li>At labs, choose policies that slow down other labs, e.g. reduce public helpful research outputs

<li>Alter the publishing system and incentives to reduce research dissemination. E.g. A journal verifies research results and releases the fact of their publication without any details, maintains records of research priority for later release, and distributes funding for participation. (This is how Szilárd and co. arranged the mitigation of 1940s nuclear research helping Germany, except I’m not sure if the compensatory funding idea was used.<sup id="fnref5"><a href="#fn5" rel="footnote">5</a></sup>)

<li>The above actions would be taken through choices made by scientists, or funders, or legislators, or labs, or public observers, etc. Communicate with those parties, or help them act. For instance, <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities?commentId=JqT3t2jyj3NXDFkDr">produce compelling demos of risk, agitate for stigmatization of risky actions</a>, write science fiction to illustrate the problems broadly and evocatively. 
<h3>Coordination is not miraculous world government, usually</h3>


<p>
The common image of coordination seems to be explicit, centralized, involving of every party in the world, and something like cooperating on a prisoners’ dilemma: incentives push every rational party toward defection at all times, yet maybe through deontological virtues or sophisticated decision theories or strong international treaties, everyone manages to not defect for enough teetering moments to find another solution.
</p>
<p>
That is a possible way coordination could be. But if what you want is for lots of people to coincide in doing one thing when they might have done another, then there are quite a few ways of achieving that. 
</p>
<p>
Consider some other case studies of coordinated behavior:
</p>
<ul>

<li><strong>Not eating sand.</strong> The whole world coordinates to barely eat any sand at all. How do they manage it? It is actually not in almost anyone’s interest to eat sand, so the mere maintenance of sufficient epistemological health to have this widely recognized does the job.

<li><strong>Eschewing bestiality: </strong>probably some people think bestiality is moral, but enough don’t that engaging in it would risk huge stigma. Thus the world coordinates fairly well on doing very little of it.

<li><strong>Not wearing Victorian attire on the streets: </strong>this is similar but with no moral blame involved. Historic garb is arguably often more aesthetically pleasing than modern, but even people who strongly agree find it unthinkable to wear it in general, and assiduously avoid it except for when they have ‘excuses’ such as a special party. This is a very strong coordination against what appears to otherwise be a ubiquitous incentive (to look more aesthetically pleasing). As far as I can tell, it’s powered substantially by the fact that it is ‘not done’ and would now be weird to do otherwise. (Which is a very general-purpose mechanism.)

<li><strong>Political correctness:</strong> public discourse has strong norms about what it is okay to say, which do not appear to derive from a vast majority of people agreeing about this (as with bestiality say). New ideas about what constitutes being politically correct sometimes spread widely. This coordinated behavior seems to be roughly due to decentralized application of social punishment, from both a core of proponents, and from people who fear punishment for not punishing others. Then maybe also from people who are concerned by non-adherence to what now appears to be the norm given the actions of the others. This differs from the above examples, because it seems like it could persist even with a very small set of people agreeing with an object-level norm. If failing to advocate for the norm gets you publicly shamed by advocates, then you might tend to advocate for it, making the pressure stronger for everyone else. 
</li>
&lt;/ul&gt;
</li>
&lt;/ol&gt;
<p>
These are all cases of very broadscale coordination of behavior, none of which involve prisoners’ dilemma type situations, or people making explicit agreements which they then have an incentive to break. They do not involve centralized organization of huge multilateral agreements. Coordination can come from everyone individually wanting to make a certain choice for correlated reasons, or from people wanting to do things that those around them are doing, or from distributed behavioral dynamics such as punishment of violations, or from collaboration in thinking about a topic.
</p>
<p>
In the case of AI, if existential risk from it is in fact high, the situation in theory looks like that of avoiding eating sand. It’s an option that a rational person wouldn’t want to take if they were just alone and not facing any kind of multi-agent situation. If AI is that bad, then not taking this inferior option could come from a coordination mechanism as simple as distribution of good information. But even failing coordination from insight into the situation, still other models might work. For instance, if there came to be widespread concern that this is unethical or questionable research to partake in, that alone might substantially lessen participation in it and perhaps lead to a wide crop of regulation. Such regulation wouldn’t need to be centrally organized, as long as it grew up in different places similarly strongly. And all rational governments should be similarly concerned about losing power to automated power-seeking systems with unverifiable goals.  
</p>
<h2>The arms race model and its alternatives</h2>


<p>
Ok, maybe in principle you might hope to coordinate to not do self-destructive things, but realistically, if the US tries to slow down, won’t China or Facebook or someone less cautious take over the world? 
</p>
<p>
Let’s be more careful about the game we are playing, game-theoretically speaking.
</p>
<p>
<strong>The arms race</strong>
</p>
<p>
What is an arms race, game theoretically? It’s an iterated prisoners’ dilemma. Each step looks something like this:
</p>
<p>


<p id="gdcalert1"><span style="color: red; font-weight: bold">&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image1.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br />(<a href="#">Back to top</a>)(<a href="#gdcalert2">Next alert</a>)<br /><span style="color: red; font-weight: bold">&gt;&gt;&gt;&gt;&gt; </span></p>


<img src="images/image1.png" width="" alt="alt_text" title="image_tooltip" />

</p>
<p>
In this example, building weapons costs one unit. Alice having more weapons than Bob results in Alice taking ten units of value from Bob. And vice versa. It’s always better to build weapons than not (unless you think your opponent’s actions are logically tied to yours or something next level), and it’s always better to get the hell out of this game.
</p>
<p>
This is not at all what the current situation looks like, if you think AI poses a substantial risk of destroying the world.
</p>
<p>
<strong>The suicide race</strong>
</p>
<p>
A closer model: as above except if anyone chooses to build, everything is destroyed.
</p>
<p>


<p id="gdcalert2"><span style="color: red; font-weight: bold">&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image2.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br />(<a href="#">Back to top</a>)(<a href="#gdcalert3">Next alert</a>)<br /><span style="color: red; font-weight: bold">&gt;&gt;&gt;&gt;&gt; </span></p>


<img src="images/image2.png" width="" alt="alt_text" title="image_tooltip" />

</p>
<p>
This is importantly different from the classic ‘arms race’ in that pressing the ‘everyone loses now’ button isn’t an equilibrium strategy. That is, for anyone who thinks powerful misaligned AI is certain death, the existence of other possible AI builders is not any reason to ‘race’. 
</p>
<p>
But almost nobody is that pessimistic—there’s at least some chance that the players ‘align the AI’, and then things should be ok. So let’s try a milder version—
</p>
<p>
<strong>Safety-or-suicide race</strong> 
</p>
<p>
Now suppose building strong AI just gets you a big risk of destroying everything, not definite doom. A very simple version of this: just like the last game, except that if anyone builds, there is only a 50% chance of everyone losing 10, and in the other 50% of cases alignment works out and everyone gets the normal arms race game payoffs instead. So if you build AI alone, and get lucky on the probabilistic apocalypse, can still win big. The expected payoffs come out at:
</p>
<p>
Let’s say for a simple example, if you unilaterally build then you have a 50% chance of destroying everything and a 50% chance of taking the other person’s stuff, as in the original arms race. If you both build, you have a 50% chance of destroying everything, and 50% of not. (And building still costs 1 unit.)
</p>

<table>
  <tr>
   <td>
   </td>
   <td>pass
   </td>
   <td>build
   </td>
  </tr>
  <tr>
   <td>pass
   </td>
   <td>0,0
   </td>
   <td>-10, -1
   </td>
  </tr>
  <tr>
   <td>build
   </td>
   <td>-1, -10
   </td>
   <td>-6, -6
   </td>
  </tr>
</table>


<p>
Now you want to do whatever the other player is doing: build if they’ll build, pass if they’ll pass. 
</p>
<p>
If the odds of destroying the world were very low, this would become the original arms race, and you’d always want to build. If very high, it would become the suicide race, and you’d never want to build. What the probabilities have to be in the real world to get you into something like these different phases is going to be different, because all these parameters are made up. But my point stands: in terms of simplish models, it’s very non-obvious that we are in or near an arms race. And therefore, very non-obvious that racing to build advanced AI faster is even promising at a first pass.
</p>
<p>
In less game-theoretic terms: if you don’t seem anywhere near solving alignment, then racing as hard as you can to be the one who it falls upon to have solved alignment—especially if that means having less time to do so, though I haven’t discussed that here—is probably unstrategic. Having more ideologically pro-safety AI designers win an ‘arms race’ against less concerned teams is futile if you don’t have a way for such people to implement enough safety to actually not die, which seems like a very live possibility. (<a href="https://twitter.com/robbensinger/status/1536954285040119809">Robby Bensinger</a> and maybe Andrew Critch somewhere make similar points.)
</p>
<p>
Conversations with my friends on this kind of topic can go like this:
</p>
<p>

    <strong>Me</strong>: there’s no real incentive to race if the prize is mutual death
</p>
<p>

    <strong>Them</strong>: sure, but it isn’t—if there’s a sliver of hope of surviving unaligned AI, and if your side taking control in that case is a bit better in expectation, and if they are going to build powerful AI anyway, then it’s worth racing. The whole future is on the line!
</p>
<p>

    <strong>Me:</strong> Wouldn’t you still be better off directing your own efforts to safety, since your safety efforts will also help everyone end up with a safe AI? 
</p>
<p>

    <strong>Them</strong>: It will probably only help them somewhat—you don’t know if the other side will use your safety research. But also, it’s not just that they have less safety research. Their values are probably worse, by your lights. 
</p>
<p>

    <strong>Me</strong>: If they succeed at alignment, are foreign values really worse than local ones? Probably any humans with vast intelligence at hand have a similar shot at creating a glorious human-ish utopia, no?
</p>
<p>

    <strong>Them</strong>: No, even if you’re right that being similarly human gets you to similar values in the end, the other parties might be more foolish than our side, and lock-in some poorly thought-through version of their values that they want at the moment, or even if all projects would be so foolish, our side might have better poorly thought-through values to lock in, as well as being more likely to use safety ideas at all. Even if racing is very likely to lead to death, and survival is very likely to lead to squandering most of the value, in that sliver of happy worlds so much is at stake in whether it is us or someone else doing the squandering!
</p>
<p>

    <strong>Me</strong>: Hmm, seems complicated, I’m going to need paper for this.
</p>
<p>
<strong>The complicated race/anti-race</strong>
</p>
<p>
<a href="https://docs.google.com/spreadsheets/d/1tq5jiok33sh89xQxLSBIZQGiuQ8hc8_n0673YU68poM/edit?usp=sharing">Here</a> is a spreadsheet of models you can make a copy of and play with.
</p>
<p>
The first model is like this:
</p>
<ol>

<li>Each player divides their effort between safety and capabilities

<li>One player ‘wins’, i.e. builds ‘AGI’ (artificial general intelligence) first. 

<li>P(Alice wins) is a logistic function of Alice’s capabilities investment relative to Bob’s

<li>Each players’ total safety is their own safety investment plus a fraction of the other’s safety investment.

<li>For each player there is some distribution of outcomes if they achieve safety, and a set of outcomes if they do not, which takes into account e.g. their proclivities for enacting stupid near-term lockins.

<li>The outcome is a distribution over winners and states of alignment, each of which is a distribution of worlds (e.g. utopia, near-term good lockin..)

<li>That all gives us a number of utils (Delicious utils!)
</li>
&lt;/ol&gt;
<p>
The second model is the same except that instead of dividing effort between safety and capabilities, you choose a speed, and the amount of alignment being done by each party is an exogenous parameter. 
</p>
<p>
These models probably aren’t very good, but so far support a key claim I want to make here: it’s pretty non-obvious whether one should go faster or slower in this kind of scenario—it’s sensitive to a lot of different parameters in plausible ranges. 
</p>
<p>
Furthermore, I don’t think the results of quantitative analysis match people’s intuitions here.
</p>
<p>
For example, here’s a situation which I think sounds intuitively like a you-should-race world, but where in the first model above, you should actually go as slowly as possible (this should be the one plugged into the spreadsheet now):
</p>
<ul>

<li><strong>AI is pretty safe:</strong> unaligned AGI has a mere 7% chance of causing doom, plus a further 7% chance of causing short term lock-in of something mediocre

<li><strong>Your opponent risks bad lock-in:</strong> If there’s a ‘lock-in’ of something mediocre, your opponent has a 5% chance of locking in something actively terrible, whereas you’ll always pick good mediocre lock-in world (and mediocre lock-ins are either 5% as good as utopia, -5% as good)

<li><strong>Your opponent risks messing up utopia:</strong> In the event of aligned AGI, you will reliably achieve the best outcome, whereas your opponent has a 5% chance of destroying everything then too

<li><strong>Safety investment obliterates your chance of getting to AGI first: </strong>moving from no safety at all to full safety means you go from a 50% chance of being first to a 0% chance

<li><strong>Your opponent is racing: </strong>Your opponent is investing everything in capabilities and nothing in safety

<li><strong>Safety work helps others at a steep discount:</strong>  your safety work contributes 50% to the other player’s safety 
</li>
&lt;/ul&gt;
<p>
Your best bet here (on this model) is still to maximize safety investment. Why? Because by aggressively pursuing safety, you can get the other side half way to full safety, which is worth a lot more than than the lost chance of winning. Especially since if you ‘win’, you do so without much safety, and your victory without safety is worse than your opponent’s victory with safety, even if that too is far from perfect.
</p>
<p>
So if you are in a situation in this space, and the other party is racing, it’s not obvious if it is even in your narrow interests within the game to go faster at the expense of safety, though it may be.
</p>
<p>
These models are flawed in many ways, but I think they are better than the intuitive models that support arms-racing. My guess is that the next better still models remain nuanced.
</p>
<h4><em>Other equilibria and other games</em></h4>


<p>
Even if it would be in your interests to race if the other person were racing, ‘(do nothing, do nothing)’ is often an equilibrium too in these games. At least for various settings of the parameters. It doesn’t necessarily make sense to do nothing in the hope of getting to that equilibrium if you know your opponent to be mistaken about that and racing anyway, but in conjunction with communicating with your ‘opponent’, it seems like a theoretically good strategy.
</p>
<p>
This has all been assuming the structure of the game. I think the traditional response to an arms race situation is to remember that you are in a more elaborate world with all kinds of unmodeled affordances, and try to get out of the arms race. 
</p>
<h2>Being friends with risk-takers</h2>


<h3>Caution is cooperative</h3>


<p>
Another big concern is that pushing for slower AI progress is ‘defecting’ against AI researchers who are friends of the AI safety community. 
</p>
<p>
For instance:
</p>
<p>

    <a href="https://www.lesswrong.com/posts/CXaQj85r4LtafCBi8/should-we-postpone-agi-until-we-reach-safety?commentId=w2vSA2cPvE8hR563Z">Steven Byrnes</a>:
</p>
<p>

    “I think that trying to slow down research towards AGI through regulation would fail, because everyone (politicians, voters, lobbyists, business, etc.) likes scientific research and technological development, it creates jobs, it cures diseases, etc. etc., and you're saying we should have less of that. So I think the effort would fail, and also be massively counterproductive by making the community of AI researchers see the community of AGI safety / alignment people as their enemies, morons, weirdos, Luddites, whatever.”
</p>
<p>
(Also a good example of the view criticized earlier, that regulation of things that create jobs and cure diseases just doesn’t happen.)
</p>
<p>

    <a href="https://twitter.com/ESYudkowsky/status/1555272014939598848?s=20&amp;t=H6JiYdHs7kAnoaZshPV5sg">Eliezer Yudkowsky</a>:
</p>
<p>

    [On worry that spreading fear about AI would alienate top AI labs] “This is the primary reason I didn't, and told others not to, earlier connect the point about human extinction from AGI with AI labs.  Kerry has correctly characterized the position he is arguing against, IMO.  I myself estimate the public will be toothless vs AGI lab heads.”
</p>
<p>
I don’t think this is a natural or reasonable way to see things, because:
</p>
<ol>

<li>The researchers themselves probably don’t want to destroy the world. Many of them also actually <a href="https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/">agree</a> that AI is a serious existential risk. So in two natural ways, pushing for caution is cooperative with many if not most AI researchers.

<li>AI researchers do not have a moral right to endanger the world, that someone would be stepping on by requiring that they move more cautiously. Like, why does 'cooperation' look like the safety people bowing to what the more reckless capabilities people want, to the point of fearing to represent their actual interests, while the capabilities people uphold their side of the ‘cooperation’ by going ahead and building dangerous AI? This situation might make sense as a natural consequence of different people’s power in the situation. But then don’t call it a ‘cooperation’, from which safety-oriented parties would be dishonorably ‘defecting’ were they to consider exercising any power they did have. 
</li>
&lt;/ol&gt;
<p>
It could be that people in control of AI capabilities would respond negatively to AI safety people pushing for slower progress. But that should be called ‘we might get punished’ not ‘we shouldn’t defect’. ‘Defection’ has moral connotations that are not due. Calling one side pushing for their preferred outcome ‘defection’ unfairly disempowers them by wrongly setting commonsense morality against them.
</p>
<p>
At least if it is the safety side. If any of the available actions are ‘defection’ that the world in general should condemn, I claim that it is probably ‘building machines that will plausibly destroy the world, or standing by while it happens’. 
</p>
<p>
(This would be more complicated if the people involved were confident that they wouldn’t destroy the world and I merely disagreed with them. But <a href="https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/">about half of surveyed researchers</a> are actually more pessimistic than me. And in a situation where the median AI researcher thinks the field has a 5-10% chance of causing human extinction, how confident can any responsible person be in their own judgment that it is safe?)  
</p>
<p>
On top of all that, I worry that highlighting the narrative that wanting more cautious progress is defection is further destructive, because it makes it more likely that AI capabilities people see AI safety people as thinking of themselves as betraying AI researchers, if anyone engages in any such efforts. Which makes the efforts more aggressive. Like, if every time you see friends, you refer to it as ‘cheating on my partner’, your partner may reasonably feel hurt by your continual desire to see friends, even though the activity itself is innocuous.
</p>
<h3>‘We’ are not the US, ‘we’ are not the AI safety community</h3>


<p>
“If ‘we’ try to slow down AI, then the other side might win.” “If ‘we’ ask for regulation, then it might harm ‘our’ relationships with AI capabilities companies.” Who are these ‘we’s? Why are people strategizing for those groups in particular? 
</p>
<p>
Even if slowing AI were uncooperative, and it were important for the AI Safety community to cooperate with the AI capabilities community, couldn’t one of the many people not in the AI Safety community work on it? 
</p>
<p>
I have a <a href="https://meteuphoric.com/2010/08/23/who-are-we/">longstanding irritation</a> with thoughtless talk about what ‘we’ should do, without regard for what collective one is speaking for. So I may be too sensitive about it here. But I think confusions arising from this have genuine consequences.
</p>
<p>
I think when people say ‘we’ here, they generally imagine that they are strategizing on behalf of, a) the AI safety community, b) the USA, c) themselves or d) they and their readers. But those are a small subset of people, and not even obviously the ones the speaker can most influence (does the fact that you are sitting in the US really make the US more likely to listen to your advice than e.g. Estonia? Yeah probably on average, but not infinitely much.) If these naturally identified-with groups don’t have good options, that hardly means there are no options to be had, or to be communicated to other parties. Could the speaker speak to a different ‘we’? Maybe someone in the ‘we’ the speaker has in mind knows someone not in that group? If there is a strategy for anyone in the world, and you can talk, then there is probably a strategy for you.
</p>
<p>
The starkest appearance of error along these lines to me is in writing off the slowing of AI as inherently destructive of relations between the AI safety community and other AI researchers. If we grant that such activity would be seen as a betrayal (which seems unreasonable to me, but maybe), surely it could only be a betrayal if carried out by the AI safety community. There are quite a lot of people who aren’t in the AI safety community and have a stake in this, so maybe some of them could do something. It seems like a huge oversight to give up on all slowing of AI progress because you are only considering affordances available to the AI Safety Community. 
</p>
<p>
Another example: if the world were in the basic arms race situation sometimes imagined, and the United States would be willing to make laws to mitigate AI risk, but could not because China would barge ahead, then that means China is in a great place to mitigate AI risk. Unlike the US, China could propose mutual slowing down, and the US would go along. Maybe it’s not impossible to communicate this to relevant people in China. 
</p>
<p>
An oddity of this kind of discussion which feels related is the persistent assumption that one’s ability to act is restricted to the United States. Maybe I fail to understand the extent to which Asia is an alien and distant land where agency doesn’t apply, but for instance I just wrote to like a thousand machine learning researchers there, and maybe a hundred wrote back, and it was a lot like interacting with people in the US.
</p>
<p>
I’m pretty ignorant about what interventions will work in any particular country, including the US, but I just think it’s weird to come to the table assuming that you can essentially only affect things in one country. Especially if the situation is that you believe you have unique knowledge about what is in the interests of people in other countries. Like, fair enough I would be deal-breaker-level pessimistic if you wanted to get an Asian government to elect you leader or something. But if you think advanced AI is highly likely to destroy the world, including other countries, then the situation is totally different. If you are right, then everyone’s incentives are basically aligned. 
</p>
<p>
I more weakly suspect some related mental shortcut is misshaping the discussion of arms races in general. The thought that something is a ‘race’ seems much stickier than alternatives, even if the true incentives don’t really make it a race. Like, against the laws of game theory, people sort of expect the enemy to try to believe falsehoods, because it will better contribute to their racing. And this feels like realism. The uncertain details of billions of people one barely knows about, with all manner of interests and relationships, just really wants to form itself into an ‘us’ and a ‘them’ in zero-sum battle. This is a mental shortcut that could really kill us.
</p>
<p>
My impression is that in practice, for many of the technologies slowed down for risk or ethics, mentioned above, countries with fairly disparate cultures have converged on similar approaches to caution. I take this as evidence that none of ethical thought, social influence, political power, or rationality are actually very siloed by country, and in general the ‘countries in contest’ model of everything isn’t very good.
</p>
<h2>Notes on tractability</h2>


<h3>Convincing people doesn’t seem that hard</h3>


<p>
When I say that ‘coordination’ can just look like popular opinion punishing an activity, or that other countries don’t have much real incentive to build machines that will kill them, I think a common objection is that convincing people of the real situation is hopeless. The picture seems to be that the argument for AI risk is extremely sophisticated and only able to be appreciated by the most elite of intellectual elites—e.g. it’s hard enough to convince professors on Twitter, so surely the masses are beyond its reach, and foreign governments too. 
</p>
<p>
This doesn’t match my overall experience on various fronts.
</p>
<p>
Some observations:
</p>
<ul>

<li>The <a href="https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/">median surveyed ML researcher</a> seems to think AI will destroy humanity with 5-10% chance, as I mentioned

<li><a href="https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like">Often people are already intellectually convinced</a> but haven’t integrated that into their behavior, and it isn’t hard to help them organize to act on their tentative beliefs

<li>As noted by <a href="https://astralcodexten.substack.com/p/why-not-slow-ai-progress">Scott</a>, a lot of AI safety people have gone into AI capabilities including running AI capabilities orgs, so those people presumably consider AI to be risky already

<li>I don’t remember ever having any trouble discussing AI risk with random strangers. Sometimes they are also fairly worried (e.g. a makeup artist at Sephora gave an extended rant about the dangers of advanced AI, and my driver in Santiago excitedly concurred and showed me <em><a href="https://en.wikipedia.org/wiki/Homo_Deus:_A_Brief_History_of_Tomorrow">Homo Deus</a></em> open on his front seat). The form of the concerns are probably a bit different from those of the AI Safety community, but I think broadly closer to, ‘AI agents are going to kill us all’ than ‘algorithmic bias will be bad’. I can’t remember how many times I have tried this, but pre-pandemic I used to talk to Uber drivers a lot, due to having no idea how to avoid it. I explained AI risk to my therapist recently, as an aside regarding his sense that I might be catastrophizing, and I feel like it went okay, though we may need to discuss again. 

<li>My impression is that most people haven’t even come into contact with the arguments that might bring one to agree precisely with the AI safety community. For instance, my guess is that a lot of people assume that someone actually programmed modern AI systems, and if you told them that in fact they are random connections jiggled in an gainful direction unfathomably many times, just as mysterious to their makers, they might also fear misalignment. 

<li>Nick Bostrom, Eliezer Yudkokwsy, and other early thinkers have had decent success at convincing a bunch of other people to worry about this problem, e.g. me. And to my knowledge, without writing any compelling and accessible account of why one should do so that would take less than two hours to read.

<li>I arrogantly think I could write a broadly compelling and accessible case for AI risk
</li>
&lt;/ul&gt;
<p>
My weak guess is that immovable AI risk skeptics are concentrated in intellectual circles near the AI risk people, especially on Twitter, and that people with less of a horse in the intellectual status race are more readily like, ‘oh yeah, superintelligent robots are probably bad’. I’m pretty non-confident about this, but I could run a survey if it was interesting to people, or probably there already is one.
</p>
<h3>The people you need to convince are probably not that many</h3>


<p>
I could be wrong, but I’d guess reality 
</p>
<h3>Buying time is big</h3>


<p>
You probably aren’t going to avoid AGI forever, and maybe huge efforts will buy you a couple of years<sup id="fnref6"><a href="#fn6" rel="footnote">6</a></sup>. Could that even be worth it? 
</p>
</li> 
&lt;/ul&gt;
<p>
Seems pretty plausible:
</p>
<ol>

<li>Whatever kind of other AI safety research or policy you were doing could be happening at a non-negligible rate per year.

<li>Geopolitics just changes pretty often. If you seriously think a big determiner of how badly things go is inability to coordinate with certain groups, then every year gets you non-negligible opportunities for the situation changing in a favorable way. 

<li>Other stuff happens over time. If you can take your doom today or after a couple of years of random shit happening, the latter seems non-negligibly better in general.
</li>
&lt;/ol&gt;
<p>
It is also not obvious to me that these are the time-scales on the table. My sense is that things which are slowed down by regulation or general societal distaste are often slowed down much more than a year or two, and Eliezer’s stories presume that the world is full of collectives either trying to destroy the world or badly mistaken about it, which is not a foregone conclusion.
</p>
<h3>Delay is probably finite by default </h3>


<p>
While some people worry that any delay would be so short as to be negligible, others seem to fear that if AI research were halted, it would never start again and we would fail to go to space or something. This sounds so wild to me that I think I’m missing too much of the reasoning to usefully counterargue.
</p>
<h2>Safety from speed</h2>


<p>
Maybe it’s actually better for safety to have AI go fast at present, for various reasons. Notably:
</p>
<ol>

<li>Implementing what can be implemented as soon as possible probably means smoother progress, which is probably safer

<li>If the main thing achieved by slowing down AI progress is more time for safety research, and safety research is more effective when carried out in the context of more advanced AI, and there is a certain amount of slowing down that can be done (e.g. because one is in fact in an arms race but has some lead over competitors), then it might better to use one’s slowing budget later

<li>(More things go here I think)
</li>
&lt;/ol&gt;
<p>
This seems plausible. I don’t know of a good analysis of these considerations, and am not going to do one here. My impression is that it could go either way.
</p>
<h2>Moods and philosophies, heuristics and attitudes </h2>


<p>
It’s not clear what role these psychological characters should play in a rational assessment of how to act, but I think they do play a role, so I want to argue about them.
</p>
<h3>Technological choice is not luddism</h3>


<p>
Some technologies are better than others [citation not needed]. The best pro-technology visions should disproportionately involve awesome technologies and avoid shitty technologies, I claim. If you think AGI is highly likely to destroy the world, then it is the pinnacle of shittiness as a technology. Being opposed to having it into your techno-utopia is about as luddite as refusing to have <a href="https://en.wikipedia.org/wiki/Doramad_Radioactive_Toothpaste">radioactive toothpaste</a> there. Colloquially, Luddites are against progress if it comes as technology<sup id="fnref7"><a href="#fn7" rel="footnote">7</a></sup>. Even if that’s a terrible position, its wise reversal is not the endorsement of all ‘technology’, regardless of whether it comes as progress.
</p>
<h3>Non-AGI visions of near-term thriving</h3>


<p>
Perhaps slowing down AI progress means foregoing our own generation’s hope for life-changing technologies. Some people thus find it psychologically difficult to aim for less AI progress (with its real personal costs), rather than shooting for the perhaps unlikely ‘safe AGI soon’ scenario.
</p>
<p>
I’m not sure that this is a real dilemma. The narrow AI progress we have seen already—further applications of current techniques at current scales—seems plausibly able to help a lot with longevity and other medicine for instance. And to the extent AI efforts could be focused on e.g. medically relevant narrow systems over creating agentic scheming gods, it doesn’t sound crazy to imagine making more progress on anti-aging etc as a result (even before taking into account the probability that the agentic scheming god does not prioritize your physical wellbeing as hoped).
</p>
<h3>Robust priors vs. specific galaxy-brained models</h3>


<p>
There are things that are robustly good in the world, and things that are good on highly specific inside-view models and terrible if those models are wrong. Slowing dangerous tech development seems like the former, whereas forwarding arms races for dangerous tech between world superpowers seems more like the latter. There is a general question of how much to trust your reasoning and risk the galaxy-brained plan<sup id="fnref8"><a href="#fn8" rel="footnote">8</a></sup>. But whatever your take on that, I think we should all agree that the less thought you have put into it, the more you should regress to the robustly good actions. Like, if it just occurred to you to take out a large loan to buy a fancy car, you probably shouldn’t do it because most of the time it’s a poor choice. Whereas if you have been thinking about it for a month, you might be sure enough that you are in the rare situation where it will pay off. 
</p>
<p>
On this particular topic, it feels like people are going with the specific galaxy-brained inside-view terrible-if-wrong model off the bat, then not thinking about it more. 
</p>
<h3><em>Cheems mindset/can’t do attitude</em></h3>


<p>
Suppose you have a friend, and you say ‘let’s go to the beach’ to them. Sometimes the friend is like ‘hell yes’ and then even if you don’t have towels or a mode of transport or time or a beach, you make it happen. Other times, even if you have all of those things, and your friend nominally wants to go to the beach, they will note that they have a package coming later, and that it might be windy, and their jacket needs washing. And when you solve those problems, they will note that it’s not that long until dinner time. You might infer that in the latter case your friend just doesn’t want to go to the beach. And sometimes that is the main thing going on! But I think there are also broader differences in attitudes: sometimes people are looking for ways to make things happen, and sometimes they are looking for reasons that they can’t happen. This is sometimes called a ‘<a href="https://normielisation.substack.com/p/cheems-mindset">cheems attitude</a>’, or I like to call it (more accessibly) a ‘can’t do attitude’.
</p>
<p>
My experience in talking about slowing down AI with people is that they seem to have a can’t do attitude. They don’t want it to be a reasonable course: they want to write it off. 
</p>
<p>
Which both seems suboptimal, and is strange in contrast with historical attitudes to more technical problem-solving. (As highlighted in my dialogue from the start of the post.)
</p>
<p>
It seems to me that if the same degree of can’t-do attitude were applied to technical safety, there would be no AI safety community because in 2005 Eliezer would have noticed any obstacles to alignment and given up and gone home.
</p>
<p>
To quote a friend on this, what would it look like if we *actually tried*?
</p>
<h2>Conclusion</h2>


<p>
I could go either way on whether any interventions to slow down AI in the near term were a good idea. But I think a lot of the opinions on the subject are poorly thought through, in error, and have wrongly repelled the further thought that might rectify them. I hope to have helped a bit here by examining some such considerations enough to show that there are no good grounds for immediate dismissal. There are difficulties and questions, but if the same standards for ambition were applied here as elsewhere, I think we would see answers and action.
</p>
<h3>Acknowledgements</h3>


<p>
Thanks Adam Scholl, Ronny Fernandez, Joe Carlsmith, Ben Weinstein-Raun, Owain Evans, Matthijs Maas, Aysja Johnson, Jaan Tallinn, Rick Korzekwa, Andrew Critch, Michael Vassar, Jessica Taylor, Rohin Shah, Jeffrey Heninger, Zach Stein-Perlman, Anthony Aguirre, Matthew Barnett, David Krueger, Harlan Stewart, Rafe Kennedy, Nick Beckstead, Leopold Aschenbrenner, Michaël Trazzi, Oliver Habryka, Shahar Avin, Luke Muehlhauser, Michael Nielsen, and many others including people people who have talked to me about this and related topics at parties and in Ubers and so on.
</p>

<!-- Footnotes themselves at the bottom. -->

<h2>Notes</h2>
<div class="footnotes">
<hr />
<ol><li id="fn1">

<p>
     I haven’t heard this in recent times, so maybe views have changed. An example of earlier times: <a href="https://blog.givewell.org/2015/09/30/differential-technological-development-some-early-thinking/">Nick Beckstead, 2015</a>: “One idea we sometimes hear is that it would be harmful to speed up the development of artificial intelligence because not enough work has been done to ensure that when very advanced artificial intelligence is created, it will be safe. This problem, it is argued, would be even worse if progress in the field accelerated. However, very advanced artificial intelligence could be a useful tool for overcoming other potential global catastrophic risks. If it comes sooner—and the world manages to avoid the risks that it poses directly—the world will spend less time at risk from these other factors....
<p>
    I found that speeding up advanced artificial intelligence—according to my simple interpretation of these survey results—could easily result in reduced net exposure to the most extreme global catastrophic risks...”&nbsp;<a href="#fnref1" rev="footnote">&#8617;</a><li id="fn2">
<p>
     For instance Nick Bostrom illustrates this kind of position (though apparently rejects it; from Superintelligence, found <a href="https://forum.effectivealtruism.org/posts/g6549FAQpQ5xobihj/differential-technological-development">here</a>): “Suppose that a policymaker proposes to cut funding for a certain research field, out of concern for the risks or long-term consequences of some hypothetical technology that might eventually grow from its soil. She can then expect a howl of opposition from the research community. Scientists and their public advocates often say that it is futile to try to control the evolution of technology by blocking research. If some technology is feasible (the argument goes) it will be developed regardless of any particular policymaker’s scruples about speculative future risks. Indeed, the more powerful the capabilities that a line of development promises to produce, the surer we can be that somebody, somewhere, will be motivated to pursue it. Funding cuts will not stop progress or forestall its concomitant dangers.”&nbsp;<a href="#fnref2" rev="footnote">&#8617;</a><li id="fn3">
<p>
     (Some inspiration from Matthijs Maas’ <a href="https://airtable.com/shrVHVYqGnmAyEGsz">spreadsheet</a>, from <a href="https://verfassungsblog.de/paths-untaken/">Paths Untaken</a>, and from GPT-3.)&nbsp;<a href="#fnref3" rev="footnote">&#8617;</a><li id="fn4">
<p>
     Private conversation with Rick Korzekwa, who may have read <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1139110/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1139110/</a> and an internal draft at <a href="aiimpacts.org/">AI Impacts</a>, probably forthcoming.&nbsp;<a href="#fnref4" rev="footnote">&#8617;</a><li id="fn5">

<p>
     “To aid in promoting secrecy, schemes to improve incentives were devised. One
<p>
    method sometimes used was for authors to send papers to journals to establish their claim
<p>
    to the finding but ask that publication of the papers be delayed indefinitely.26,27,28,29
<p>
    Szilárd also suggested offering funding in place of credit in the short term for scientists
<p>
    willing to submit to secrecy and organizing limited circulation of key papers.30” - <a href="https://intelligence.org/files/SzilardNuclearWeapons.pdf">Me, previously</a>&nbsp;<a href="#fnref5" rev="footnote">&#8617;</a><li id="fn6">
<p>
     Or so worries Eliezer Yudkowsky:
	 <a href="https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy">Eliezer (1</a>) 
<ul>
 
<li>“... this isn't primarily a social-political problem, of just getting people to listen.  Even if DeepMind listened, and Anthropic knew, and they both backed off from destroying the world, that would just mean Facebook AI Research destroyed the world a year(?) later.”
</li> 
</ul>

<li><a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">Eliezer (2</a>) 
<ul>
 
<li>“We can't just "decide not to build AGI" because GPUs are everywhere, and knowledge of algorithms is constantly being improved and published; 2 years after the leading actor has the capability to destroy the world, 5 other actors will have the capability to destroy the world.  The given lethal challenge is to solve within a time limit, driven by the dynamic in which, over time, increasingly weak actors with a smaller and smaller fraction of total computing power, become able to build AGI and destroy the world.  Powerful actors all refraining in unison from doing the suicidal thing just delays this time limit - it does not lift it, unless computer hardware and computer software progress are both brought to complete severe halts across the whole Earth.  The current state of this cooperation to have every big actor refrain from doing the stupid thing, is that at present some large actors with a lot of researchers and computing power are led by people who vocally disdain all talk of AGI safety (eg Facebook AI Research).  Note that needing to solve AGI alignment only within a time limit, but with unlimited safe retries for rapid experimentation on the full-powered system; or only on the first critical try, but with an unlimited time bound; would both be terrifically humanity-threatening challenges by historical standards individually.”&nbsp;<a href="#fnref6" rev="footnote">&#8617;</a><li id="fn7">
<p>
     I’d guess real Luddites also thought the technological changes they faced were anti-progress, but in that case were they wrong to want to avoid them?&nbsp;<a href="#fnref7" rev="footnote">&#8617;</a><li id="fn8">
<p>
      <a href="https://www.forourposterity.com/burkean-longtermism/">Leopold Aschenbrenner</a> partly defines ‘Burkean Longtermism’ thus: “We should be skeptical of any radical inside-view schemes to positively steer the long-run future, given the froth of uncertainty about the consequences of our actions.”&nbsp;<a href="#fnref8" rev="footnote">&#8617;</a>

&lt;/ol&gt;&lt;/div&gt;
</p></li></p></li></li></ul></li></p></li></p></p></p></p></p></li></p></li></p></li></p></li></p></p></li></ol></div></li></li></ol></li></li></ol></li></li></li></li></li></ul></li></ol></li></li></li></li></li></ul></li></li></li></li></li></li></ol></li></li></ul></li></li></li></li></li></li></li></ol></li></li></li></li></li></li></li></ol></li></li></li></li></li></ol></li></li></li></li></li></li></li></ol></li></li></ol></li></ol></li></ol>
</li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/10/14/ai_counterargs.html">
          Counterarguments to the basic AI risk case</a><span class="post-meta"> Oct 14, 2022</span>
      </h3><p><em>Crossposted from <a href="https://aiimpacts.org/counterarguments-to-the-basic-ai-x-risk-case/">The AI Impacts blog</a>.</em></p>

<p>This is going to be a list of holes I see in the basic argument for existential risk from superhuman AI systems<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>.</p>

<p>To start, here’s an outline of what I take to be the basic case<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup>:</p>

<blockquote>

  <p><strong>I. If superhuman AI systems are built, any given system is likely to be ‘goal-directed’</strong></p>

  <p>Reasons to expect this:</p>

  <ol><li>Goal-directed behavior is likely to be valuable, e.g. economically. </li><li>Goal-directed entities may tend to arise from machine learning training processes not intending to create them (at least via the methods that are likely to be used).</li><li>‘Coherence arguments’ may imply that systems with some goal-directedness will become more strongly goal-directed over time.</li></ol>

  <p><strong>II. If goal-directed superhuman AI systems are built, their desired outcomes will probably be about as bad as an empty universe by human lights </strong></p>

  <p>Reasons to expect this:</p>

  <ol><li>Finding useful goals that aren’t extinction-level bad appears to be hard: we don’t have a way to usefully point at human goals, and divergences from human goals seem likely to produce goals that are in intense conflict with human goals, due to a) most goals producing <a href="https://en.wikipedia.org/wiki/Instrumental_convergence">convergent incentives</a> for controlling everything, and b) value <a href="https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile">being</a> ‘fragile’, such that an entity with ‘similar’ values will generally create a future of virtually no value.</li><li>Finding goals that are extinction-level bad and temporarily useful appears to be easy: for example, advanced AI with the sole objective ‘maximize company revenue’ might profit said company for a time before gathering the influence and wherewithal to pursue the goal in ways that blatantly harm society.</li><li>Even if humanity found acceptable goals, giving a powerful AI system any specific goals appears to be hard. We don’t know of any procedure to do it, and we have theoretical reasons to expect that AI systems produced through machine learning training will generally end up with goals other than those they were trained according to. Randomly aberrant goals resulting are probably extinction-level bad for reasons described in II.1 above.</li></ol>

  <p><strong>III. If most goal-directed superhuman AI systems have bad goals, the future will very likely be bad</strong></p>

  <p>That is, a set of ill-motivated goal-directed superhuman AI systems, of a scale likely to occur, would be capable of taking control over the future from humans. This is supported by at least one of the following being true:</p>

  <ol><li><strong>Superhuman AI would destroy humanity rapidly. </strong>This may be via ultra-powerful capabilities at e.g. technology design and strategic scheming, or through gaining such powers in an ‘intelligence explosion‘ (self-improvement cycle). Either of those things may happen either through exceptional heights of intelligence being reached or through highly destructive ideas being available to minds only mildly beyond our own.</li><li><strong>Superhuman AI would gradually come to control the future via accruing power and resources.</strong> Power and resources would be more available to the AI system(s) than to humans on average, because of the AI having far greater intelligence.</li></ol>

</blockquote>

<p>Below is a list of gaps in the above, as I see it, and counterarguments. A ‘gap’ is not necessarily unfillable, and may have been filled in any of the countless writings on this topic that I haven’t read. I might even think that a given one can probably be filled. I just don’t know what goes in it.  </p>

<p>This blog post is an attempt to run various arguments by you all on the way to making pages on <a href="http://aiimpacts.org/">AI Impacts</a> about arguments for AI risk and corresponding counterarguments. At some point in that process I hope to also read others’ arguments, but this is not that day. So what you have here is a bunch of arguments that occur to me, not an exhaustive literature review. </p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>That is, systems that are somewhat more capable than the most capable human. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Based on countless conversations in the AI risk community, and various reading. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
<a href="/2022/10/14/ai_counterargs.html" span class="post-meta">Continue reading &#8594;</a></li>
  
    <li>
      <h3>
        <a class="post-link" href="/2022/10/12/calibration.html">
          Calibration of a thousand predictions</a><span class="post-meta"> Oct 12, 2022</span>
      </h3><p>I’ve been making predictions in a spreadsheet for the last four years, and I recently got to a thousand resolved predictions. Some observations:</p>

<ol>
  <li>
    <p>I’m surprisingly well calibrated for things that mostly aren’t my own behavior<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>. Here’s the calibration curve for 630 resolved predictions in that class:</p>

    <p style="text-align:center;"><img src="https://hosting.photobucket.com/images/i/katjasgrace/Calibration_for_no-special-context_forecasts_(mostly_excluding_much_own-behavior_prediction)(1).png" alt="calibration no context predictions" width="500" /></p>
  </li>
</ol>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>I have a column where I write context on some predictions, which is usually that they are my own work goal, or otherwise a prediction about how I will behave. This graph excludes those, but keeps in some own-behavior prediction which I didn’t flag for whatever reason.) <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
<a href="/2022/10/12/calibration.html" span class="post-meta">Continue reading &#8594;</a></li>
  
</ul>

<!-- Pagination links -->
<div class="pagination">
  
    <span class="previous">Previous</span>
  
  <span class="page_number ">
    << Page: 1 of 18 >>
  </span>
  
    <a href="/blog/page2/" class="next">Next</a>
  
</div>


<!-- Feel free to add content and custom Front Matter to this file.
To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults [message that came with blog I think]

Originally I had a file index.markdown, which I replaced with this one in order to try to get pagination to work.
I also added two loops to a mostly empty page for that.-->


  &nbsp;

</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
    <!-- <div>
      <h2 class="footer-heading"><a href="/list.html">Full archive</a></h2>
    </div> -->

    <!-- <div>
      <h2 class="footer-heading">Search</h2>
<script async src="https://cse.google.com/cse.js?cx=06d4880e018d74eb0">
</script>
<div class="gcse-search"></div>
</div> -->


    <br>

    <h2 class="footer-heading"><a href="/">world spirit sock puppet</a></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <p>Inclusive writings of Katja Grace</p>
        <a href="/list.html">Full archive</a>
        <br>
        <ul class="contact-list">
          <li class="p-name">
            <!--Katja Grace-->
            </li><ul class="social-media-list"><li><a href="https://www.twitter.com/KatjaGrace"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">KatjaGrace</span></a></li></ul>
<li><a class="u-email" href="mailto:katjasolveig@gmail.com">katjasolveig@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2">
        <!--<ul class="social-media-list"><li><a href="https://www.twitter.com/KatjaGrace"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">KatjaGrace</span></a></li></ul>
-->
      </div>

      <div class="footer-col footer-col-3">
        <h4 class="footer-heading">Search</h4>
<script async src="https://cse.google.com/cse.js?cx=06d4880e018d74eb0">
</script>
<div class="gcse-search"></div>
</div>
    </div>

  </div>


</footer>
</body>

</html>

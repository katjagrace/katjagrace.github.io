<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-01-03T03:37:37-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">world spirit sock puppet</title><subtitle>Inclusive writings of Katja Grace</subtitle><author><name>Katja Grace</name></author><entry><title type="html">How to eat potato chips while typing</title><link href="http://localhost:4000/2023/01/03/how-to-eat-potato-chips.html" rel="alternate" type="text/html" title="How to eat potato chips while typing" /><published>2023-01-03T02:32:00-08:00</published><updated>2023-01-03T02:32:00-08:00</updated><id>http://localhost:4000/2023/01/03/how-to-eat-potato-chips</id><content type="html" xml:base="http://localhost:4000/2023/01/03/how-to-eat-potato-chips.html">&lt;p&gt;Chopsticks.&lt;/p&gt;
&lt;div class=&quot;captioned-image-container&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd25d2ed4-4fe6-47f1-aeeb-52b729f06dbc_3024x4032.png&quot; alt=&quot;eating potato chips with chopsticks&quot; width=&quot;250&quot; class=&quot;center&quot; /&gt;&lt;!--ex--&gt;&lt;/p&gt;

  &lt;p&gt;Sometimes I want to eat snacks while I work, honestly because I have a weirdly moving dread of stretches of work-time containing nothing in any way gratifying to me. Possibly because I realistically anticipate that I will not remain working long under those conditions.&lt;/p&gt;
  &lt;p&gt;But sadly many good snacks are oily, crumby, or otherwise ill-suited to being touched repeatedly interleaved with touching one’s keyboard. I think I usually deal with this by eating much snack while not typing at all for a bit, but sort of acting as though I think I’m going to, then washing my hands, then going back to the typing for a while, then repeating—largely defeating the purpose. I also get around this issue by getting bubble tea, a snack which can be substantially consumed with your mouth. &lt;/p&gt;
  &lt;p&gt;I have often vaguely imagined chopsticks helping, but also imagined that they wouldn’t actually. Today I learned that they work well, at least for potato chips and popcorn.&lt;/p&gt;
  &lt;p&gt;You might think standard Western cutlery would be the first thing to try, but a) it’s not good for hard, crunchy objects, and b) it’s somehow worse to reach to the side with both hands and do a knife-and-fork action than it is to reach with one hand, so if you need multiple implements to manipulate your snack, chopsticks seem superior.&lt;/p&gt;
  &lt;p&gt;&lt;span&gt;(For more practical eating advice, see &lt;/span&gt;&lt;a href=&quot;https://meteuphoric.com/2016/06/26/are-you-missing-universal-human-skills/&quot; rel=&quot;&quot;&gt;how to cut and chew meat&lt;/a&gt;&lt;span&gt;. I also learned to drink from a can in the last year or so, but am unlikely to write it up unless I learn that this is a problem for anyone else ever.)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;</content><author><name>Katja Grace</name></author><category term="practicaladvice" /><category term="insight" /><summary type="html">Chopsticks.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd25d2ed4-4fe6-47f1-aeeb-52b729f06dbc_3024x4032.png" /><media:content medium="image" url="https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd25d2ed4-4fe6-47f1-aeeb-52b729f06dbc_3024x4032.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Pacing: inexplicably good</title><link href="http://localhost:4000/2022/12/31/pacing.html" rel="alternate" type="text/html" title="Pacing: inexplicably good" /><published>2022-12-31T19:00:00-08:00</published><updated>2022-12-31T19:00:00-08:00</updated><id>http://localhost:4000/2022/12/31/pacing</id><content type="html" xml:base="http://localhost:4000/2022/12/31/pacing.html">&lt;p&gt;Pacing—walking repeatedly over the same ground—&lt;a href=&quot;https://worldspiritsockpuppet.substack.com/p/oxford-circles-and-planes-19-10-29&quot;&gt;often&lt;/a&gt; feels ineffably good while I’m doing it, but then I forget about it for ages, so I thought I’d write about it here.&lt;/p&gt;

&lt;p&gt;I don’t mean just going for an inefficient walk—it is somehow different to just step slowly in a circle around the same room for a long time, or up and down a passageway. &lt;!--ex--&gt;&lt;/p&gt;

&lt;p&gt;I don’t know why it would be good, but some ideas:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;It’s good to be physically engaged while thinking for some reason.&lt;/em&gt; I used to do ‘gymflection’ with a friend, where we would do strength exercises at the gym, and meanwhile be reflecting on our lives and what is going well and what we might do better. This felt good in a way that didn’t seem to come from either activity alone. (This wouldn’t explain why it would differ from walking though.)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Different working memory setup:&lt;/em&gt; if you pace around in the same vicinity, your thoughts get kind of attached to the objects you are looking at. So next time you get to the green tiles say, they remind you of what you were thinking of last time you were there. This allows for a kind of repeated cycling back through recent topics, but layering different things into the mix with each loop, which is a nice way of thinking. Perhaps a bit like having additional working memory.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I wonder if going for a walk doesn’t really get 1) in a satisfying way, because my mind easily wanders from the topic at hand and also from my surrounds, so it less feels like I’m really grappling with something and being physical, and more like I’m daydreaming elsewhere. So maybe 2) is needed also, to both stick with a topic and attend to the physical world for a while. I don’t put a high probability on this detailed theory.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="lifeadvice" /><category term="psychology" /><category term="meteuphoric" /><summary type="html">Pacing—walking repeatedly over the same ground—often feels ineffably good while I’m doing it, but then I forget about it for ages, so I thought I’d write about it here. I don’t mean just going for an inefficient walk—it is somehow different to just step slowly in a circle around the same room for a long time, or up and down a passageway.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/kenny-eliason-m_rrkduuHkU-unsplash.jpg" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/kenny-eliason-m_rrkduuHkU-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Worldly Positions archive, briefly with private drafts</title><link href="http://localhost:4000/2022/12/30/worldly-positions-archives.html" rel="alternate" type="text/html" title="Worldly Positions archive, briefly with private drafts" /><published>2022-12-30T02:56:00-08:00</published><updated>2022-12-30T02:56:00-08:00</updated><id>http://localhost:4000/2022/12/30/worldly-positions-archives</id><content type="html" xml:base="http://localhost:4000/2022/12/30/worldly-positions-archives.html">&lt;p&gt;I realized it was hard to peruse past &lt;a href=&quot;https://worldlypositions.tumblr.com&quot;&gt;Worldly Positions&lt;/a&gt; posts without logging in to Tumblr, which seemed pretty bad. So I followed Substack’s instructions to import the archives into &lt;a href=&quot;https://worldspiritsockpuppet.substack.com/&quot;&gt;world spirit sock stack&lt;/a&gt;. And it worked pretty well, except that SUBSTACK ALSO PUBLISHED MY UNPUBLISHED WORLDLY POSITIONS DRAFTS! What on Earth? That’s so bad. Did I misunderstand what happened somehow in my rush to unpublish them? Maybe. But they definitely had ‘unpublish’ buttons, so that’s pretty incriminating. &lt;!--ex--&gt;&lt;/p&gt;

&lt;p&gt;This seems to have turned out alright for me, since it looks like I just never wrote any drafts that would be too embarrassing to anyone other than myself. And the most embarrassing to myself are probably at the level of bad and abortive poetry. Plus it turned up a few decent drafts to finish, and the adrenaline was a welcome pick-me-up in my current retreat-driven stimulation drought.&lt;/p&gt;

&lt;p&gt;Some good bits of the archive (from pre-&lt;a href=&quot;https://worldspiritsockpuppet.com/worldlypositions.html&quot;&gt;WSSP&lt;/a&gt; times) according to me:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://worldspiritsockpuppet.substack.com/p/mine-craft-20-07-03&quot;&gt;Mine-craft&lt;/a&gt;: the composition of the ego in a procedurally generated sandbox game&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://worldspiritsockpuppet.substack.com/p/claudius-emanuel-g-s-17-10-20&quot;&gt;The time I rented a robot baby&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://worldspiritsockpuppet.substack.com/p/tv-night-realer-photography-19-06-11&quot;&gt;Why fiction is more horrifying than war photography&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://worldspiritsockpuppet.substack.com/p/home-up-and-down-colder-and-warmer-20-02-20&quot;&gt;Home: up and down, colder and warmer&lt;/a&gt;: miscellanious thoughts on e.g. warmth and coldness, the sincerity of historic advertising, and why negativity is deep&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://worldspiritsockpuppet.substack.com/p/bay-bridge-having-fun-19-06-06&quot;&gt;How I learned to have fun on command&lt;/a&gt; though I rarely remember to do it&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://worldspiritsockpuppet.substack.com/p/england-attunement-and-borders-20-01-30&quot;&gt;England: Attunement and borders&lt;/a&gt;, in which I get possible attunement, companionship, and a visa&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Katja Grace</name></author><category term="meta" /><category term="worldlypositions" /><summary type="html">I realized it was hard to peruse past Worldly Positions posts without logging in to Tumblr, which seemed pretty bad. So I followed Substack’s instructions to import the archives into world spirit sock stack. And it worked pretty well, except that SUBSTACK ALSO PUBLISHED MY UNPUBLISHED WORLDLY POSITIONS DRAFTS! What on Earth? That’s so bad. Did I misunderstand what happened somehow in my rush to unpublish them? Maybe. But they definitely had ‘unpublish’ buttons, so that’s pretty incriminating.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://static.tumblr.com/010f7823b98a70509677554b6bbb2f46/j5ffzji/yiIoqg5l6/tumblr_static_6naihwpnpqww8okw804s08g_2048_v2.jpg" /><media:content medium="image" url="https://static.tumblr.com/010f7823b98a70509677554b6bbb2f46/j5ffzji/yiIoqg5l6/tumblr_static_6naihwpnpqww8okw804s08g_2048_v2.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">More ways to spot abysses</title><link href="http://localhost:4000/2022/12/29/how-to-stare-into-the-abyss.html" rel="alternate" type="text/html" title="More ways to spot abysses" /><published>2022-12-29T09:30:00-08:00</published><updated>2022-12-29T09:30:00-08:00</updated><id>http://localhost:4000/2022/12/29/how-to-stare-into-the-abyss</id><content type="html" xml:base="http://localhost:4000/2022/12/29/how-to-stare-into-the-abyss.html">&lt;p&gt;I liked Ben Kuhn’s &lt;a href=&quot;https://www.benkuhn.net/abyss/&quot;&gt;‘Staring into the abyss as a core life skill’&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’d summarize as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;If you are making a major error—professionally, romantically, religiously, etc—it can be hard to look at that fact and correct.&lt;/li&gt;
  &lt;li&gt;However it’s super important. Evidence: successful people do this well.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This seems pretty plausible to me.&lt;/p&gt;

&lt;p&gt;(He has a lot of concrete examples, which are probably pretty helpful for internalizing this.)&lt;/p&gt;

&lt;p&gt;His suggestions for how to do better helped me a bit, but not that much, so I made up my own additional prompts for finding abysses I should consider staring into, which worked relatively well for me:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;If you were currently making a big mistake, what would it be?&lt;/li&gt;
  &lt;li&gt;What are some things that would be hard to acknowledge, if they were true?&lt;/li&gt;
  &lt;li&gt;Looking back on this time from five years hence, what do you think you’ll wish you changed earlier?&lt;/li&gt;
  &lt;li&gt;If you were forced to quit something, what do you want it to be?&lt;/li&gt;
  &lt;li&gt;(Variant on 1:) If you were currently making a big mistake that would be gut-wrenching to learn was a mistake, what would it be?&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Katja Grace</name></author><category term="lifestrategy" /><category term="selfimprovement" /><category term="response" /><summary type="html">I liked Ben Kuhn’s ‘Staring into the abyss as a core life skill’. I’d summarize as: If you are making a major error—professionally, romantically, religiously, etc—it can be hard to look at that fact and correct. However it’s super important. Evidence: successful people do this well. This seems pretty plausible to me. (He has a lot of concrete examples, which are probably pretty helpful for internalizing this.) His suggestions for how to do better helped me a bit, but not that much, so I made up my own additional prompts for finding abysses I should consider staring into, which worked relatively well for me: If you were currently making a big mistake, what would it be? What are some things that would be hard to acknowledge, if they were true? Looking back on this time from five years hence, what do you think you’ll wish you changed earlier? If you were forced to quit something, what do you want it to be? (Variant on 1:) If you were currently making a big mistake that would be gut-wrenching to learn was a mistake, what would it be?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/tim-johnson-ywIZ8ZYxzWU-unsplash.jpg" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/tim-johnson-ywIZ8ZYxzWU-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Let’s think about slowing down AI</title><link href="http://localhost:4000/2022/12/22/lets-think-about-slowing-down-ai.html" rel="alternate" type="text/html" title="Let&apos;s think about slowing down AI" /><published>2022-12-22T09:14:00-08:00</published><updated>2022-12-22T09:14:00-08:00</updated><id>http://localhost:4000/2022/12/22/lets-think-about-slowing-down-ai</id><content type="html" xml:base="http://localhost:4000/2022/12/22/lets-think-about-slowing-down-ai.html">&lt;p&gt;&lt;em&gt;(Crossposted from AI Impacts Blog)&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;Averting doom by not building the doom machine&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;If you fear that someone will build a machine that will seize control of the world and annihilate humanity, then one kind of response is to try to build further machines that will seize control of the world even earlier without destroying it, forestalling the ruinous machine’s conquest. An alternative or complementary kind of response is to try to avert such machines being built at all, at least while the degree of their apocalyptic tendencies is ambiguous. &lt;/p&gt;

&lt;p&gt;The latter approach seems to me  like the kind of basic and obvious thing worthy of at least consideration, and also in its favor, fits nicely in the genre ‘stuff that it isn’t that hard to imagine happening in the real world’. Yet my impression is that for people worried about extinction risk from artificial intelligence, strategies under the heading ‘actively slow down AI progress’ have historically been dismissed and ignored (though ‘don’t actively speed up AI progress’ is popular).&lt;/p&gt;

&lt;p&gt;The conversation near me over the years has felt a bit like this: &lt;/p&gt;

&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Some people: &lt;/strong&gt;AI might kill everyone. We should design a godlike super-AI of perfect goodness to prevent that.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Others: &lt;/strong&gt;wow that sounds extremely ambitious&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Some people: &lt;/strong&gt;yeah but it’s very important and also we are extremely smart so idk it could work&lt;/p&gt;

  &lt;p&gt;[Work on it for a decade and a half]&lt;/p&gt;

  &lt;p&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Some people: &lt;/strong&gt;ok that’s pretty hard, we give up&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt; oh huh shouldn’t we maybe try to stop the building of this dangerous AI? &lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Some people:&lt;/strong&gt; hmm, that would involve coordinating numerous people—we may be arrogant enough to think that we might build a god-machine that can take over the world and remake it as a paradise, but we aren’t delusional&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This seems like an error to me. (And &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like&quot;&gt;lately&lt;/a&gt;, &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://forum.effectivealtruism.org/posts/8CMuNwKMcR55jhd8W/instead-of-technical-research-more-people-should-focus-on&quot;&gt;to&lt;/a&gt; &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://twitter.com/KerryLVaughan/status/1536364299089854471&quot;&gt;a&lt;/a&gt; &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://www.lesswrong.com/posts/yhRTjBs6oiNcjRgcx/the-case-for-doing-something-else-if-alignment-is-doomed&quot;&gt;bunch&lt;/a&gt; &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://twitter.com/scholl_adam/status/1556989092784615424&quot;&gt;of&lt;/a&gt; &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://forum.effectivealtruism.org/posts/6LNvQYyNQpDQmnnux/slowing-down-ai-progress-is-an-underexplored-alignment&quot;&gt;other&lt;/a&gt; &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://forum.effectivealtruism.org/posts/pJuS5iGbazDDzXwJN/the-history-epistemology-and-strategy-of-technological&quot;&gt;people&lt;/a&gt;.) &lt;/p&gt;

&lt;!--ex--&gt;

&lt;p&gt;I don’t have a strong view on whether anything in the space of ‘try to slow down some AI research’ should be done. But I think a) the naive first-pass guess should be a strong ‘probably’, and b) a decent amount of thinking should happen before writing off everything in this large space of interventions. Whereas customarily the tentative answer seems to be, ‘of course not’ and then the topic seems to be avoided for further thinking. (At least in my experience—the AI safety community is large, and for most things I say here, different experiences are probably had in different bits of it.)&lt;/p&gt;

&lt;p&gt;Maybe my strongest view is that one shouldn’t apply such different standards of ambition to these different classes of intervention. Like: yes, there appear to be substantial difficulties in slowing down AI progress to good effect. But in technical alignment, mountainous challenges are met with enthusiasm for mountainous efforts. And it is very non-obvious that the scale of difficulty here is much larger than that involved in designing acceptably safe versions of machines capable of taking over the world before anyone else in the world designs dangerous versions. &lt;/p&gt;

&lt;p&gt;I’ve been talking about this with people over the past many months, and have accumulated an abundance of reasons for not trying to slow down AI, most of which I’d like to argue about at least a bit. My impression is that arguing in real life has coincided with people moving toward my views.&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;Quick clarifications&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;First, to fend off misunderstanding—&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I take ‘slowing down dangerous AI’ to include any of:&amp;nbsp;&lt;ol&gt;&lt;li&gt;reducing the speed at which AI progress is made in general, e.g. as would occur if general funding for AI declined.&lt;/li&gt;&lt;li&gt;shifting AI efforts from work leading more directly to risky outcomes to other work, e.g. as might occur if there was broadscale concern about very large AI models, and people and funding moved to other projects.&lt;/li&gt;&lt;li&gt;Halting categories of work until strong confidence in its safety is possible, e.g. as would occur if AI researchers agreed that certain systems posed catastrophic risks and should not be developed until they did not. (This might mean a permanent end to some systems, if they were intrinsically unsafe.)&lt;/li&gt;&lt;/ol&gt;(So in particular, I’m including both actions whose direct aim is slowness in general, and actions whose aim is requiring safety before specific developments, which implies slower progress.)&lt;/li&gt;



&lt;li&gt;I do think there is serious attention on some versions of these things, generally under other names. I see people thinking about ‘differential progress’ (b. above), and strategizing about coordination to slow down AI at some point in the future (e.g. at ‘deployment’). And I think a lot of consideration is given to avoiding actively speeding up AI progress. What I’m saying is missing are, a) consideration of actively working to slow down AI now, and b) shooting straightforwardly to ‘slow down AI’, rather than wincing from that and only considering examples of it that show up under another conceptualization (perhaps this is an unfair diagnosis).&lt;/li&gt;



&lt;li&gt;AI Safety is a big community, and I’ve only ever been seeing a one-person window into it, so maybe things are different e.g. in DC, or in different conversations in Berkeley. I’m just saying that for my corner of the world, the level of disinterest in this has been notable, and in my view misjudged.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;&lt;strong&gt;Why not slow down AI? Why not consider it?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Ok, so if we tentatively suppose that this topic is worth even thinking about, what do we think? Is slowing down AI a good idea at all? Are there great reasons for dismissing it?&lt;/p&gt;

&lt;p&gt;&lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://astralcodexten.substack.com/p/why-not-slow-ai-progress&quot; target=&quot;_blank&quot;&gt;Scott Alexander wrote a post&lt;/a&gt; a little while back raising reasons to dislike the idea, roughly:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Do you want to lose an arms race? If the AI safety community tries to slow things down, it will disproportionately slow down progress in the US, and then people elsewhere will go fast and get to be the ones whose competence determines whether the world is destroyed, and whose values determine the future if there is one. Similarly, if AI safety people criticize those contributing to AI progress, it will mostly discourage the most friendly and careful AI capabilities companies, and the reckless ones will get there first.&lt;/li&gt;



&lt;li&gt;One might contemplate ‘coordination’ to avoid such morbid races. But coordinating anything with the whole world seems wildly tricky. For instance, some countries are large, scary, and hard to talk to.&lt;/li&gt;



&lt;li&gt;Agitating for slower AI progress is ‘defecting’ against the AI capabilities folks, who are good friends of the AI safety community, and their friendship is strategically valuable for ensuring that safety is taken seriously in AI labs (as well as being non-instrumentally lovely! Hi AI capabilities friends!).&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Other opinions I’ve heard, some of which I’ll address:&lt;/p&gt;

&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Slowing AI progress is futile: for all your efforts you’ll probably just die a few years later&lt;/li&gt;



&lt;li&gt;Coordination based on convincing people that AI risk is a problem is absurdly ambitious. It’s practically impossible to convince AI professors of this, let alone any real fraction of humanity, and you’d need to convince a massive number of people.&lt;/li&gt;



&lt;li&gt;What are we going to do, build powerful AI never and die when the Earth is eaten by the sun?&lt;/li&gt;



&lt;li&gt;It’s actually better for safety if AI progress moves fast. This might be because the faster AI capabilities work happens, the smoother AI progress will be, and this is more important than the duration of the period. Or speeding up progress now might force future progress to be correspondingly slower. Or &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://www.lesswrong.com/posts/EzAt4SbtQcXtDNhHK/confused-why-a-capabilities-research-is-good-for-alignment&quot;&gt;because&lt;/a&gt; safety work is probably better when done just before building the relevantly risky AI, in which case the best strategy might be to get as close to dangerous AI as possible and then stop and do safety work. Or if safety work is very useless ahead of time, maybe delay is fine, but there is little to gain by it.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Specific routes to slowing down AI are not worth it. For instance, avoiding working on AI capabilities research is bad because &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://forum.effectivealtruism.org/posts/qjsWZJWcvj3ug5Xja/agrippa-s-shortform?commentId=PWuNscobqudn7puy7&quot;&gt;it’s so helpful&lt;/a&gt; for &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://forum.effectivealtruism.org/posts/qjsWZJWcvj3ug5Xja/agrippa-s-shortform?commentId=EKeegJgLNcPYZ4Kac&quot;&gt;learning&lt;/a&gt; on the path to working on alignment. And AI safety people working in AI capabilities can be a force for making safer choices at those companies.&lt;/li&gt;



&lt;li&gt;Advanced AI will help enough with other existential risks as to represent a net lowering of existential risk overall.&lt;a id=&quot;footnote-anchor-1&quot; href=&quot;#footnote-1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;



&lt;li&gt;Regulators are ignorant about the nature of advanced AI (partly because it doesn’t exist, so everyone is ignorant about it). Consequently they won’t be able to regulate it effectively, and bring about desired outcomes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;My impression is that there are also less endorsable or less altruistic or more silly motives floating around for this attention allocation. Some things that have come up at least once in talking to people about this, or that seem to be going on:&lt;/p&gt;

&lt;div class=&quot;is-layout-flow wp-block-group has-black-color has-text-color&quot;&gt;
  &lt;div class=&quot;wp-block-group__inner-container&quot;&gt;
    &lt;ul&gt;
&lt;li&gt;Advanced AI might bring manifold wonders, e.g. long lives of unabated thriving. Getting there a bit later is fine for posterity, but for our own generation it could mean dying as our ancestors did while on the cusp of a utopian eternity. Which would be pretty disappointing. For a person who really believes in this future, it can be tempting to shoot for the best scenario—humanity builds strong, safe AI in time to save this generation—rather than the scenario where our own lives are inevitably lost.&lt;/li&gt;



&lt;li&gt;Sometimes people who have a heartfelt appreciation for the flourishing that technology has afforded so far can find it painful to be superficially on the side of Luddism here.&lt;/li&gt;



&lt;li&gt;Figuring out how minds work well enough to create new ones out of math is an incredibly deep and interesting intellectual project, which feels right to take part in. It can be hard to intuitively feel like one shouldn’t do it.&lt;br /&gt;&lt;br /&gt;(Illustration from a &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://en.wikipedia.org/wiki/Richard_S._Sutton&quot; target=&quot;_blank&quot;&gt;co-founder of modern computational reinforcement learning&lt;/a&gt;: )

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;It will be the greatest intellectual achievement of all time.&lt;br /&gt;&lt;br /&gt;An achievement of science, of engineering, and of the humanities, &lt;br /&gt;whose significance is beyond humanity, &lt;br /&gt;beyond life, &lt;br /&gt;beyond good and bad.&lt;/p&gt;&amp;mdash; Richard Sutton (@RichardSSutton) &lt;a href=&quot;https://twitter.com/RichardSSutton/status/1575619655778983936?ref_src=twsrc%5Etfw&quot;&gt;September 29, 2022&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; 


&lt;li&gt;It is uncomfortable to contemplate projects that would put you in conflict with other people. Advocating for slower AI feels like trying to impede someone else’s project, which feels adversarial and can feel like it has a higher burden of proof than just working on your own thing.&lt;/li&gt;



&lt;li&gt;‘Slow-down-AGI’ sends people’s minds to e.g. industrial sabotage or terrorism, rather than more boring courses, such as, ‘lobby for labs developing shared norms for when to pause deployment of models’. This understandably encourages dropping the thought as soon as possible.&lt;/li&gt;



&lt;li&gt;My weak guess is that there’s a kind of bias at play in AI risk thinking in general, where any force that isn’t zero is taken to be arbitrarily intense. Like, if there is pressure for agents to exist, there will arbitrarily quickly be arbitrarily agentic things. If there is a feedback loop, it will be arbitrarily strong. Here, if stalling AI can’t be forever, then it’s essentially zero time. If a regulation won’t obstruct every dangerous project, then is worthless. Any finite economic disincentive for dangerous AI is nothing in the face of the omnipotent economic incentives for AI. I think this is a bad mental habit: things in the real world often come down to actual finite quantities. This is very possibly an unfair diagnosis. (I’m not going to discuss this later; this is pretty much what I have to say.)&lt;/li&gt;



&lt;li&gt;I sense an assumption that slowing progress on a technology would be a radical and unheard-of move.&lt;/li&gt;



&lt;li&gt;I agree with &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like&quot; target=&quot;_blank&quot;&gt;lc&lt;/a&gt; that there seems to have been a quasi-taboo on the topic, which perhaps explains a lot of the non-discussion, though still calls for its own explanation. I think it suggests that concerns about uncooperativeness play a part, and the same for thinking of slowing down AI as centrally involving antisocial strategies.&lt;/li&gt;
&amp;lt;/ul&amp;gt;
&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;



&lt;p&gt;I’m not sure if any of this fully resolves why AI safety people haven’t thought about slowing down AI more, or whether people should try to do it. But my sense is that many of the above reasons are at least somewhat wrong, and motives somewhat misguided, so I want to argue about a lot of them in turn, including both arguments and vague motivational themes.&lt;/p&gt;



&lt;h1&gt;The mundanity of the proposal&lt;/h1&gt;



&lt;h2&gt;&lt;strong&gt;Restraint is not radical&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;There seems to be a common thought that technology is a kind of inevitable path along which the world must tread, and that trying to slow down or avoid any part of it would be both futile and extreme.&lt;a id=&quot;footnote-anchor-2&quot; href=&quot;#footnote-2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But empirically, the world doesn’t pursue every technology—it barely pursues any technologies.&lt;/p&gt;



&lt;h3&gt;&lt;strong&gt;Sucky technologies&lt;/strong&gt;&lt;/h3&gt;



&lt;p&gt;For a start, there are many machines that there is no pressure to make, because they have no value. Consider a machine that sprays shit in your eyes. We can technologically do that, but probably nobody has ever built that machine.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This might seem like a stupid example, because no serious ‘technology is inevitable’ conjecture is going to claim that totally pointless technologies are inevitable. But if you are sufficiently pessimistic about AI, I think this is the right comparison: if there are kinds of AI that would cause huge net costs to their creators if created, according to our best understanding, then they are at least as useless to make as the ‘spray shit in your eyes’ machine. We might accidentally make them due to error, but there is not some deep economic force pulling us to make them. If unaligned superintelligence destroys the world with high probability when you ask it to do a thing, then this is the category it is in, and it is not strange for its designs to just rot in the scrap-heap, with the machine that sprays shit in your eyes and the machine that spreads caviar on roads.&lt;/p&gt;



&lt;p&gt;Ok, but maybe the relevant actors are very committed to being wrong about whether unaligned superintelligence would be a great thing to deploy. Or maybe you think the situation is less immediately dire and building existentially risky AI &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://aiimpacts.org/incentives-to-create-x-risky-ai-systems/&quot;&gt;really would&lt;/a&gt; be good for the people making decisions (e.g. because the costs won’t arrive for a while, and the people care a lot about a shot at scientific success relative to a chunk of the future). If the apparent economic incentives are large, are technologies unavoidable?&lt;/p&gt;



&lt;h3&gt;&lt;strong&gt;Extremely valuable technologies&lt;/strong&gt;&lt;/h3&gt;



&lt;p&gt;It doesn’t look like it to me. Here are a few technologies which I’d guess have substantial economic value, where research progress or uptake appears to be drastically slower than it could be, for reasons of concern about safety or ethics&lt;a id=&quot;footnote-anchor-3&quot; href=&quot;#footnote-3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;



&lt;ol&gt;
&lt;li&gt;Huge amounts of medical research, including really important medical research e.g. The FDA &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7326309/#:~:text=Nonetheless%2C%20in%201978%20the%20controversy%20resulted%20in%20a%20US%20FDA%20ban%20on%20subsequent%20vaccine%20trials%20which%20was%20eventually%20overturned%2030%20years%20later.&quot;&gt;banned&lt;/a&gt; human trials of strep A vaccines from the 70s to the 2000s, in spite of &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6474463/#:~:text=Worldwide%2C%20the%20death%20toll%20is%20estimated%20at%20500%20000%20annually&quot;&gt;500,000 global deaths every year&lt;/a&gt;. A lot of people also died while covid vaccines went through all the proper trials.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Nuclear energy&lt;/li&gt;



&lt;li&gt;Fracking&lt;/li&gt;



&lt;li&gt;Various genetics things: genetic modification of foods, gene drives, early recombinant DNA researchers famously organized a moratorium and then ongoing research guidelines including prohibition of certain experiments (see the &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://en.wikipedia.org/wiki/Asilomar_Conference_on_Recombinant_DNA&quot;&gt;Asilomar Conference&lt;/a&gt;)&lt;/li&gt;



&lt;li&gt;Nuclear, biological, and maybe chemical weapons (or maybe these just aren’t useful)&lt;/li&gt;



&lt;li&gt;Various human reproductive innovation: cloning of humans, genetic manipulation of humans (a notable example of an economically valuable technology that is to my knowledge barely pursued across different countries, without explicit coordination between those countries, even though it would make those countries more competitive. Someone used CRISPR on babies in China, but was &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://www.science.org/content/article/creator-crispr-babies-nears-release-prison-where-does-embryo-editing-stand&quot;&gt;imprisoned&lt;/a&gt; for it.)&lt;/li&gt;



&lt;li&gt;Recreational drug development&lt;/li&gt;



&lt;li&gt;Geoengineering&lt;/li&gt;



&lt;li&gt;Much of science about humans? I recently ran &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/&quot;&gt;this survey&lt;/a&gt;, and was reminded how encumbering ethical rules are for even incredibly innocuous research. As far as I could tell the EU now makes it illegal to collect data in the EU unless you promise to delete the data from anywhere that it might have gotten to if the person who gave you the data wishes for that at some point. In all, dealing with this and IRB-related things added maybe more than half of the effort of the project. Plausibly I misunderstand the rules, but I doubt other researchers are radically better at figuring them out than I am.&lt;/li&gt;



&lt;li&gt;There are probably examples from fields considered distasteful or embarrassing to associate with, but it’s hard as an outsider to tell which fields are genuinely hopeless versus erroneously considered so. If there are economically valuable health interventions among those considered wooish, I imagine they would be much slower to be identified and pursued by scientists with good reputations than a similarly promising technology not marred in that way. Scientific research into intelligence is more clearly slowed by stigma, but it is less clear to me what the economically valuable upshot would be.&lt;/li&gt;



&lt;li&gt;(I think there are many other things that could be in this list, but I don’t have time to review them at the moment. &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://wiki.aiimpacts.org/doku.php?id=responses_to_ai:technological_inevitability:incentivized_technologies_not_pursued:start&quot;&gt;This page&lt;/a&gt; might collect more of them in future.)&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;It seems to me that intentionally slowing down progress in technologies to give time for even probably-excessive caution is commonplace. (And this is just looking at things slowed down over caution or ethics specifically—probably there are also other reasons things get slowed down.)&lt;/p&gt;



&lt;p&gt;Furthermore, among valuable technologies that nobody is especially trying to slow down, it seems common enough for progress to be massively slowed by relatively minor obstacles, which is further evidence for a lack of overpowering strength of the economic forces at play. For instance, Fleming first took notice of mold&amp;#8217;s effect on bacteria in 1928, but nobody took a serious, high-effort shot at developing it as a drug until 1939.&lt;a id=&quot;footnote-anchor-4&quot; href=&quot;#footnote-4&quot;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; Furthermore, in the thousands of years preceding these events, &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://en.wikipedia.org/wiki/History_of_penicillin#Early_history&quot; target=&quot;_blank&quot;&gt;various&lt;/a&gt; &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://en.wikipedia.org/wiki/History_of_penicillin#Early_scientific_evidence&quot; target=&quot;_blank&quot;&gt;people&lt;/a&gt; noticed numerous times that mold, other fungi or plants inhibited bacterial growth, but didn’t exploit this observation even enough for it not to be considered a new discovery in the 1920s. Meanwhile, people dying of infection was quite a thing. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://dash.harvard.edu/bitstream/handle/1/8889467/Gottfried05.pdf&quot; target=&quot;_blank&quot;&gt;In 1930&lt;/a&gt; about 300,000 Americans died of bacterial illnesses per year (around 250/100k).&lt;/p&gt;



&lt;p&gt;My guess is that people make real choices about technology, and they do so in the face of economic forces that are feebler than commonly thought.&amp;nbsp;&lt;/p&gt;



&lt;h2&gt;&lt;strong&gt;Restraint is not terrorism, usually&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;I think people have historically imagined weird things when they think of ‘slowing down AI’. I posit that their central image is sometimes terrorism (which understandably they don’t want to think about for very long), and sometimes some sort of implausibly utopian global agreement.&lt;/p&gt;



&lt;p&gt;Here are some other things that ‘slow down AI capabilities’ could look like (where the best positioned person to carry out each one differs, but if you are not that person, you could e.g. talk to someone who is):&lt;/p&gt;



&lt;ol&gt;
&lt;li&gt;Don’t actively forward AI progress, e.g. by devoting your life or millions of dollars to it (this one is often considered already)&lt;/li&gt;



&lt;li&gt;Try to convince researchers, funders, hardware manufacturers, institutions etc that they too should stop actively forwarding AI progress&lt;/li&gt;



&lt;li&gt;Try to get any of those people to stop actively forwarding AI progress even if they don’t agree with you: through negotiation, payments, public reproof, or other activistic means.&lt;/li&gt;



&lt;li&gt;Try to get the message to the world that AI is heading toward being seriously endangering. If AI progress is broadly condemned, this will trickle into myriad decisions: job choices, lab policies, national laws. To do this, for instance &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities?commentId=JqT3t2jyj3NXDFkDr&quot;&gt;produce compelling demos of risk, agitate for stigmatization of risky actions&lt;/a&gt;, write science fiction illustrating the problems broadly and evocatively (I think this has actually been helpful repeatedly in the past), go on TV, write opinion pieces, help organize and empower the people who are already concerned, etc.&lt;/li&gt;



&lt;li&gt;Help organize the researchers who think their work is potentially omnicidal into coordinated action on not doing it.&lt;/li&gt;



&lt;li&gt;Move AI resources from dangerous research to other research. Move investments from projects that lead to large but poorly understood capabilities, to projects that lead to understanding these things e.g. theory before scaling (see &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://forum.effectivealtruism.org/posts/g6549FAQpQ5xobihj/differential-technological-development&quot; target=&quot;_blank&quot;&gt;differential technological development&lt;/a&gt; in general&lt;a id=&quot;footnote-anchor-5&quot; href=&quot;#footnote-5&quot;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;).&lt;/li&gt;



&lt;li&gt;Formulate specific precautions for AI researchers and labs to take in different well-defined future situations, &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://en.wikipedia.org/wiki/Asilomar_Conference_on_Recombinant_DNA&quot;&gt;Asilomar Conference&lt;/a&gt; style. These could include more intense vetting by particular parties or methods, modifying experiments, or pausing lines of inquiry entirely. Organize labs to coordinate on these.&lt;/li&gt;



&lt;li&gt;Reduce available compute for AI, e.g. via regulation of production and trade, seller choices, purchasing compute, trade strategy.&lt;/li&gt;



&lt;li&gt;At labs, choose policies that slow down other labs, e.g. reduce public helpful research outputs&lt;/li&gt;



&lt;li&gt;Alter the publishing system and incentives to reduce research dissemination. E.g. A journal verifies research results and releases the fact of their publication without any details, maintains records of research priority for later release, and distributes funding for participation. (This is how Szilárd and co. arranged the mitigation of 1940s nuclear research helping Germany, except I’m not sure if the compensatory funding idea was used.&lt;a id=&quot;footnote-anchor-6&quot; href=&quot;#footnote-6&quot;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;)&lt;/li&gt;



&lt;li&gt;The above actions would be taken through choices made by scientists, or funders, or legislators, or labs, or public observers, etc. Communicate with those parties, or help them act.&lt;/li&gt;
&lt;/ol&gt;



&lt;h2&gt;&lt;strong&gt;Coordination is not miraculous world government, usually&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The common image of coordination seems to be explicit, centralized, involving of every party in the world, and something like cooperating on a prisoners’ dilemma: incentives push every rational party toward defection at all times, yet maybe through deontological virtues or sophisticated decision theories or strong international treaties, everyone manages to not defect for enough teetering moments to find another solution.&lt;/p&gt;



&lt;p&gt;That is a possible way coordination could be. (And I think one that shouldn’t be seen as so hopeless—the world has actually coordinated on some impressive things, e.g. nuclear non-proliferation.) But if what you want is for lots of people to coincide in doing one thing when they might have done another, then there are quite a few ways of achieving that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Consider some other case studies of coordinated behavior:&lt;/p&gt;



&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Not eating sand.&lt;/strong&gt; The whole world coordinates to barely eat any sand at all. How do they manage it? It is actually not in almost anyone’s interest to eat sand, so the mere maintenance of sufficient epistemological health to have this widely recognized does the job.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Eschewing bestiality: &lt;/strong&gt;probably some people think bestiality is moral, but enough don’t that engaging in it would risk huge stigma. Thus the world coordinates fairly well on doing very little of it.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Not wearing Victorian attire on the streets:&lt;/strong&gt; this is similar but with no moral blame involved. Historic dress is arguably often more aesthetic than modern dress, but even people who strongly agree find it unthinkable to wear it in general, and assiduously avoid it except for when they have ‘excuses’ such as a special party. This is a very strong coordination against what appears to otherwise be a ubiquitous incentive (to be nicer to look at). As far as I can tell, it’s powered substantially by the fact that it is ‘not done’ and would now be weird to do otherwise. (Which is a very general-purpose mechanism.)&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Political correctness:&lt;/strong&gt; public discourse has strong norms about what it is okay to say, which do not appear to derive from a vast majority of people agreeing about this (as with bestiality say). New ideas about what constitutes being politically correct sometimes spread widely. This coordinated behavior seems to be roughly due to decentralized application of social punishment, from both a core of proponents, and from people who fear punishment for not punishing others. Then maybe also from people who are concerned by non-adherence to what now appears to be the norm given the actions of the others. This differs from the above examples, because it seems like it could persist even with a very small set of people agreeing with the object-level reasons for a norm. If failing to advocate for the norm gets you publicly shamed by advocates, then you might tend to advocate for it, making the pressure stronger for everyone else.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;These are all cases of very broadscale coordination of behavior, none of which involve prisoners’ dilemma type situations, or people making explicit agreements which they then have an incentive to break. They do not involve centralized organization of huge multilateral agreements. Coordinated behavior can come from everyone individually wanting to make a certain choice for correlated reasons, or from people wanting to do things that those around them are doing, or from distributed behavioral dynamics such as punishment of violations, or from collaboration in thinking about a topic.&lt;/p&gt;



&lt;p&gt;You might think they are weird examples that aren’t very related to AI. I think, a) it’s important to remember the plethora of weird dynamics that actually arise in human group behavior and not get carried away theorizing about AI in a world drained of everything but prisoners’ dilemmas and binding commitments, and b) the above are actually all potentially relevant dynamics here.&lt;/p&gt;



&lt;p&gt;If AI in fact poses a large existential risk within our lifetimes, such that it is net bad for any particular individual, then the situation in theory looks a lot like that in the ‘avoiding eating sand’ case. It’s an option that a rational person wouldn’t want to take if they were just alone and not facing any kind of multi-agent situation. If AI is that dangerous, then not taking this inferior option could largely come from a coordination mechanism as simple as distribution of good information. (You still need to deal with irrational people and people with unusual values.)&lt;/p&gt;



&lt;p&gt;But even failing coordinated caution from ubiquitous insight into the situation, other models might work. For instance, if there came to be somewhat widespread concern that AI research is bad, that might&amp;nbsp;substantially lessen participation in it, beyond the set of people who are concerned, via mechanisms similar to those described above. Or it might give rise to a wide crop of local regulation, enforcing whatever behavior is deemed acceptable. Such regulation need not be centrally organized across the world to serve the purpose of coordinating the world, as long as it grew up in different places similarly. Which might happen because different locales have similar interests (all rational governments should be similarly concerned about losing power to automated power-seeking systems with unverifiable goals), or because—as with individuals—there are social dynamics which support norms arising in a non-centralized way.&lt;/p&gt;



&lt;h1&gt;The arms race model and its alternatives&lt;/h1&gt;



&lt;p&gt;Ok, maybe in principle you might hope to coordinate to not do self-destructive things, but realistically, if the US tries to slow down, won’t China or Facebook or someone less cautious take over the world?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Let’s be more careful about the game we are playing, game-theoretically speaking.&lt;/p&gt;



&lt;h2&gt;The arms race&lt;/h2&gt;



&lt;p&gt;What is an arms race, game theoretically? It’s an iterated &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://en.wikipedia.org/wiki/Prisoner%27s_dilemma&quot;&gt;prisoners’ dilemma&lt;/a&gt;, seems to me. Each round looks something like this:&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter is-resized&quot;&gt;&lt;a href=&quot;https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F17202441-ea13-41b0-b0d5-d83f30e2ffec_424x410.png&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;&gt;&lt;img decoding=&quot;async&quot; src=&quot;https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F17202441-ea13-41b0-b0d5-d83f30e2ffec_424x410.png&quot; alt=&quot;&quot; width=&quot;293&quot; height=&quot;283&quot; /&gt;&lt;/a&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;&lt;strong&gt;Player 1 chooses a row, Player 2 chooses a column, and the resulting payoffs are listed in each cell, for {Player 1, Player 2}&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;In this example, building weapons costs one unit. If anyone ends the round with more weapons than anyone else, they take all of their stuff (ten units).&lt;/p&gt;



&lt;p&gt;In a single round of the game it’s always better to build weapons than not (assuming your actions are devoid of implications about your opponent’s actions). And it’s always better to get the hell out of this game.&lt;/p&gt;



&lt;p&gt;This is not much like what the current AI situation looks like, if you think AI poses a substantial risk of destroying the world.&lt;/p&gt;



&lt;h2&gt;The suicide race&lt;/h2&gt;



&lt;p&gt;A closer model: as above except if anyone chooses to build, everything is destroyed (everyone loses all their stuff—ten units of value—as well as one unit if they built).&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter is-resized&quot;&gt;&lt;a href=&quot;https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc8c586a-0b5e-4197-b9be-d083a6531fb3_424x410.png&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;&gt;&lt;img decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc8c586a-0b5e-4197-b9be-d083a6531fb3_424x410.png&quot; alt=&quot;&quot; width=&quot;293&quot; height=&quot;284&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;This is importantly different from the classic ‘arms race’ in that pressing the ‘everyone loses now’ button isn’t an equilibrium strategy.&lt;/p&gt;



&lt;p&gt;That is: for anyone who thinks powerful misaligned AI represents near-certain death, the existence of other possible AI builders is not any reason to ‘race’.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But few people are that pessimistic. How about a milder version where there’s a good chance that the players ‘align the AI’?&lt;/p&gt;



&lt;h2&gt;&lt;strong&gt;The safety-or-suicide race&lt;/strong&gt;&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Ok, let’s do a game like the last but where if anyone builds, everything is only maybe destroyed (minus ten to all), and in the case of survival, everyone returns to the original arms race fun of redistributing stuff based on who built more than whom (+10 to a builder and -10 to a non-builder if there is one of each). So if you build AI alone, and get lucky on the probabilistic apocalypse, can still win big.&lt;/p&gt;



&lt;p&gt;Let’s take 50% as the chance of doom if any building happens. Then we have a game whose expected payoffs are half way between those in the last two games:&lt;/p&gt;


&lt;div class=&quot;wp-block-image is-style-default&quot;&gt;
&lt;figure class=&quot;aligncenter is-resized&quot;&gt;&lt;a href=&quot;https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeaa5be6-1f1b-4c7f-ad58-571c26df5a37_424x410.png&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;&gt;&lt;img decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeaa5be6-1f1b-4c7f-ad58-571c26df5a37_424x410.png&quot; alt=&quot;&quot; width=&quot;292&quot; height=&quot;282&quot; /&gt;&lt;/a&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;&lt;strong&gt;(These are expected payoffs—the minus one unit return to building alone comes from the one unit cost of building, plus half a chance of losing ten in an extinction event and half a chance of taking ten from your opponent in a world takeover event.)&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;Now you want to do whatever the other player is doing: build if they’ll build, pass if they’ll pass.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;If the odds of destroying the world were very low, this would become the original arms race, and you’d always want to build. If very high, it would become the suicide race, and you’d never want to build. What the probabilities have to be in the real world to get you into something like these different phases is going to be different, because all these parameters are made up (the downside of human extinction is not 10x the research costs of building powerful AI, for instance).&lt;/p&gt;



&lt;p&gt;But my point stands: even in terms of simplish models, it’s very non-obvious that we are in or near an arms race. And therefore, very non-obvious that racing to build advanced AI faster is even promising at a first pass.&lt;/p&gt;



&lt;p&gt;In less game-theoretic terms: if you don’t seem anywhere near solving alignment, then racing as hard as you can to be the one who it falls upon to have solved alignment—especially if that means having less time to do so, though I haven’t discussed that here—is probably unstrategic. Having more ideologically pro-safety AI designers win an ‘arms race’ against less concerned teams is futile if you don’t have a way for such people to implement enough safety to actually not die, which seems like a very live possibility. (&lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://twitter.com/robbensinger/status/1536954285040119809&quot;&gt;Robby Bensinger&lt;/a&gt; and maybe Andrew Critch somewhere make similar points.)&lt;/p&gt;



&lt;p&gt;Conversations with my friends on this kind of topic can go like this:&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;&lt;strong&gt;Me&lt;/strong&gt;: there’s no real incentive to race if the prize is mutual death&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Them&lt;/strong&gt;: sure, but it isn’t—if there’s a sliver of hope of surviving unaligned AI, and if your side taking control in that case is a bit better in expectation, and if they are going to build powerful AI anyway, then it’s worth racing. The whole future is on the line!&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Me:&lt;/strong&gt; Wouldn’t you still be better off directing your own efforts to safety, since your safety efforts will also help everyone end up with a safe AI?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Them&lt;/strong&gt;: It will probably only help them somewhat—you don’t know if the other side will use your safety research. But also, it’s not just that they have less safety research. Their values are probably worse, by your lights.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Me&lt;/strong&gt;: If they succeed at alignment, are foreign values really worse than local ones? Probably any humans with vast intelligence at hand have a similar shot at creating a glorious human-ish utopia, no?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Them&lt;/strong&gt;: No, even if you’re right that being similarly human gets you to similar values in the end, the other parties might be more foolish than our side, and lock-in&lt;a href=&quot;#footnote-7&quot;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; some poorly thought-through version of their values that they want at the moment, or even if all projects would be so foolish, our side might have better poorly thought-through values to lock in, as well as being more likely to use safety ideas at all. Even if racing is very likely to lead to death, and survival is very likely to lead to squandering most of the value, in that sliver of happy worlds so much is at stake in whether it is us or someone else doing the squandering!&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Me&lt;/strong&gt;: Hmm, seems complicated, I’m going to need paper for this.&lt;/p&gt;
&lt;/blockquote&gt;



&lt;h2&gt;&lt;strong&gt;The complicated race/anti-race&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://docs.google.com/spreadsheets/d/1tq5jiok33sh89xQxLSBIZQGiuQ8hc8_n0673YU68poM/edit?usp=sharing&quot;&gt;Here&lt;/a&gt; is a spreadsheet of models you can make a copy of and play with.&lt;/p&gt;



&lt;p&gt;The first model is like this:&lt;/p&gt;



&lt;ol&gt;
&lt;li&gt;Each player divides their effort between safety and capabilities&lt;/li&gt;



&lt;li&gt;One player ‘wins’, i.e. builds ‘AGI’ (artificial general intelligence) first.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;P(Alice wins) is a logistic function of Alice’s capabilities investment relative to Bob’s&lt;/li&gt;



&lt;li&gt;Each players’ total safety is their own safety investment plus a fraction of the other’s safety investment.&lt;/li&gt;



&lt;li&gt;For each player there is some distribution of outcomes if they achieve safety, and a set of outcomes if they do not, which takes into account e.g. their proclivities for enacting stupid near-term lock-ins.&lt;/li&gt;



&lt;li&gt;The outcome is a distribution over winners and states of alignment, each of which is a distribution of worlds (e.g. utopia, near-term good lock-in..)&lt;/li&gt;



&lt;li&gt;That all gives us a number of utils (Delicious utils!)&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The second model is the same except that instead of dividing effort between safety and capabilities, you choose a speed, and the amount of alignment being done by each party is an exogenous parameter.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These models probably aren’t very good, but so far support a key claim I want to make here: it’s pretty non-obvious whether one should go faster or slower in this kind of scenario—it’s sensitive to a lot of different parameters in plausible ranges.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Furthermore, I don’t think the results of quantitative analysis match people’s intuitions here.&lt;/p&gt;



&lt;p&gt;For example, here’s a situation which I think sounds intuitively like a you-should-race world, but where in the first model above, you should actually go as slowly as possible (this should be the one plugged into the spreadsheet now):&lt;/p&gt;



&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AI is pretty safe:&lt;/strong&gt; unaligned AGI has a mere 7% chance of causing doom, plus a further 7% chance of causing short term lock-in of something mediocre&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Your opponent risks bad lock-in:&lt;/strong&gt; If there’s a ‘lock-in’ of something mediocre, your opponent has a 5% chance of locking in something actively terrible, whereas you’ll always pick good mediocre lock-in world (and mediocre lock-ins are either 5% as good as utopia, -5% as good)&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Your opponent risks messing up utopia:&lt;/strong&gt; In the event of aligned AGI, you will reliably achieve the best outcome, whereas your opponent has a 5% chance of ending up in a ‘mediocre bad’ scenario then too.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Safety investment obliterates your chance of getting to AGI first: &lt;/strong&gt;moving from no safety at all to full safety means you go from a 50% chance of being first to a 0% chance&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Your opponent is racing: &lt;/strong&gt;Your opponent is investing everything in capabilities and nothing in safety&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Safety work helps others at a steep discount:&lt;/strong&gt;&amp;nbsp; your safety work contributes 50% to the other player’s safety&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Your best bet here (on this model) is still to maximize safety investment. Why? Because by aggressively pursuing safety, you can get the other side half way to full safety, which is worth a lot more than than the lost chance of winning. Especially since if you ‘win’, you do so without much safety, and your victory without safety is worse than your opponent’s victory with safety, even if that too is far from perfect.&lt;/p&gt;



&lt;p&gt;So if you are in a situation in this space, and the other party is racing, it’s not obvious if it is even in your narrow interests within the game to go faster at the expense of safety, though it may be.&lt;/p&gt;



&lt;p&gt;These models are flawed in many ways, but I think they are better than the intuitive models that support arms-racing. My guess is that the next better still models remain nuanced.&lt;/p&gt;



&lt;h2&gt;Other equilibria and other games&lt;/h2&gt;



&lt;p&gt;Even if it would be in your interests to race if the other person were racing, ‘(do nothing, do nothing)’ is often an equilibrium too in these games. At least for various settings of the parameters. It doesn’t necessarily make sense to do nothing in the hope of getting to that equilibrium if you know your opponent to be mistaken about that and racing anyway, but in conjunction with communicating with your ‘opponent’, it seems like a theoretically good strategy.&lt;/p&gt;



&lt;p&gt;This has all been assuming the structure of the game. I think the traditional response to an arms race situation is to remember that you are in a more elaborate world with all kinds of unmodeled affordances, and try to get out of the arms race.&amp;nbsp;&lt;/p&gt;



&lt;h1&gt;Being friends with risk-takers&lt;/h1&gt;



&lt;h2&gt;&lt;strong&gt;Caution is cooperative&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Another big concern is that pushing for slower AI progress is ‘defecting’ against AI researchers who are friends of the AI safety community.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For instance &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://www.lesswrong.com/posts/CXaQj85r4LtafCBi8/should-we-postpone-agi-until-we-reach-safety?commentId=w2vSA2cPvE8hR563Z&quot;&gt;Steven Byrnes&lt;/a&gt;:&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;“I think that trying to slow down research towards AGI through regulation would fail, because everyone (politicians, voters, lobbyists, business, etc.) likes scientific research and technological development, it creates jobs, it cures diseases, etc. etc., and you&amp;#8217;re saying we should have less of that. So I think the effort would fail, and also be massively counterproductive by making the community of AI researchers see the community of AGI safety / alignment people as their enemies, morons, weirdos, Luddites, whatever.”&lt;/p&gt;
&lt;/blockquote&gt;



&lt;p&gt;(Also a good example of the view criticized earlier, that regulation of things that create jobs and cure diseases just doesn’t happen.)&lt;/p&gt;



&lt;p&gt;Or Eliezer Yudkowsky, on worry that spreading fear about AI would alienate top AI labs:&lt;/p&gt;



&lt;figure class=&quot;wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter&quot;&gt;&lt;div class=&quot;wp-block-embed__wrapper&quot;&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-width=&quot;550&quot; data-dnt=&quot;true&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;This is the primary reason I didn&amp;#39;t, and told others not to, earlier connect the point about human extinction from AGI with AI labs.  Kerry has correctly characterized the position he is arguing against, IMO.  I myself estimate the public will be toothless vs AGI lab heads.&lt;/p&gt;&amp;mdash; Eliezer Yudkowsky (@ESYudkowsky) &lt;a href=&quot;https://twitter.com/ESYudkowsky/status/1555272014939598848?ref_src=twsrc%5Etfw&quot;&gt;August 4, 2022&lt;/a&gt;&lt;/blockquote&gt;&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;&lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://twitter.com/ESYudkowsky/status/1555272014939598848?s=20&amp;amp;t=H6JiYdHs7kAnoaZshPV5sg&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;I don’t think this is a natural or reasonable way to see things, because:&lt;/p&gt;



&lt;ol&gt;
&lt;li&gt;The researchers themselves probably don’t want to destroy the world. Many of them also actually &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/&quot;&gt;agree&lt;/a&gt; that AI is a serious existential risk. So in two natural ways, pushing for caution is cooperative with many if not most AI researchers.&lt;/li&gt;



&lt;li&gt;AI researchers do not have a moral right to endanger the world, that someone would be stepping on by requiring that they move more cautiously. Like, why does &amp;#8216;cooperation&amp;#8217; look like the safety people bowing to what the more reckless capabilities people want, to the point of fearing to represent their actual interests, while the capabilities people uphold their side of the ‘cooperation’ by going ahead and building dangerous AI? This situation might make sense as a natural consequence of different people’s power in the situation. But then don’t call it a ‘cooperation’, from which safety-oriented parties would be dishonorably ‘defecting’ were they to consider exercising any power they did have.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;It could be that people in control of AI capabilities would respond negatively to AI safety people pushing for slower progress. But that should be called ‘we might get punished’ not ‘we shouldn’t defect’. ‘Defection’ has moral connotations that are not due. Calling one side pushing for their preferred outcome ‘defection’ unfairly disempowers them by wrongly setting commonsense morality against them.&lt;/p&gt;



&lt;p&gt;At least if it is the safety side. If any of the available actions are ‘defection’ that the world in general should condemn, I claim that it is probably ‘building machines that will plausibly destroy the world, or standing by while it happens’.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;(This would be more complicated if the people involved were confident that they wouldn’t destroy the world and I merely disagreed with them. But &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/&quot;&gt;about half of surveyed researchers&lt;/a&gt; are actually more pessimistic than me. And in a situation where the median AI researcher thinks the field has a 5-10% chance of causing human extinction, how confident can any responsible person be in their own judgment that it is safe?)&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;On top of all that, I worry that highlighting the narrative that wanting more cautious progress is defection is further destructive, because it makes it more likely that AI capabilities people see AI safety people as thinking of themselves as betraying AI researchers, if anyone engages in any such efforts. Which makes the efforts more aggressive. Like, if every time you see friends, you refer to it as ‘cheating on my partner’, your partner may reasonably feel hurt by your continual desire to see friends, even though the activity itself is innocuous.&lt;/p&gt;



&lt;h2&gt;&lt;strong&gt;‘We’ are not the US, ‘we’ are not the AI safety community&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;“If ‘we’ try to slow down AI, then the other side might win.” “If ‘we’ ask for regulation, then it might harm ‘our’ relationships with AI capabilities companies.” Who are these ‘we’s? Why are people strategizing for those groups in particular?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Even if slowing AI were uncooperative, and it were important for the AI Safety community to cooperate with the AI capabilities community, couldn’t one of the many people not in the AI Safety community work on it?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I have a &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://meteuphoric.com/2010/08/23/who-are-we/&quot;&gt;longstanding irritation&lt;/a&gt; with thoughtless talk about what ‘we’ should do, without regard for what collective one is speaking for. So I may be too sensitive about it here. But I think confusions arising from this have genuine consequences.&lt;/p&gt;



&lt;p&gt;I think when people say ‘we’ here, they generally imagine that they are strategizing on behalf of, a) the AI safety community, b) the USA, c) themselves or d) they and their readers. But those are a small subset of people, and not even obviously the ones the speaker can most influence (does the fact that you are sitting in the US really make the US more likely to listen to your advice than e.g. Estonia? Yeah probably on average, but not infinitely much.) If these naturally identified-with groups don’t have good options, that hardly means there are no options to be had, or to be communicated to other parties. Could the speaker speak to a different ‘we’? Maybe someone in the ‘we’ the speaker has in mind knows someone not in that group? If there is a strategy for anyone in the world, and you can talk, then there is probably a strategy for you.&lt;/p&gt;



&lt;p&gt;The starkest appearance of error along these lines to me is in writing off the slowing of AI as inherently destructive of relations between the AI safety community and other AI researchers. If we grant that such activity would be seen as a betrayal (which seems unreasonable to me, but maybe), surely it could only be a betrayal if carried out by the AI safety community. There are quite a lot of people who aren’t in the AI safety community and have a stake in this, so maybe some of them could do something. It seems like a huge oversight to give up on all slowing of AI progress because you are only considering affordances available to the AI Safety Community.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Another example: if the world were in the basic arms race situation sometimes imagined, and the United States would be willing to make laws to mitigate AI risk, but could not because China would barge ahead, then that means China is in a great place to mitigate AI risk. Unlike the US, China could propose mutual slowing down, and the US would go along. Maybe it’s not impossible to communicate this to relevant people in China.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;An oddity of this kind of discussion which feels related is the persistent assumption that one’s ability to act is restricted to the United States. Maybe I fail to understand the extent to which Asia is an alien and distant land where agency doesn’t apply, but for instance I just wrote to like a thousand machine learning researchers there, and maybe a hundred wrote back, and it was a lot like interacting with people in the US.&lt;/p&gt;



&lt;p&gt;I’m pretty ignorant about what interventions will work in any particular country, including the US, but I just think it’s weird to come to the table assuming that you can essentially only affect things in one country. Especially if the situation is that you believe you have unique knowledge about what is in the interests of people in other countries. Like, fair enough I would be deal-breaker-level pessimistic if you wanted to get an Asian government to elect you leader or something. But if you think advanced AI is highly likely to destroy the world, including other countries, then the situation is totally different. If you are right, then everyone’s incentives are basically aligned.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I more weakly suspect some related mental shortcut is misshaping the discussion of arms races in general. The thought that something is a ‘race’ seems much stickier than alternatives, even if the true incentives don’t really make it a race. Like, against the laws of game theory, people sort of expect the enemy to try to believe falsehoods, because it will better contribute to their racing. And this feels like realism. The uncertain details of billions of people one barely knows about, with all manner of interests and relationships, just really wants to form itself into an ‘us’ and a ‘them’ in zero-sum battle. This is a mental shortcut that could really kill us.&lt;/p&gt;



&lt;p&gt;My impression is that in practice, for many of the technologies slowed down for risk or ethics, mentioned in section ‘Extremely valuable technologies’ above, countries with fairly disparate cultures have converged on similar approaches to caution. I take this as evidence that none of ethical thought, social influence, political power, or rationality are actually very siloed by country, and in general the ‘countries in contest’ model of everything isn’t very good.&lt;/p&gt;



&lt;h1&gt;Notes on tractability&lt;/h1&gt;



&lt;h2&gt;&lt;strong&gt;Convincing people doesn’t seem that hard&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;When I say that ‘coordination’ can just look like popular opinion punishing an activity, or that other countries don’t have much real incentive to build machines that will kill them, I think a common objection is that convincing people of the real situation is hopeless. The picture seems to be that the argument for AI risk is extremely sophisticated and only able to be appreciated by the most elite of intellectual elites—e.g. it’s hard enough to convince professors on Twitter, so surely the masses are beyond its reach, and foreign governments too.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This doesn’t match my overall experience on various fronts.&lt;/p&gt;



&lt;p&gt;Some observations:&lt;/p&gt;



&lt;ul&gt;
&lt;li&gt;The &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/&quot;&gt;median surveyed ML researcher&lt;/a&gt; seems to think AI will destroy humanity with 5-10% chance, as I mentioned&lt;/li&gt;



&lt;li&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like&quot;&gt;Often people are already intellectually convinced&lt;/a&gt; but haven’t integrated that into their behavior, and it isn’t hard to help them organize to act on their tentative beliefs&lt;/li&gt;



&lt;li&gt;As noted by &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://astralcodexten.substack.com/p/why-not-slow-ai-progress&quot;&gt;Scott&lt;/a&gt;, a lot of AI safety people have gone into AI capabilities including running AI capabilities orgs, so those people presumably consider AI to be risky already&lt;/li&gt;



&lt;li&gt;I don’t remember ever having any trouble discussing AI risk with random strangers. Sometimes they are also fairly worried (e.g. a makeup artist at Sephora gave an extended rant about the dangers of advanced AI, and my driver in Santiago excitedly concurred and showed me &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://en.wikipedia.org/wiki/Homo_Deus:_A_Brief_History_of_Tomorrow&quot;&gt;&lt;em&gt;Homo Deus&lt;/em&gt;&lt;/a&gt; open on his front seat). The form of the concerns are probably a bit different from those of the AI Safety community, but I think broadly closer to, ‘AI agents are going to kill us all’ than ‘algorithmic bias will be bad’. I can’t remember how many times I have tried this, but pre-pandemic I used to talk to Uber drivers a lot, due to having no idea how to avoid it. I explained AI risk to my therapist recently, as an aside regarding his sense that I might be catastrophizing, and I feel like it went okay, though we may need to discuss again.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;My impression is that most people haven’t even come into contact with the arguments that might bring one to agree precisely with the AI safety community. For instance, my guess is that a lot of people assume that someone actually programmed modern AI systems, and if you told them that in fact they are random connections jiggled in an gainful direction unfathomably many times, just as mysterious to their makers, they might also fear misalignment.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Nick Bostrom, Eliezer Yudkokwsy, and other early thinkers have had decent success at convincing a bunch of other people to worry about this problem, e.g. me. And to my knowledge, without writing any compelling and accessible account of why one should do so that would take less than two hours to read.&lt;/li&gt;



&lt;li&gt;I arrogantly think I could write a broadly compelling and accessible case for AI risk&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;My weak guess is that immovable AI risk skeptics are concentrated in intellectual circles near the AI risk people, especially on Twitter, and that people with less of a horse in the intellectual status race are more readily like, ‘oh yeah, superintelligent robots are probably bad’. It’s not clear that most people even need convincing that there is a problem, though they don’t seem to consider it the most pressing problem in the world. (Though all of this may be different in cultures I am more distant from, e.g. in China.) I’m pretty non-confident about this, but skimming &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://wiki.aiimpacts.org/doku.php?id=responses_to_ai:public_opinion_on_ai:surveys_of_public_opinion_on_ai:surveys_of_us_public_opinion_on_ai&amp;amp;s[]=american&amp;amp;s[]=public&amp;amp;s[]=opinion&amp;amp;s[]=survey&quot; target=&quot;_blank&quot;&gt;survey evidence&lt;/a&gt; suggests there is substantial though not overwhelming public concern about AI in the US&lt;a id=&quot;footnote-anchor-8&quot; href=&quot;#footnote-8&quot;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;



&lt;h2&gt;&lt;strong&gt;Do you need to convince everyone?&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;I could be wrong, but I’d guess convincing the ten most relevant leaders of AI labs that this is a massive deal, worth prioritizing, actually gets you a decent slow-down. I don’t have much evidence for this.&lt;/p&gt;



&lt;h2&gt;&lt;strong&gt;Buying time is big&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;You probably aren’t going to avoid AGI forever, and maybe huge efforts will buy you a couple of years.&lt;a id=&quot;footnote-anchor-9&quot; href=&quot;#footnote-9&quot;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; Could that even be worth it?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Seems pretty plausible:&lt;/p&gt;



&lt;ol&gt;
&lt;li&gt;Whatever kind of other AI safety research or policy work people were doing could be happening at a non-negligible rate per year. (Along with all other efforts to make the situation better—if you buy a year, that’s eight billion extra person years of time, so only a tiny bit has to be spent usefully for this to be big. If a lot of people are worried, that doesn’t seem crazy.)&lt;/li&gt;



&lt;li&gt;Geopolitics just changes pretty often. If you seriously think a big determiner of how badly things go is inability to coordinate with certain groups, then every year gets you non-negligible opportunities for the situation changing in a favorable way.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Public opinion can change a lot quickly. If you can only buy one year, you might still be buying a decent shot of people coming around and granting you more years. Perhaps especially if new evidence is actively avalanching in—people changed their minds a lot in February 2020.&lt;/li&gt;



&lt;li&gt;Other stuff happens over time. If you can take your doom today or after a couple of years of random events happening, the latter seems non-negligibly better in general.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;It is also not obvious to me that these are the time-scales on the table. My sense is that things which are slowed down by regulation or general societal distaste are often slowed down much more than a year or two, and Eliezer’s stories presume that the world is full of collectives either trying to destroy the world or badly mistaken about it, which is not a foregone conclusion.&lt;/p&gt;



&lt;h2&gt;&lt;strong&gt;Delay is probably finite by default&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;While some people worry that any delay would be so short as to be negligible, others seem to fear that if AI research were halted, it would never start again and we would fail to go to space or something. This sounds so wild to me that I think I’m missing too much of the reasoning to usefully counterargue.&lt;/p&gt;



&lt;h2&gt;Obstruction doesn’t need discernment&lt;/h2&gt;



&lt;p&gt;Another purported risk of trying to slow things down is that it might involve getting regulators involved, and they might be fairly ignorant about the details of futuristic AI, and so tenaciously make the wrong regulations. Relatedly, if you call on the public to worry about this, they might have inexacting worries that call for impotent solutions and distract from the real disaster.&lt;/p&gt;



&lt;p&gt;I don’t buy it. If all you want is to slow down a broad area of activity, my guess is that ignorant regulations do just fine at that every day (usually unintentionally). In particular, my impression is that if you mess up regulating things, a usual outcome is that many things are randomly slower than hoped. If you wanted to speed a specific thing up, that’s a very different story, and might require understanding the thing in question.&lt;/p&gt;



&lt;p&gt;The same goes for social opposition. Nobody need understand the details of how genetic engineering works for its ascendancy to be seriously impaired by people not liking it. Maybe by their lights it still isn’t optimally undermined yet, but just not liking anything in the vicinity does go a long way.&lt;/p&gt;



&lt;p&gt;This has nothing to do with regulation or social shaming specifically. You need to understand much less about a car or a country or a conversation to mess it up than to make it run well. It is a consequence of the &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://en.wikipedia.org/wiki/Anna_Karenina_principle&quot;&gt;general rule&lt;/a&gt; that there are many more ways for a thing to be dysfunctional than functional: destruction is easier than creation.&lt;/p&gt;



&lt;p&gt;Back at the object level, I tentatively expect efforts to broadly slow down things in the vicinity of AI progress to slow down AI progress on net, even if poorly aimed.&lt;/p&gt;



&lt;h1&gt;Safety from speed, clout from complicity&lt;/h1&gt;



&lt;p&gt;Maybe it’s actually better for safety to have AI go fast at present, for various reasons. Notably:&lt;/p&gt;



&lt;ol&gt;
&lt;li&gt;Implementing what can be implemented as soon as possible probably means smoother progress, which is probably safer because a) it makes it harder for one party shoot ahead of everyone and gain power, and b) people make better choices all around if they are correct about what is going on (e.g. they don’t put trust in systems that turn out to be much more powerful than expected).&lt;/li&gt;



&lt;li&gt;If the main thing achieved by slowing down AI progress is more time for safety research, and safety research is more effective when carried out in the context of more advanced AI, and there is a certain amount of slowing down that can be done (e.g. because one is in fact in an arms race but has some lead over competitors), then it might better to use one’s slowing budget later.&lt;/li&gt;



&lt;li&gt;If there is some underlying curve of potential for progress (e.g. if money that might be spent on hardware just grows a certain amount each year), then perhaps if we push ahead now that will naturally require they be slower later, so it won’t affect the overall time to powerful AI, but will mean we spend more time in the informative pre-catastrophic-AI era.&lt;/li&gt;



&lt;li&gt;(More things go here I think)&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;And maybe it’s worth it to work on capabilities research at present, for instance because:&lt;/p&gt;



&lt;ol&gt;
&lt;li&gt;As a researcher, working on capabilities prepares you to work on safety&lt;/li&gt;



&lt;li&gt;You think the room where AI happens will afford good options for a person who cares about safety&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;These all seem plausible. But also plausibly wrong. I don’t know of a decisive analysis of any of these considerations, and am not going to do one here. My impression is that they could basically all go either way.&lt;/p&gt;



&lt;p&gt;I am actually particularly skeptical of the final argument, because if you believe what I take to be the normal argument for AI risk—that superhuman artificial agents won’t have acceptable values, and will aggressively manifest whatever values they do have, to the sooner or later annihilation of humanity—then the sentiments of the people turning on such machines seem like a very small factor, so long as they still turn the machines on. And I suspect that ‘having a person with my values doing X’ is commonly overrated. But the world is messier than these models, and I’d still pay a lot to be in the room to try.&lt;/p&gt;



&lt;h1&gt;Moods and philosophies, heuristics and attitudes&amp;nbsp;&lt;/h1&gt;



&lt;p&gt;It’s not clear what role these psychological characters should play in a rational assessment of how to act, but I think they do play a role, so I want to argue about them.&lt;/p&gt;



&lt;h2&gt;&lt;strong&gt;&lt;em&gt;Technological choice is not luddism&lt;/em&gt;&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Some technologies are better than others [citation not needed]. The best pro-technology visions should disproportionately involve awesome technologies and avoid shitty technologies, I claim. If you think AGI is highly likely to destroy the world, then it is the pinnacle of shittiness as a technology. Being opposed to having it into your techno-utopia is about as luddite as refusing to have &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://en.wikipedia.org/wiki/Doramad_Radioactive_Toothpaste&quot; target=&quot;_blank&quot;&gt;radioactive toothpaste&lt;/a&gt; there. Colloquially, Luddites are against progress if it comes as technology.&lt;a id=&quot;footnote-anchor-10&quot; href=&quot;#footnote-10&quot;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt; Even if that’s a terrible position, its wise reversal is not the endorsement of all ‘technology’, regardless of whether it comes as progress.&lt;/p&gt;



&lt;h2&gt;&lt;strong&gt;&lt;em&gt;Non-AGI visions of near-term thriving&lt;/em&gt;&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Perhaps slowing down AI progress means foregoing our own generation’s hope for life-changing technologies. Some people thus find it psychologically difficult to aim for less AI progress (with its real personal costs), rather than shooting for the perhaps unlikely ‘safe AGI soon’ scenario.&lt;/p&gt;



&lt;p&gt;I’m not sure that this is a real dilemma. The narrow AI progress we have seen already—i.e. further applications of current techniques at current scales—seems plausibly able to help a lot with longevity and other medicine for instance. And to the extent AI efforts could be focused on e.g. medically relevant narrow systems over creating agentic scheming gods, it doesn’t sound crazy to imagine making more progress on anti-aging etc as a result (even before taking into account the probability that the agentic scheming god does not prioritize your physical wellbeing as hoped). Others disagree with me here.&lt;/p&gt;



&lt;h2&gt;&lt;strong&gt;&lt;em&gt;Robust priors vs. specific galaxy-brained models&lt;/em&gt;&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;There are things that are robustly good in the world, and things that are good on highly specific inside-view models and terrible if those models are wrong. Slowing dangerous tech development seems like the former, whereas forwarding arms races for dangerous tech between world superpowers seems more like the latter.&lt;a id=&quot;footnote-anchor-11&quot; href=&quot;#footnote-11&quot;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; There is a general question of how much to trust your reasoning and risk the galaxy-brained plan.&lt;a id=&quot;footnote-anchor-12&quot; href=&quot;#footnote-12&quot;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt; But whatever your take on that, I think we should all agree that the less thought you have put into it, the more you should regress to the robustly good actions. Like, if it just occurred to you to take out a large loan to buy a fancy car, you probably shouldn’t do it because most of the time it’s a poor choice. Whereas if you have been thinking about it for a month, you might be sure enough that you are in the rare situation where it will pay off.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;On this particular topic, it feels like people are going with the specific galaxy-brained inside-view terrible-if-wrong model off the bat, then not thinking about it more.&amp;nbsp;&lt;/p&gt;



&lt;h2&gt;&lt;strong&gt;&lt;em&gt;Cheems mindset/can’t do attitude&lt;/em&gt;&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Suppose you have a friend, and you say ‘let’s go to the beach’ to them. Sometimes the friend is like ‘hell yes’ and then even if you don’t have towels or a mode of transport or time or a beach, you make it happen. Other times, even if you have all of those things, and your friend nominally wants to go to the beach, they will note that they have a package coming later, and that it might be windy, and their jacket needs washing. And when you solve those problems, they will note that it’s not that long until dinner time. You might infer that in the latter case your friend just doesn’t want to go to the beach. And sometimes that is the main thing going on! But I think there are also broader differences in attitudes: sometimes people are looking for ways to make things happen, and sometimes they are looking for reasons that they can’t happen. This is sometimes called a ‘&lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://normielisation.substack.com/p/cheems-mindset&quot;&gt;cheems attitude&lt;/a&gt;’, or I like to call it (more accessibly) a ‘can’t do attitude’.&lt;/p&gt;



&lt;p&gt;My experience in talking about slowing down AI with people is that they seem to have a can’t do attitude. They don’t want it to be a reasonable course: they want to write it off.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Which both seems suboptimal, and is strange in contrast with historical attitudes to more technical problem-solving. (As highlighted in my dialogue from the start of the post.)&lt;/p&gt;



&lt;p&gt;It seems to me that if the same degree of can’t-do attitude were applied to technical safety, there would be no AI safety community because in 2005 Eliezer would have noticed any obstacles to alignment and given up and gone home.&lt;/p&gt;



&lt;p&gt;To quote a friend on this, what would it look like if we *actually tried*?&lt;/p&gt;



&lt;h1&gt;Conclusion&lt;/h1&gt;



&lt;p&gt;This has been a miscellany of critiques against a pile of reasons I’ve met for not thinking about slowing down AI progress. I don’t think we’ve seen much reason here to be very pessimistic about slowing down AI, let alone reason for not even thinking about it.&lt;/p&gt;



&lt;p&gt;I could go either way on whether any interventions to slow down AI in the near term are a good idea. My tentative guess is yes, but my main point here is just that we should think about it.&lt;/p&gt;



&lt;p&gt;A lot of opinions on this subject seem to me to be poorly thought through, in error, and to have wrongly repelled the further thought that might rectify them. I hope to have helped a bit here by examining some such considerations enough to demonstrate that there are no good grounds for immediate dismissal. There are difficulties and questions, but if the same standards for ambition were applied here as elsewhere, I think we would see answers and action.&lt;/p&gt;



&lt;h2&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Thanks to Scott Alexander, Adam Scholl, Matthijs Maas, Joe Carlsmith, Ben Weinstein-Raun, Ronny Fernandez, Aysja Johnson, Jaan Tallinn, Rick Korzekwa, Owain Evans, Andrew Critch, Michael Vassar, Jessica Taylor, Rohin Shah, Jeffrey Heninger, Zach Stein-Perlman, Anthony Aguirre, Matthew Barnett, David Krueger, Harlan Stewart, Rafe Kennedy, Nick Beckstead, Leopold Aschenbrenner, Michaël Trazzi, Oliver Habryka, Shahar Avin, Luke Muehlhauser, Michael Nielsen, Nathan Young and quite a few others for discussion and/or encouragement.&lt;/p&gt;



&lt;h2&gt;Notes&lt;/h2&gt;



&lt;p&gt;&lt;a id=&quot;footnote-1&quot; href=&quot;#footnote-anchor-1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; I haven’t heard this in recent times, so maybe views have changed. An example of earlier times: &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://blog.givewell.org/2015/09/30/differential-technological-development-some-early-thinking/&quot; target=&quot;_blank&quot;&gt;Nick Beckstead, 2015&lt;/a&gt;: “One idea we sometimes hear is that it would be harmful to speed up the development of artificial intelligence because not enough work has been done to ensure that when very advanced artificial intelligence is created, it will be safe. This problem, it is argued, would be even worse if progress in the field accelerated. However, very advanced artificial intelligence could be a useful tool for overcoming other potential global catastrophic risks. If it comes sooner—and the world manages to avoid the risks that it poses directly—the world will spend less time at risk from these other factors&amp;#8230;.&lt;br /&gt;&lt;br /&gt;I found that speeding up advanced artificial intelligence—according to my simple interpretation of these survey results—could easily result in reduced net exposure to the most extreme global catastrophic risks&amp;#8230;”&lt;br /&gt;&lt;/p&gt;



&lt;p&gt;&lt;a id=&quot;footnote-2&quot; href=&quot;#footnote-anchor-2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; This is closely related to Bostrom’s &lt;strong&gt;Technological completion conjecture:&lt;/strong&gt; “If scientific and technological development efforts do not effectively cease, then all important basic capabilities that could be obtained through some possible technology will be obtained.” (Bostrom, &lt;em&gt;Superintelligence&lt;/em&gt;, pp. 228, Chapter 14, 2014)&lt;br /&gt;&lt;br /&gt;Bostrom illustrates this kind of position (though apparently rejects it; from Superintelligence, found &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://forum.effectivealtruism.org/posts/g6549FAQpQ5xobihj/differential-technological-development&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;): “Suppose that a policymaker proposes to cut funding for a certain research field, out of concern for the risks or long-term consequences of some hypothetical technology that might eventually grow from its soil. She can then expect a howl of opposition from the research community. Scientists and their public advocates often say that it is futile to try to control the evolution of technology by blocking research. If some technology is feasible (the argument goes) it will be developed regardless of any particular policymaker’s scruples about speculative future risks. Indeed, the more powerful the capabilities that a line of development promises to produce, the surer we can be that somebody, somewhere, will be motivated to pursue it. Funding cuts will not stop progress or forestall its concomitant dangers.”&lt;br /&gt;&lt;br /&gt;This kind of thing is also discussed by &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://journals.sagepub.com/doi/10.1177/0162243915579283&quot; target=&quot;_blank&quot;&gt;Dafoe&lt;/a&gt; and &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4118618&quot; target=&quot;_blank&quot;&gt;Sundaram, Maas &amp;amp; Beard&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;



&lt;p&gt;&lt;a id=&quot;footnote-3&quot; href=&quot;#footnote-anchor-3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; (Some inspiration from Matthijs Maas’ &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://airtable.com/shrVHVYqGnmAyEGsz&quot; target=&quot;_blank&quot;&gt;spreadsheet&lt;/a&gt;, from &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://verfassungsblog.de/paths-untaken/&quot; target=&quot;_blank&quot;&gt;Paths Untaken&lt;/a&gt;, and from GPT-3.)&lt;/p&gt;



&lt;p&gt;&lt;a id=&quot;footnote-4&quot; href=&quot;#footnote-anchor-4&quot;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; From a private conversation with Rick Korzekwa, who may have read &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1139110/&quot; target=&quot;_blank&quot;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1139110/&lt;/a&gt; and an internal draft at &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;http://aiimpacts.org/&quot; target=&quot;_blank&quot;&gt;AI Impacts&lt;/a&gt;, probably forthcoming.&lt;/p&gt;



&lt;p&gt;&lt;a id=&quot;footnote-5&quot; href=&quot;#footnote-anchor-5&quot;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; More &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://forum.effectivealtruism.org/posts/J6QCmkQmuRaP7skje/differential-technology-development-preprint-on-the-concept&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt; and &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://forum.effectivealtruism.org/topics/differential-progress&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;. I haven’t read any of these, but it’s been a topic of discussion for a while.&lt;/p&gt;



&lt;p&gt;&lt;a id=&quot;footnote-6&quot; href=&quot;#footnote-anchor-6&quot;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; “To aid in promoting secrecy, schemes to improve incentives were devised. One method sometimes used was for authors to send papers to journals to establish their claim to the finding but ask that publication of the papers be delayed indefinitely.26,27,28,29 Szilárd also suggested offering funding in place of credit in the short term for scientists willing to submit to secrecy and organizing limited circulation of key papers.30” &amp;#8211; &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://intelligence.org/files/SzilardNuclearWeapons.pdf&quot; target=&quot;_blank&quot;&gt;Me, previously&lt;/a&gt;&lt;/p&gt;



&lt;p&gt;&lt;a id=&quot;footnote-7&quot; href=&quot;#footnote-anchor-7&quot;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; &amp;#8216;Lock-in’ of values is the act of using powerful technology such as AI to ensure that specific values will stably control the future.&lt;/p&gt;



&lt;p&gt;&lt;a id=&quot;footnote-8&quot; href=&quot;#footnote-anchor-8&quot;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; And also &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://dl.acm.org/doi/abs/10.1145/3306618.3314232&quot; target=&quot;_blank&quot;&gt;in Britain&lt;/a&gt;:&lt;br /&gt;&lt;br /&gt;&amp;#8216;This paper discusses the results of a nationally representative survey of the UK population on their perceptions of AI…the most common visions of the impact of AI elicit significant anxiety. Only two of the eight narratives elicited more excitement than concern (AI making life easier, and extending life). Respondents felt they had no control over AI’s development, citing the power of corporations or government, or versions of technological determinism. Negotiating the deployment of AI will require contending with these anxieties.&amp;#8217;&lt;/p&gt;



&lt;p&gt;&lt;a id=&quot;footnote-9&quot; href=&quot;#footnote-anchor-9&quot;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; Or so worries Eliezer Yudkowsky—&lt;br /&gt;In &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy&quot; target=&quot;_blank&quot;&gt;MIRI announces new &amp;#8220;Death With Dignity&amp;#8221; strategy&lt;/a&gt;:&lt;/p&gt;



&lt;ul&gt;
&lt;li&gt;“&amp;#8230; this isn&amp;#8217;t primarily a social-political problem, of just getting people to listen.&amp;nbsp; Even if DeepMind listened, and Anthropic knew, and they both backed off from destroying the world, that would just mean Facebook AI Research destroyed the world a year(?) later.”&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;In &lt;a target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot; href=&quot;https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities&quot;&gt;AGI Ruin: A List of Lethalities&lt;/a&gt;:&lt;/p&gt;



&lt;ul&gt;
&lt;li&gt;“We can&amp;#8217;t just &amp;#8220;decide not to build AGI&amp;#8221; because GPUs are everywhere, and knowledge of algorithms is constantly being improved and published; 2 years after the leading actor has the capability to destroy the world, 5 other actors will have the capability to destroy the world.&amp;nbsp; The given lethal challenge is to solve within a time limit, driven by the dynamic in which, over time, increasingly weak actors with a smaller and smaller fraction of total computing power, become able to build AGI and destroy the world.&amp;nbsp; Powerful actors all refraining in unison from doing the suicidal thing just delays this time limit &amp;#8211; it does not lift it, unless computer hardware and computer software progress are both brought to complete severe halts across the whole Earth.&amp;nbsp; The current state of this cooperation to have every big actor refrain from doing the stupid thing, is that at present some large actors with a lot of researchers and computing power are led by people who vocally disdain all talk of AGI safety (eg Facebook AI Research).&amp;nbsp; Note that needing to solve AGI alignment only within a time limit, but with unlimited safe retries for rapid experimentation on the full-powered system; or only on the first critical try, but with an unlimited time bound; would both be terrifically humanity-threatening challenges by historical standards individually.”&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;&lt;a id=&quot;footnote-10&quot; href=&quot;#footnote-anchor-10&quot;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt; I’d guess real Luddites also thought the technological changes they faced were anti-progress, but in that case were they wrong to want to avoid them?&lt;/p&gt;



&lt;p&gt;&lt;a id=&quot;footnote-11&quot; href=&quot;#footnote-anchor-11&quot;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; I hear &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://forum.effectivealtruism.org/posts/cXBznkfoPJAjacFoT/are-you-really-in-a-race-the-cautionary-tales-of-szilard-and&quot; target=&quot;_blank&quot;&gt;this&lt;/a&gt; is an elaboration on this theme, but I haven’t read it.&lt;/p&gt;



&lt;p&gt;&lt;a id=&quot;footnote-12&quot; href=&quot;#footnote-anchor-12&quot;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt; &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://www.forourposterity.com/burkean-longtermism/&quot; target=&quot;_blank&quot;&gt;Leopold Aschenbrenner&lt;/a&gt; partly defines ‘Burkean Longtermism’ thus: “We should be skeptical of any radical inside-view schemes to positively steer the long-run future, given the froth of uncertainty about the consequences of our actions.”&lt;/p&gt;
&lt;/li&gt;&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;</content><author><name>Katja Grace</name></author><category term="ai" /><category term="meteuphoric" /><summary type="html">(Crossposted from AI Impacts Blog) Averting doom by not building the doom machine If you fear that someone will build a machine that will seize control of the world and annihilate humanity, then one kind of response is to try to build further machines that will seize control of the world even earlier without destroying it, forestalling the ruinous machine’s conquest. An alternative or complementary kind of response is to try to avert such machines being built at all, at least while the degree of their apocalyptic tendencies is ambiguous.  The latter approach seems to me  like the kind of basic and obvious thing worthy of at least consideration, and also in its favor, fits nicely in the genre ‘stuff that it isn’t that hard to imagine happening in the real world’. Yet my impression is that for people worried about extinction risk from artificial intelligence, strategies under the heading ‘actively slow down AI progress’ have historically been dismissed and ignored (though ‘don’t actively speed up AI progress’ is popular). The conversation near me over the years has felt a bit like this:  Some people: AI might kill everyone. We should design a godlike super-AI of perfect goodness to prevent that. Others: wow that sounds extremely ambitious Some people: yeah but it’s very important and also we are extremely smart so idk it could work [Work on it for a decade and a half] Some people: ok that’s pretty hard, we give up Others: oh huh shouldn’t we maybe try to stop the building of this dangerous AI?  Some people: hmm, that would involve coordinating numerous people—we may be arrogant enough to think that we might build a god-machine that can take over the world and remake it as a paradise, but we aren’t delusional This seems like an error to me. (And lately, to a bunch of other people.) </summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6333c761-362a-431d-ba20-d6fc9ffcddf1_512x512.png" /><media:content medium="image" url="https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6333c761-362a-431d-ba20-d6fc9ffcddf1_512x512.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Counterarguments to the basic AI risk case</title><link href="http://localhost:4000/2022/10/14/ai_counterargs.html" rel="alternate" type="text/html" title="Counterarguments to the basic AI risk case" /><published>2022-10-14T05:43:00-07:00</published><updated>2022-10-14T05:43:00-07:00</updated><id>http://localhost:4000/2022/10/14/ai_counterargs</id><content type="html" xml:base="http://localhost:4000/2022/10/14/ai_counterargs.html">&lt;p&gt;&lt;em&gt;Crossposted from &lt;a href=&quot;https://aiimpacts.org/counterarguments-to-the-basic-ai-x-risk-case/&quot;&gt;The AI Impacts blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This is going to be a list of holes I see in the basic argument for existential risk from superhuman AI systems&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;To start, here’s an outline of what I take to be the basic case&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;&lt;strong&gt;I. If superhuman AI systems are built, any given system is likely to be ‘goal-directed’&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Reasons to expect this:&lt;/p&gt;

  &lt;ol&gt;&lt;li&gt;Goal-directed behavior is likely to be valuable, e.g. economically. &lt;/li&gt;&lt;li&gt;Goal-directed entities may tend to arise from machine learning training processes not intending to create them (at least via the methods that are likely to be used).&lt;/li&gt;&lt;li&gt;‘Coherence arguments’ may imply that systems with some goal-directedness will become more strongly goal-directed over time.&lt;/li&gt;&lt;/ol&gt;

  &lt;p&gt;&lt;strong&gt;II. If goal-directed superhuman AI systems are built, their desired outcomes will probably be about as bad as an empty universe by human lights &lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Reasons to expect this:&lt;/p&gt;

  &lt;ol&gt;&lt;li&gt;Finding useful goals that aren’t extinction-level bad appears to be hard: we don’t have a way to usefully point at human goals, and divergences from human goals seem likely to produce goals that are in intense conflict with human goals, due to a) most goals producing &lt;a href=&quot;https://en.wikipedia.org/wiki/Instrumental_convergence&quot;&gt;convergent incentives&lt;/a&gt; for controlling everything, and b) value &lt;a href=&quot;https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile&quot;&gt;being&lt;/a&gt; ‘fragile’, such that an entity with ‘similar’ values will generally create a future of virtually no value.&lt;/li&gt;&lt;li&gt;Finding goals that are extinction-level bad and temporarily useful appears to be easy: for example, advanced AI with the sole objective ‘maximize company revenue’ might profit said company for a time before gathering the influence and wherewithal to pursue the goal in ways that blatantly harm society.&lt;/li&gt;&lt;li&gt;Even if humanity found acceptable goals, giving a powerful AI system any specific goals appears to be hard. We don’t know of any procedure to do it, and we have theoretical reasons to expect that AI systems produced through machine learning training will generally end up with goals other than those they were trained according to. Randomly aberrant goals resulting are probably extinction-level bad for reasons described in II.1 above.&lt;/li&gt;&lt;/ol&gt;

  &lt;p&gt;&lt;strong&gt;III. If most goal-directed superhuman AI systems have bad goals, the future will very likely be bad&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;That is, a set of ill-motivated goal-directed superhuman AI systems, of a scale likely to occur, would be capable of taking control over the future from humans. This is supported by at least one of the following being true:&lt;/p&gt;

  &lt;ol&gt;&lt;li&gt;&lt;strong&gt;Superhuman AI would destroy humanity rapidly. &lt;/strong&gt;This may be via ultra-powerful capabilities at e.g. technology design and strategic scheming, or through gaining such powers in an ‘intelligence explosion‘ (self-improvement cycle). Either of those things may happen either through exceptional heights of intelligence being reached or through highly destructive ideas being available to minds only mildly beyond our own.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Superhuman AI would gradually come to control the future via accruing power and resources.&lt;/strong&gt; Power and resources would be more available to the AI system(s) than to humans on average, because of the AI having far greater intelligence.&lt;/li&gt;&lt;/ol&gt;

&lt;/blockquote&gt;

&lt;p&gt;Below is a list of gaps in the above, as I see it, and counterarguments. A ‘gap’ is not necessarily unfillable, and may have been filled in any of the countless writings on this topic that I haven’t read. I might even think that a given one can probably be filled. I just don’t know what goes in it.  &lt;/p&gt;

&lt;p&gt;This blog post is an attempt to run various arguments by you all on the way to making pages on &lt;a href=&quot;http://aiimpacts.org/&quot;&gt;AI Impacts&lt;/a&gt; about arguments for AI risk and corresponding counterarguments. At some point in that process I hope to also read others’ arguments, but this is not that day. So what you have here is a bunch of arguments that occur to me, not an exhaustive literature review. &lt;/p&gt;

&lt;!--ex--&gt;

&lt;h2&gt;&lt;strong&gt;Counterarguments&lt;/strong&gt;&lt;/h2&gt;

&lt;h3&gt;&lt;strong&gt;&lt;em&gt;A. Contra “superhuman AI systems will be ‘goal-directed’”&lt;/em&gt;&lt;/strong&gt;&lt;/h3&gt;

&lt;h4&gt;Different calls to ‘goal-directedness’ don’t necessarily mean the same concept&lt;/h4&gt;

&lt;p&gt;‘Goal-directedness’ is a vague concept. It is unclear that the ‘goal-directednesses’ that are favored by economic pressure, training dynamics or coherence arguments (the component arguments in part I of the argument above) are the same ‘goal-directedness’ that implies a zealous drive to control the universe (i.e. that makes most possible goals very bad, fulfilling II above). &lt;/p&gt;

&lt;p&gt;One well-defined concept of goal-directedness is ‘utility maximization’: always doing what maximizes a particular utility function, given a particular set of beliefs about the world. &lt;/p&gt;

&lt;p&gt;Utility maximization does seem to quickly engender an interest in controlling literally everything, at least for many utility functions one might have&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. If you want things to go a certain way, then you have reason to control anything which gives you any leverage over that, i.e. potentially all resources in the universe (i.e. agents have ‘&lt;a href=&quot;https://en.wikipedia.org/wiki/Instrumental_convergence&quot;&gt;convergent instrumental goals&lt;/a&gt;’). This is in serious conflict with anyone else with resource-sensitive goals, even if &lt;em&gt;prima facie&lt;/em&gt; those goals didn’t look particularly opposed. For instance, a person who wants all things to be red and another person who wants all things to be cubes may not seem to be at odds, given that all things could be red cubes. However if these projects might each fail for lack of energy, then they are probably at odds. &lt;/p&gt;

&lt;p&gt;Thus utility maximization is a notion of goal-directedness that allows Part II of the argument to work, by making a large class of goals deadly.&lt;/p&gt;

&lt;p&gt;You might think that any other concept of ‘goal-directedness’ would also lead to this zealotry. If one is inclined toward outcome O in any plausible sense, then does one not have an interest in anything that might help procure O? No: if a system is not a ‘coherent’ agent, then it can have a tendency to bring about O in a range of circumstances, without this implying that it will take any given effective opportunity to pursue O. This assumption of consistent adherence to a particular evaluation of everything is part of utility maximization, not a law of physical systems. Call machines that push toward particular goals but are not utility maximizers pseudo-agents. &lt;/p&gt;

&lt;p&gt;Can pseudo-agents exist? Yes—utility maximization is computationally intractable, so any physically existent ‘goal-directed’ entity is going to be a pseudo-agent. We are all pseudo-agents, at best. But it seems something like a spectrum. At one end is a thermostat, then maybe a thermostat with a better algorithm for adjusting the heat. Then maybe a thermostat which intelligently controls the windows. After a lot of honing, you might have a system much more like a utility-maximizer: a system that deftly seeks out and seizes well-priced opportunities to make your room 68 degrees—upgrading your house, buying R&amp;amp;D, influencing your culture, building a vast mining empire. Humans might not be very far on this spectrum, but they seem enough like utility-maximizers already to be alarming. (And it might not be well-considered as a one-dimensional spectrum—for instance, perhaps ‘tendency to modify oneself to become more coherent’ is a fairly different axis from ‘consistency of evaluations of options and outcomes’, and calling both ‘more agentic’ is obscuring.)&lt;/p&gt;

&lt;p&gt;Nonetheless, it seems plausible that there is a large space of systems which strongly increase the chance of some desirable objective O occurring without even acting as much like maximizers of an identifiable utility function as humans would. For instance, without searching out novel ways of making O occur, or modifying themselves to be more consistently O-maximizing. Call these ‘weak pseudo-agents’. &lt;/p&gt;

&lt;p&gt;For example, I can imagine a system constructed out of a huge number of ‘IF X THEN Y’ statements (reflexive responses), like ‘if body is in hallway, move North’, ‘if hands are by legs and body is in kitchen, raise hands to waist’.., equivalent to a kind of vector field of motions, such that for every particular state, there are directions that all the parts of you should be moving. I could imagine this being designed to fairly consistently cause O to happen within some context. However since such behavior would not be produced by a process optimizing O, you shouldn’t expect it to find new and strange routes to O, or to seek O reliably in novel circumstances. There appears to be zero pressure for this thing to become more coherent, unless its design already involves reflexes to move its thoughts in certain ways that lead it to change itself. I expect you could build a system like this that reliably runs around and tidies your house say, or runs your social media presence, without it containing any impetus to become a more coherent agent (because it doesn’t have any reflexes that lead to pondering self-improvement in this way).&lt;/p&gt;

&lt;p&gt;It is not clear that economic incentives generally favor the far end of this spectrum over weak pseudo-agency. There are incentives toward systems being more like utility maximizers, but also incentives against. &lt;/p&gt;

&lt;p&gt;The reason any kind of ‘goal-directedness’ is incentivised in AI systems is that then the system can be given an objective by someone hoping to use their cognitive labor, and the system will make that objective happen. Whereas a similar non-agentic AI system might still do almost the same cognitive labor, but require an agent (such as a person) to look at the objective and decide what should be done to achieve it, then ask the system for that. Goal-directedness means automating this high-level strategizing. &lt;/p&gt;

&lt;p&gt;Weak pseudo-agency fulfills this purpose to some extent, but not as well as utility maximization. However if we think that utility maximization is difficult to wield without great destruction, then that suggests a disincentive to creating systems with behavior closer to utility-maximization. Not just from the world being destroyed, but from the same dynamic causing more minor divergences from expectations, if the user can’t specify their own utility function well. &lt;/p&gt;

&lt;p&gt;That is, if it is true that utility maximization tends to lead to very bad outcomes relative to any slightly different goals (in the absence of great advances in the field of AI alignment), then the most economically favored level of goal-directedness seems unlikely to be as far as possible toward utility maximization. More likely it is a level of pseudo-agency that achieves a lot of the users’ desires without bringing about sufficiently detrimental side effects to make it not worthwhile. (This is likely more agency than is socially optimal, since some of the side-effects will be harms to others, but there seems no reason to think that it is a very high degree of agency.)&lt;/p&gt;

&lt;p&gt;Some minor but perhaps illustrative evidence: anecdotally, people prefer interacting with others who predictably carry out their roles or adhere to deontological constraints, rather than consequentialists in pursuit of broadly good but somewhat unknown goals. For instance, employers would often prefer employees who predictably follow rules than ones who try to forward company success in unforeseen ways.&lt;/p&gt;

&lt;p&gt;The other arguments to expect goal-directed systems mentioned above seem more likely to suggest approximate utility-maximization rather than some other form of goal-directedness, but it isn’t that clear to me. I don’t know what kind of entity is most naturally produced by contemporary ML training. Perhaps someone else does. I would guess that it’s more like the reflex-based agent described above, at least at present. But present systems aren’t the concern.&lt;/p&gt;

&lt;p&gt;Coherence arguments are arguments for being coherent a.k.a. maximizing a utility function, so one might think that they imply a force for utility maximization in particular. That seems broadly right. Though note that these are arguments that there is some pressure for the system to modify itself to become more coherent. What actually results from specific systems modifying themselves seems like it might have details not foreseen in an abstract argument merely suggesting that the status quo is suboptimal whenever it is not coherent. Starting from a state of arbitrary incoherence and moving iteratively in one of many pro-coherence directions produced by whatever whacky mind you currently have isn’t obviously guaranteed to increasingly approximate maximization of some sensical utility function. For instance, take an entity with a cycle of preferences, apples &amp;gt; bananas = oranges &amp;gt; pears &amp;gt; apples. The entity notices that it sometimes treats oranges as better than pears and sometimes worse. It tries to correct by adjusting the value of oranges to be the same as pears. The new utility function is exactly as incoherent as the old one. Probably moves like this are rarer than ones that make you more coherent in this situation, but I don’t know, and I also don’t know if this is a great model of the situation for incoherent systems that could become more coherent.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters: &lt;/em&gt;&lt;/strong&gt;&lt;em&gt;AI systems proliferate, and have various goals. Some AI systems try to make money in the stock market. Some make movies. Some try to direct traffic optimally. Some try to make the Democratic party win an election. Some try to make Walmart maximally profitable. These systems have no perceptible desire to optimize the universe for forwarding these goals because they aren’t maximizing a general utility function, they are more ‘behaving like someone who is trying to make Walmart profitable’. They make strategic plans and think about their comparative advantage and forecast business dynamics, but they don’t build nanotechnology to manipulate everybody’s brains, because that’s not the kind of behavior pattern they were designed to follow. The world looks kind of like the current world, in that it is fairly non-obvious what any entity’s ‘utility function’ is. It often looks like AI systems are ‘trying’ to do things, but there’s no reason to think that they are enacting a rational and consistent plan, and they rarely do anything shocking or galaxy-brained.&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;Ambiguously strong forces for goal-directedness need to meet an ambiguously high bar to cause a risk&lt;/h4&gt;

&lt;p&gt;The forces for goal-directedness mentioned in I are presumably of finite strength. For instance, if coherence arguments correspond to pressure for machines to become more like utility maximizers, there is an empirical answer to how fast that would happen with a given system. There is also an empirical answer to how ‘much’ goal directedness is needed to bring about disaster, supposing that utility maximization would bring about disaster and, say, being a rock wouldn’t. Without investigating these empirical details, it is unclear whether a particular qualitatively identified force for goal-directedness will cause disaster within a particular time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters: &lt;/em&gt;&lt;/strong&gt;&lt;em&gt;There are not that many systems doing something like utility maximization in the new AI economy. Demand is mostly for systems more like GPT or DALL-E, which transform inputs in some known way without reference to the world, rather than ‘trying’ to bring about an outcome. Maybe the world was headed for more of the latter, but ethical and safety concerns reduced desire for it, and it wasn’t that hard to do something else. Companies setting out to make non-agentic AI systems have no trouble doing so. Incoherent AIs are never observed making themselves more coherent, and training has never produced an agent unexpectedly. There are lots of vaguely agentic things, but they don’t pose much of a problem. There are a few things at least as agentic as humans, but they are a small part of the economy.&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;&lt;strong&gt;&lt;em&gt;B. Contra “goal-directed AI systems’ goals will be bad”&lt;/em&gt;&lt;/strong&gt;&lt;/h3&gt;

&lt;h4&gt;Small differences in utility functions may not be catastrophic&lt;/h4&gt;

&lt;p&gt;Arguably, humans are likely to have somewhat different values to one another even after arbitrary reflection. If so, there is some extended region of the space of possible values that the values of different humans fall within. That is, ‘human values’ is not a single point.&lt;/p&gt;

&lt;p&gt;If the values of misaligned AI systems fall within that region, this would not appear to be worse in expectation than the situation where the long-run future was determined by the values of humans other than you. (This may still be a huge loss of value relative to the alternative, if a future determined by your own values is vastly better than that chosen by a different human, and if you also expected to get some small fraction of the future, and will now get much less. These conditions seem non-obvious however, and if they obtain you should worry about more general problems than AI.)&lt;/p&gt;

&lt;p&gt;Plausibly even a single human, after reflecting, could on their own come to different places in a whole region of specific values, depending on somewhat arbitrary features of how the reflecting period went. In that case, even the values-on-reflection of a single human is an extended region of values space, and an AI which is only slightly misaligned could be the same as some version of you after reflecting.&lt;/p&gt;

&lt;p&gt;There is a further larger region, ‘that which can be reliably enough aligned with typical human values via incentives in the environment’, which is arguably larger than the circle containing most human values. Human society makes use of this a lot: for instance, most of the time particularly evil humans don’t do anything too objectionable because it isn’t in their interests. This region is probably smaller for more capable creatures such as advanced AIs, but still it is some size.&lt;/p&gt;

&lt;p&gt;Thus it seems that some amount&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; of AI divergence from your own values is probably broadly fine, i.e. not worse than what you should otherwise expect without AI.&lt;/p&gt;

&lt;p&gt;Thus in order to arrive at a conclusion of doom, it is not enough to argue that we cannot align AI perfectly. The question is a quantitative one of whether we can get it close enough. And how close is ‘close enough’ is not known. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters: &lt;/em&gt;&lt;/strong&gt;&lt;em&gt;there are many superintelligent goal-directed AI systems around. They are trained to have human-like goals, but we know that their training is imperfect and none of them has goals exactly like those presented in training. However if you just heard about a particular system’s intentions, you wouldn’t be able to guess if it was an AI or a human. Things happen much faster than they were, because superintelligent AI is superintelligent, but not obviously in a direction less broadly in line with human goals than when humans were in charge.&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;Differences between AI and human values may be small &lt;/h4&gt;

&lt;p&gt;AI trained to have human-like goals will have something close to human-like goals. How close? Call it &lt;em&gt;d&lt;/em&gt;, for a particular occasion of training AI. &lt;/p&gt;

&lt;p&gt;If &lt;em&gt;d&lt;/em&gt; doesn’t have to be 0 for safety (from above), then there is a question of whether it is an acceptable size. &lt;/p&gt;

&lt;p&gt;I know of two issues here, pushing &lt;em&gt;d&lt;/em&gt; upward. One is that with a finite number of training examples, the fit between the true function and the learned function will be wrong. &lt;a href=&quot;https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks&quot;&gt;The other&lt;/a&gt; is that you might accidentally create a monster (‘misaligned &lt;a href=&quot;https://www.alignmentforum.org/tag/mesa-optimization&quot;&gt;mesaoptimizer&lt;/a&gt;’) who understands its situation and pretends to have the utility function you are aiming for so that it can be freed and go out and manifest its own utility function, which could be just about anything. If this problem is real, then the values of an AI system might be arbitrarily different from the training values, rather than ‘nearby’ in some sense, so &lt;em&gt;d&lt;/em&gt; is probably unacceptably large. But if you avoid creating such mesaoptimizers, then it seems plausible to me that &lt;em&gt;d&lt;/em&gt; is very small. &lt;/p&gt;

&lt;p&gt;If humans also substantially learn their values via observing examples, then the variation in human values is arising from a similar process, so might be expected to be of a similar scale. If we care to make the ML training process more accurate than the human learning one, it seems likely that we could. For instance, &lt;em&gt;d&lt;/em&gt; gets smaller with more data.&lt;/p&gt;

&lt;p&gt;Another line of evidence is that for things that I have seen AI learn so far, the distance from the real thing is intuitively small. If AI learns my values as well as it learns what faces look like, it seems plausible that it carries them out better than I do.&lt;/p&gt;

&lt;p&gt;As minor additional evidence here, I don’t know how to describe any slight differences in utility functions that are catastrophic. Talking concretely, what does a utility function look like that is so close to a human utility function that an AI system has it after a bunch of training, but which is an absolute disaster? Are we talking about the scenario where the AI values a slightly different concept of justice, or values satisfaction a smidgen more relative to joy than it should? And then that’s a moral disaster because it is wrought across the cosmos? Or is it that it looks at all of our inaction and thinks we want stuff to be maintained very similar to how it is now, so crushes any efforts to improve things? &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters:&lt;/em&gt;&lt;/strong&gt;&lt;em&gt; when we try to train AI systems to care about what specific humans care about, they usually pretty much do, as far as we can tell. We basically get what we trained for. For instance, it is hard to distinguish them from the human in question. (It is still important to actually do this training, rather than making AI systems not trained to have human values.)&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;Maybe value isn’t fragile&lt;/h4&gt;

&lt;p&gt;Eliezer argued that &lt;a href=&quot;https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile&quot;&gt;value is fragile&lt;/a&gt;, via examples of ‘just one thing’ that you can leave out of a utility function, and end up with something very far away from what humans want. For instance, if you leave out ‘boredom’ then he thinks the preferred future might look like repeating the same otherwise perfect moment again and again. (His argument is perhaps longer—that post says there is a lot of important background, though the bits mentioned don’t sound relevant to my disagreement.) This sounds to me like ‘value is not resilient to having components of it moved to zero’, which is a weird usage of ‘fragile’, and in particular, doesn’t seem to imply much about smaller perturbations. And smaller perturbations seem like the relevant thing with AI systems trained on a bunch of data to mimic something. &lt;/p&gt;

&lt;p&gt;You could very analogously say ‘human faces are fragile’ because if you just leave out the nose it suddenly doesn’t look like a typical human face at all. Sure, but is that the kind of error you get when you try to train ML systems to mimic human faces? Almost none of the faces on &lt;a href=&quot;http://thispersondoesnotexist.com&quot;&gt;thispersondoesnotexist.com&lt;/a&gt; are blatantly morphologically unusual in any way, let alone noseless. Admittedly one time I saw someone whose face was neon green goo, but I’m guessing you can get the rate of that down pretty low if you care about it.&lt;/p&gt;

&lt;p&gt;Eight examples, no cherry-picking:&lt;/p&gt;

&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;
  &lt;p&gt;&lt;img src=&quot;https://lh6.googleusercontent.com/0vj-NpCiLoEKF3mXiDQHUE4mVQro_b1b6R3TxIh85ulPXlZ9jhgAPRb5IvU8tqDjM8dCdZe5vVERjjX0Y1-cAky03TErEwdlibOHJ7DbO0-zFjk3r0iM8GM8sNQHaIRDxDioNdLHDvisRtC9SrhHV23CuzUptkTbQwAoDldouCpqmq0XFGuAxovePQ&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://lh3.googleusercontent.com/Tuqy_SobcKiClmzWwtzL6jju5KJLaPa_8ZJKHsfgiNWumTPSBJNUYqvtAnck4u4DaQXQF2Vi9sZQXaDZ6aTHdVhrQWL-0gxygEYtA8PAOym1pNTknN0rGPKzcSYRx7heu2rg11lHJC-4o6-3o018H2Lu7FKK9rQgFuB9ar3g2ZbXQqp16InN-faonA&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://lh4.googleusercontent.com/0SSkt7jhDtDatv-YUQGq-01B_jsmEGAaS2iGg9tf12Ar6IYT0yYRbLm27p88DdpcI1DRMvGZzoV8IfLLoABdMok0ZMI1213ixZWkv-jws8tSoWBNzjK4ntGlIkDHRAcSnAe6-0Xhn1hCOK0sA9nPqbMZ6wP0YBVak6qgllOAbrhJ47kiO0b0u2amKw&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://lh4.googleusercontent.com/fZvZIY6hu_PhsIQEf5J24IlLk6LaUHVYblUMWdc39RxTY37QGKEh8mNiYWv03oMDNDq7S8eHqnPgOVoW5BGHXw_TP9OiKoA1riOl7NUWwpsinJjIRpfHAkzTxQHt-KY0SLY2O_Kta1qGEt07XeIWCldhYD6ZvoBrIOO-qyDmMom7HjWzdqTLT6BPPg&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://lh3.googleusercontent.com/uor40snO97cdBPci-mXbA3dJ8NPe0VE7CiIERY5i26B0I8tgtqOCuYt0VRt5OyP8H1PYOIfaAEqHKwAaRlQdETXj5-6YiO30EKJgPlsxxbduTGQMRSz0bGSfiBZyZwHBG3jhXOzaHqg7N2jTI0YiTRQG_AVtgGDdkJpH9of6ykEVJKHicuz-Z9Bwhg&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://lh6.googleusercontent.com/WbZs_lAmdUCwg6ZKO7mi5U3VPOXhezNlioBe7Gw5UYzY32ZTryJnkaCY35N05T72z_uyU_f1hv0OWxh8UrF2oTVPMRQbi647UzYXir0-4R1_QX0qcN7vPxByvcIgifB_1peLnwoyvLQCmdABo2v8_66-eckFxMbswhasGcUiDRI5G7wV3337CEp03A&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://lh4.googleusercontent.com/Iu6OOr13ntHCmLC0EXTc4Sh_nCwgSSb67Ffq75SwMR3l-r606IqXBbUQsQj1rT7UrhLgL10I7p8QzXGuJx5nv3pkp0yLURgI1yCaHmHrD-RHxjkZUxQOrQzfgennYXR54kreIN_wn15GtxmcEPjLtFx-VHqZl59nCA1CXhHJPQrtjpGg1aIfvcSdVw&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://lh5.googleusercontent.com/mb3BSXp0EmzWZ7Xeiu_0NzeKeWR66fvYHhaG_zDo7QPPGKwZR9WALKHpMTSXx5JHUl8ygFgM4FSTyXYKcqYD60uyCWGNJa77hPzVON0gr8yPA8P0vzSlSIJv42G3Ydd_qwWGHZP9_GUHm45JBEqeaOM2dQyq4RXDWHboHmyVRvWDbTputoxjS5cZcg&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;/figure&gt;&lt;/figure&gt;&lt;/figure&gt;&lt;/figure&gt;&lt;/figure&gt;&lt;/figure&gt;&lt;/figure&gt;&lt;/p&gt;

  &lt;p&gt;Skipping the nose is the kind of mistake you make if you are a child drawing a face from memory. Skipping ‘boredom’ is the kind of mistake you make if you are a person trying to write down human values from memory. My guess is that this seemed closer to the plan in 2009 when that post was written, and that people cached the takeaway and haven’t updated it for deep learning which can learn what faces look like better than you can.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters:&lt;/em&gt;&lt;/strong&gt;&lt;em&gt; there is a large region ‘around’ my values in value space that is also pretty good according to me. AI easily lands within that space, and eventually creates some world that is about as good as the best possible utopia, according to me. There aren’t a lot of really crazy and terrible value systems adjacent to my values.&lt;/em&gt;&lt;/p&gt;

  &lt;h4&gt;Short-term goals&lt;/h4&gt;

  &lt;p&gt;Utility maximization really only incentivises drastically altering the universe if one’s utility function places a high enough value on very temporally distant outcomes relative to near ones. That is, long term goals are needed for danger. A person who cares most about winning the timed chess game in front of them should not spend time accruing resources to invest in better chess-playing.&lt;/p&gt;

  &lt;p&gt;AI systems could have long-term goals via people intentionally training them to do so, or via long-term goals naturally arising from systems not trained so. &lt;/p&gt;

  &lt;p&gt;Humans seem to discount the future a lot in their usual decision-making (they have goals years in advance but rarely a hundred years) so the economic incentive to train AI to have very long term goals might be limited.&lt;/p&gt;

  &lt;p&gt;It’s not clear that training for relatively short term goals naturally produces creatures with very long term goals, though it might.&lt;/p&gt;

  &lt;p&gt;Thus if AI systems fail to have value systems relatively similar to human values, it is not clear that many will have the long time horizons needed to motivate taking over the universe.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters:&lt;/em&gt;&lt;/strong&gt;&lt;em&gt; the world is full of agents who care about relatively near-term issues, and are helpful to that end, and have no incentive to make long-term large scale schemes. Reminiscent of the current world, but with cleverer short-termism.&lt;/em&gt;&lt;/p&gt;

  &lt;h3&gt;&lt;strong&gt;&lt;em&gt;C. Contra “superhuman AI would be sufficiently superior to humans to overpower humanity”&lt;/em&gt;&lt;/strong&gt;&lt;/h3&gt;

  &lt;h4&gt;Human success isn’t from individual intelligence&lt;/h4&gt;

  &lt;p&gt;The argument claims (or assumes) that surpassing ‘human-level’ intelligence (i.e. the mental capacities of an individual human) is the relevant bar for matching the power-gaining capacity of humans, such that passing this bar in individual intellect means outcompeting humans in general in terms of power (argument III.2), if not being able to immediately destroy them all outright (argument III.1.). In a similar vein, introductions to AI risk often start by saying that humanity has triumphed over the other species because it is more intelligent, as a lead in to saying that if we make something more intelligent still, it will inexorably triumph over humanity.&lt;/p&gt;

  &lt;p&gt;This hypothesis about the provenance of human triumph seems wrong. Intellect surely helps, but humans look to be powerful largely because they share their meager intellectual discoveries with one another and consequently save them up over time&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;. You can see this starkly by comparing the material situation of Alice, a genius living in the stone age, and Bob, an average person living in 21st Century America. Alice might struggle all day to get a pot of water, while Bob might be able to summon all manner of delicious drinks from across the oceans, along with furniture, electronics, information, etc. Much of Bob’s power probably did flow from the application of intelligence, but not Bob’s individual intelligence. Alice’s intelligence, and that of those who came between them.&lt;/p&gt;

  &lt;p&gt;Bob’s greater power isn’t directly just from the knowledge and artifacts Bob inherits from other humans. He also seems to be helped for instance by much better coordination: both from a larger number people coordinating together, and from better infrastructure for that coordination (e.g. for Alice the height of coordination might be an occasional big multi-tribe meeting with trade, and for Bob it includes global instant messaging and banking systems and the Internet). One might attribute all of this ultimately to innovation, and thus to intelligence and communication, or not. I think it’s not important to sort out here, as long as it’s clear that individual intelligence isn’t the source of power.&lt;/p&gt;

  &lt;p&gt;It could still be that with a given bounty of shared knowledge (e.g. within a given society), intelligence grants huge advantages. But even that doesn’t look true here: 21st Century geniuses live basically like 21st Century people of average intelligence, give or take&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

  &lt;p&gt;Why does this matter? Well for one thing, if you make AI which is merely as smart as a human, you shouldn’t then expect it to do that much better than a genius living in the stone age. That’s what human-level intelligence gets you: nearly nothing. &lt;a href=&quot;https://meteuphoric.com/2017/12/28/why-did-everything-take-so-long/&quot;&gt;A piece of rope&lt;/a&gt; after millions of lifetimes. Humans without their culture are much like other animals. &lt;/p&gt;

  &lt;p&gt;To wield the control-over-the-world of a genius living in the 21st Century, the human-level AI would seem to need something like the other benefits that the 21st century genius gets from their situation in connection with a society. &lt;/p&gt;

  &lt;p&gt;One such thing is access to humanity’s shared stock of hard-won information. AI systems plausibly do have this, if they can get most of what is relevant by reading the internet. This isn’t obvious: people also inherit information from society through copying habits and customs, learning directly from other people, and receiving artifacts with implicit information (for instance, a factory allows whoever owns the factory to make use of intellectual work that was done by the people who built the factory, but that information may not available explicitly even for the owner of the factory, let alone to readers on the internet). These sources of information seem likely to also be available to AI systems though, at least if they are afforded the same options as humans.&lt;/p&gt;

  &lt;p&gt;My best guess is that AI systems easily do better than humans on extracting information from humanity’s stockpile, and on coordinating, and so on this account are probably in an even better position to compete with humans than one might think on the individual intelligence model, but that is a guess. In that case perhaps this misunderstanding makes little difference to the outcomes of the argument. However it seems at least a bit more complicated. &lt;/p&gt;

  &lt;p&gt;Suppose that AI systems can have access to all information humans can have access to. The power the 21st century person gains from their society is modulated by their role in society, and relationships, and rights, and the affordances society allows them as a result. Their power will vary enormously depending on whether they are employed, or listened to, or paid, or a citizen, or the president. If AI systems’ power stems substantially from interacting with society, then their power will also depend on affordances granted, and humans may choose not to grant them many affordances (see section ‘Intelligence may not be an overwhelming advantage’ for more discussion).&lt;/p&gt;

  &lt;p&gt;However suppose that your new genius AI system is also treated with all privilege. The next way that this alternate model matters is that if most of what is good in a person’s life is determined by the society they are part of, and their own labor is just buying them a tiny piece of that inheritance, then if they are for instance twice as smart as any other human, they don’t get to use technology that it twice as good. They just get a larger piece of that same shared technological bounty purchasable by anyone. Because each individual person is adding essentially nothing in terms of technology, so twice that is still basically nothing. &lt;/p&gt;

  &lt;p&gt;In contrast, I think people are often imagining that a single entity somewhat smarter than a human will be able to quickly use technologies that are somewhat better than current human technologies. This seems to be mistaking the actions of a human and the actions of a human society. If &lt;a href=&quot;https://en.wikipedia.org/wiki/Manhattan_Project#Personnel&quot;&gt;a hundred thousand people sometimes get together for a few years and make fantastic new weapons&lt;/a&gt;, you should not expect an entity somewhat smarter than a person to make even better weapons. That’s off by a factor of about a hundred thousand. &lt;/p&gt;

  &lt;p&gt;There might be places you can get far ahead of humanity by being better than a single human—it depends how much accomplishments depend on the few most capable humans in the field, and how few people are working on the problem&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;. But for instance the Manhattan Project &lt;a href=&quot;https://en.wikipedia.org/wiki/Manhattan_Project#Personnel&quot;&gt;took&lt;/a&gt; a hundred thousand people several years, and von Neumann (a mythically smart scientist) joining the project did not reduce it to an afternoon. Plausibly to me, some specific people being on the project caused it to not take twice as many person-years, though the plausible candidates here seem to be more in the business of running things than doing science directly (though that also presumably involves intelligence). But even if you are an ambitious somewhat superhuman intelligence, the influence available to you seems to plausibly be limited to making a large dent in the effort required for some particular research endeavor, not single-handedly outmoding humans across many research endeavors.&lt;/p&gt;

  &lt;p&gt;This is all reason to doubt that a small number of superhuman intelligences will rapidly take over or destroy the world (as in III.i.). This doesn’t preclude a set of AI systems that are together more capable than a large number of people from making great progress. However some related issues seem to make that less likely.&lt;/p&gt;

  &lt;p&gt;Another implication of this model is that if most human power comes from buying access to society’s shared power, i.e. interacting with the economy, you should expect intellectual labor by AI systems to usually be sold, rather than for instance put toward a private stock of knowledge. This means the intellectual outputs are mostly going to society, and the main source of potential power to an AI system is the wages received (which may allow it to gain power in the long run). However it seems quite plausible that AI systems at this stage will generally not receive wages, since they presumably do not need them to be motivated to do the work they were trained for. It also seems plausible that they would be owned and run by humans. This would seem to not involve any transfer of power to that AI system, except insofar as its intellectual outputs benefit it (e.g. if it is writing advertising material, maybe it doesn’t get paid for that, but if it can write material that slightly furthers its own goals in the world while also fulfilling the advertising requirements, then it sneaked in some influence.) &lt;/p&gt;

  &lt;p&gt;If there is AI which is moderately more competent than humans, but not sufficiently more competent to take over the world, then it is likely to contribute to this stock of knowledge and affordances shared with humans. There is no reason to expect it to build a separate competing stock, any more than there is reason for a current human household to try to build a separate competing stock rather than sell their labor to others in the economy. &lt;/p&gt;

  &lt;p&gt;In summary:&lt;/p&gt;

  &lt;ol&gt;&lt;li&gt;Functional connection with a large community of other intelligences in the past and present is probably a much bigger factor in the success of humans as a species or individual humans than is individual intelligence.&amp;nbsp;&lt;/li&gt;&lt;li&gt;Thus this also seems more likely to be important for AI success than individual intelligence. This is contrary to a usual argument for AI superiority, but probably leaves AI systems at least as likely to outperform humans, since superhuman AI is probably superhumanly good at taking in information and coordinating.&lt;/li&gt;&lt;li&gt;However it is not obvious that AI systems will have the same access to society’s accumulated information e.g. if there is information which humans learn from living in society, rather than from reading the internet.&amp;nbsp;&lt;/li&gt;&lt;li&gt;And it seems an open question whether AI systems are given the same affordances in society as humans, which also seem important to making use of the accrued bounty of power over the world that humans have. For instance, if they are not granted the same legal rights as humans, they may be at a disadvantage in doing trade or engaging in politics or accruing power.&lt;/li&gt;&lt;li&gt;The fruits of greater intelligence for an entity will probably not look like society-level accomplishments unless it is a society-scale entity&lt;/li&gt;&lt;li&gt;The route to influence with smaller fruits probably by default looks like participating in the economy rather than trying to build a private stock of knowledge.&lt;/li&gt;&lt;li&gt;If the resources from participating in the economy accrue to the owners of AI systems, not to the systems themselves, then there is less reason to expect the systems to accrue power incrementally, and they are at a severe disadvantage relative to humans.&amp;nbsp;&lt;/li&gt;&lt;/ol&gt;

  &lt;p&gt;Overall these are reasons to expect AI systems with around human-level cognitive performance to not destroy the world immediately, and to not amass power as easily as one might imagine. &lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters:&lt;/em&gt;&lt;/strong&gt;&lt;em&gt; If AI systems are somewhat superhuman, then they do impressive cognitive work, and each contributes to technology more than the best human geniuses, but not more than the whole of society, and not enough to materially improve their own affordances. They don’t gain power rapidly because they are disadvantaged in other ways, e.g. by lack of information, lack of rights, lack of access to positions of power. Their work is sold and used by many actors, and the proceeds go to their human owners. AI systems do not generally end up with access to masses of technology that others do not have access to, and nor do they have private fortunes. In the long run, as they become more powerful, they might take power if other aspects of the situation don’t change. &lt;/em&gt;&lt;/p&gt;

  &lt;h4&gt;AI agents may not be radically superior to combinations of humans and non-agentic machines&lt;/h4&gt;

  &lt;p&gt;‘Human level capability’ is a moving target. For comparing the competence of advanced AI systems to humans, the relevant comparison is with humans who have state-of-the-art AI and other tools. For instance, the human capacity to make art quickly has recently been improved by a variety of AI art systems. If there were now an agentic AI system that made art, it would make art much faster than a human of 2015, but perhaps hardly faster than a human of late 2022. If humans continually have access to tool versions of AI capabilities, it is not clear that agentic AI systems must ever have an overwhelmingly large capability advantage for important tasks (though they might). &lt;/p&gt;

  &lt;p&gt;(This is not an argument that humans might be better than AI systems, but rather: if the gap in capability is smaller, then the pressure for AI systems to accrue power is less and thus loss of human control is slower and easier to mitigate entirely through other forces, such as subsidizing human involvement or disadvantaging AI systems in the economy.)&lt;/p&gt;

  &lt;p&gt;Some advantages of being an agentic AI system vs. a human with a tool AI system seem to be:&lt;/p&gt;

  &lt;ol&gt;&lt;li&gt;There might just not be an equivalent tool system, for instance if it is impossible to train systems without producing emergent agents.&lt;/li&gt;&lt;li&gt;When every part of a process takes into account the final goal, this should make the choices within the task more apt for the final goal (and agents know their final goal, whereas tools carrying out parts of a larger problem do not).&lt;/li&gt;&lt;li&gt;For humans, the interface for using a capability of one’s mind tends to be smoother than the interface for using a tool. For instance a person who can do fast mental multiplication can do this more smoothly and use it more often than a person who needs to get out a calculator. This seems likely to persist.&lt;/li&gt;&lt;/ol&gt;

  &lt;p&gt;1 and 2 may or may not matter much. 3 matters more for brief, fast, unimportant tasks. For instance, consider again people who can do mental calculations better than others. My guess is that this advantages them at using Fermi estimates in their lives and buying cheaper groceries, but does not make them materially better at making large financial choices well. For a one-off large financial choice, the effort of getting out a calculator is worth it and the delay is very short compared to the length of the activity. The same seems likely true of humans with tools vs. agentic AI with the same capacities integrated into their minds. Conceivably the gap between humans with tools and goal-directed AI is small for large, important tasks.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters:&lt;/em&gt;&lt;/strong&gt;&lt;em&gt; agentic AI systems have substantial advantages over humans with tools at some tasks like rapid interaction with humans, and responding to rapidly evolving strategic situations.  One-off large important tasks such as advanced science are mostly done by tool AI. &lt;/em&gt;&lt;/p&gt;

  &lt;h4&gt;Trust&lt;/h4&gt;

  &lt;p&gt;If goal-directed AI systems are only mildly more competent than some combination of tool systems and humans (as suggested by considerations in the last two sections), we still might expect AI systems to out-compete humans, just more slowly. However AI systems have one serious disadvantage as employees of humans: they are intrinsically untrustworthy, while we don’t understand them well enough to be clear on what their values are or how they will behave in any given case. Even if they did perform as well as humans at some task, if humans can’t be certain of that, then there is reason to disprefer using them. This can be thought of as two problems: firstly, slightly misaligned systems are less valuable because they genuinely do the thing you want less well, and secondly, even if they were not misaligned, if humans can’t know that (because we have no good way to verify the alignment of AI systems) then it is costly in expectation to use them. (This is only a further force acting against the supremacy of AI systems—they might still be powerful enough that using them is enough of an advantage that it is worth taking the hit on trustworthiness.)&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters: &lt;/em&gt;&lt;/strong&gt;&lt;em&gt;in places where goal-directed AI systems are not typically hugely better than some combination of less goal-directed systems and humans, the job is often given to the latter if trustworthiness matters. &lt;/em&gt;&lt;/p&gt;

  &lt;h4&gt;Headroom&lt;/h4&gt;

  &lt;p&gt;For AI to vastly surpass human performance at a task, there needs to be ample room for improvement above human level. For some tasks, there is not—tic-tac-toe is a classic example. It is not clear how close humans (or technologically aided humans) are from the limits to competence in the particular domains that will matter. It is to my knowledge an open question how much ‘headroom’ there is. My guess is a lot, but it isn’t obvious.&lt;/p&gt;

  &lt;p&gt;How much headroom there is varies by task. Categories of task for which there appears to be little headroom: &lt;/p&gt;

  &lt;ol&gt;&lt;li&gt;Tasks where we know what the best performance looks like, and humans can get close to it. For instance, machines cannot win more often than the best humans at Tic-tac-toe (playing within the rules) or solve Rubik’s cubes much more reliably, or extracting calories from fuel&lt;/li&gt;&lt;li&gt;Tasks where humans are already be reaping most of the value—for instance, perhaps most of the value of forks is in having a handle with prongs attached to the end, and while humans continue to design slightly better ones, and machines might be able to add marginal value to that project more than twice as fast as the human designers, they cannot perform twice as well in terms of the value of each fork, because forks are already 95% as good as they can be.&amp;nbsp;&lt;/li&gt;&lt;li&gt;Better performance is quickly intractable. For instance, we know that for tasks in particular complexity classes, there are computational limits to how well one can perform across the board. Or for chaotic systems, there can be limits to predictability. (That is, tasks might lack headroom not because they are simple, but because they are complex. E.g. AI probably can’t predict the weather much further out than humans.)&lt;/li&gt;&lt;/ol&gt;

  &lt;p&gt;Categories of task where a lot of headroom seems likely:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;Competitive tasks where the value of a certain level of performance depends on whether one is better or worse than one’s opponent, so that the marginal value of more performance doesn’t hit diminishing returns, as long as your opponent keeps competing and taking back what you just won. Though in one way this is like having little headroom: there’s no more value to be had—the game is zero sum. And while there might often be a lot of value to be gained by doing a bit better on the margin, still if all sides can invest, then nobody will end up better off than they were. So whether this seems more like high or low headroom depends on what we are asking exactly. Here we are asking if AI systems can do much better than humans: in a zero sum contest like this, they likely can in the sense that they can beat humans, but not in the sense of reaping anything more from the situation than the humans ever got.&lt;/li&gt;
    &lt;li&gt;Tasks where it is twice as good to do the same task twice as fast, and where speed is bottlenecked on thinking time.&lt;/li&gt;
    &lt;li&gt;Tasks where there is reason to think that optimal performance is radically better than we have seen. For instance, perhaps we can estimate how high Chess Elo rankings must go before reaching perfection by reasoning theoretically about the game, and perhaps it is very high (I don’t know).&lt;/li&gt;
    &lt;li&gt;Tasks where humans appear to use very inefficient methods. For instance, it was perhaps predictable before calculators that they would be able to do mathematics much faster than humans, because humans can only keep a small number of digits in their heads, which doesn’t seem like an intrinsically hard problem. Similarly, I hear humans often use mental machinery designed for one mental activity for fairly different ones, through analogy.&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; For instance, when I think about macroeconomics, I seem to be basically using my intuitions for dealing with water. When I do mathematics in general, I think I’m probably using my mental capacities for imagining physical objects.&lt;/li&gt;
  &lt;/ol&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters: &lt;/em&gt;&lt;/strong&gt;&lt;em&gt;many challenges in today’s world remain challenging for AI. Human behavior is not readily predictable or manipulable very far beyond what we have explored, only slightly more complicated schemes are feasible before the world’s uncertainties overwhelm planning; much better ads are soon met by much better immune responses; much better commercial decision-making ekes out some additional value across the board but most products were already fulfilling a lot of their potential; incredible virtual prosecutors meet incredible virtual defense attorneys and everything is as it was; there are a few rounds of attack-and-defense in various corporate strategies before a new equilibrium with broad recognition of those possibilities; conflicts and ‘social issues’ remain mostly intractable. There is a brief golden age of science before the newly low-hanging fruit are again plucked and it is only lightning fast in areas where thinking was the main bottleneck, e.g. not in medicine.&lt;/em&gt;&lt;/p&gt;

  &lt;h4&gt;Intelligence may not be an overwhelming advantage&lt;/h4&gt;

  &lt;p&gt;Intelligence is helpful for accruing power and resources, all things equal, but many other things are helpful too. For instance money, social standing, allies, evident trustworthiness, not being discriminated against (this was slightly discussed in section ‘Human success isn’t from individual intelligence’). AI systems are not guaranteed to have those in abundance. The argument assumes that any difference in intelligence in particular will eventually win out over any differences in other initial resources. I don’t know of reason to think that. &lt;/p&gt;

  &lt;p&gt;Empirical evidence does not seem to support the idea that cognitive ability is a large factor in success. Situations where one entity is much smarter or more broadly mentally competent than other entities regularly occur without the smarter one taking control over the other:&lt;/p&gt;

  &lt;ol&gt;&lt;li&gt;Species exist with all levels of intelligence. Elephants have not in any sense won over gnats; they do not rule gnats; they do not have obviously more control than gnats over the environment. &lt;/li&gt;&lt;li&gt;Competence does not seem to aggressively overwhelm other advantages in humans: &lt;ol&gt;&lt;li&gt;Looking at the world, intuitively the big discrepancies in power are not seemingly about intelligence.&lt;/li&gt;&lt;li&gt;IQ 130 humans &lt;a href=&quot;https://www.newscientist.com/article/dn11711-smarter-people-are-no-better-off/&quot;&gt;apparently&lt;/a&gt; earn very roughly $6000-$18,500 per year more than average IQ humans.&lt;/li&gt;&lt;li&gt;Elected representatives are apparently smarter on average, but it is a slightly shifted curve, &lt;a href=&quot;http://perseus.iies.su.se/~tpers/papers/Draft170103.pdf&quot;&gt;not a radically difference&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;MENSA isn’t a major force in the world.&lt;/li&gt;&lt;li&gt;Many places where people see huge success through being cognitively able are ones where they show off their intelligence to impress people, rather than actually using it for decision-making. For instance, writers, actors, song-writers, comedians, all sometimes become very successful through cognitive skills. Whereas scientists, engineers and authors of software use cognitive skills to make choices about the world, and less often become extremely rich and famous, say. If intelligence were that useful for strategic action, it seems like using it for that would be at least as powerful as showing it off. But maybe this is just an accident of which fields have winner-takes-all type dynamics.&lt;/li&gt;&lt;li&gt;If we look at people who evidently have good cognitive abilities given their intellectual output, their personal lives are not obviously drastically more successful, anecdotally.&lt;/li&gt;&lt;li&gt;One might counter-counter-argue that humans are very similar to one another in capability, so even if intelligence matters much more than other traits, you won’t see that by looking at  the near-identical humans. This does not seem to be true. Often at least, the difference in performance between mediocre human performance and top level human performance is &lt;a href=&quot;https://aiimpacts.org/category/speed-of-ai-transition/range-of-human-performance/&quot;&gt;large&lt;/a&gt;, relative to the space below, iirc. For instance, in chess, the Elo difference between the best and worst players is about 2000, whereas the difference between the amateur play and random play is &lt;a href=&quot;https://chess.stackexchange.com/questions/6508/what-would-be-the-elo-of-a-computer-program-that-plays-at-random&quot;&gt;maybe 400-2800 (if you accept Chess StackExchange guesses as a reasonable proxy for the truth here)&lt;/a&gt;. And &lt;a href=&quot;https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-chess/&quot;&gt;in terms of AI progress&lt;/a&gt;, amateur human play was reached in the 50s, roughly when research began, and world champion level play was reached in 1997. &lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;

  &lt;p&gt;And theoretically I don’t know why one would expect greater intelligence to win out over other advantages over time.  There are actually two questionable theories here: 1) Charlotte having more overall control than David at time 0 means that Charlotte will tend to have an even greater share of control at time 1. And, 2) Charlotte having more &lt;em&gt;intelligence&lt;/em&gt; than David at time 0 means that Charlotte will have a greater share of control at time 1 even if Bob has more overall control (i.e. more of other resources) at time 1.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters:&lt;/em&gt;&lt;/strong&gt;&lt;em&gt; there are many AI systems around, and they strive for various things. They don’t hold property, or vote, or get a weight in almost anyone’s decisions, or get paid, and are generally treated with suspicion. These things on net keep them from gaining very much power. They are very persuasive speakers however and we can’t stop them from communicating, so there is a constant risk of people willingly handing them power, in response to their moving claims that they are an oppressed minority who suffer. The main thing stopping them from winning is that their position as psychopaths bent on taking power for incredibly pointless ends is widely understood.&lt;/em&gt;&lt;/p&gt;

  &lt;h4&gt;Unclear that many goals realistically incentivise taking over the universe&lt;/h4&gt;

  &lt;p&gt;I have some goals. For instance, I want some good romance. My guess is that trying to take over the universe isn’t the best way to achieve this goal. The same goes for a lot of my goals, it seems to me. Possibly I’m in error, but I spend a lot of time pursuing goals, and very little of it trying to take over the universe. Whether a particular goal is best forwarded by trying to take over the universe as a substep seems like a quantitative empirical question, to which the answer is virtually always ‘not remotely’. Don’t get me wrong: all of these goals involve some interest in taking over the universe. All things equal, if I could take over the universe for free, I do think it would help in my romantic pursuits. But taking over the universe is not free. It’s actually super duper duper expensive and hard. So for most goals arising, it doesn’t bear considering. The idea of taking over the universe as a substep is entirely laughable for almost any human goal.&lt;/p&gt;

  &lt;p&gt;So why do we think that AI goals are different? I think the thought is that it’s radically easier for AI systems to take over the world, because all they have to do is to annihilate humanity, and they are way better positioned to do that than I am, and also better positioned to survive the death of human civilization than I am. I agree that it is likely easier, but how much easier? So much easier to take it from ‘laughably unhelpful’ to ‘obviously always the best move’? This is another quantitative empirical question.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters: &lt;/em&gt;&lt;/strong&gt;&lt;em&gt;Superintelligent AI systems pursue their goals. Often they achieve them fairly well. This is somewhat contrary to ideal human thriving, but not lethal. For instance, some AI systems are trying to maximize Amazon’s market share, within broad legality. Everyone buys truly incredible amounts of stuff from Amazon, and people often wonder if it is too much stuff. At no point does attempting to murder all humans seem like the best strategy for this. &lt;/em&gt;&lt;/p&gt;

  &lt;h4&gt;Quantity of new cognitive labor is an empirical question, not addressed&lt;/h4&gt;

  &lt;p&gt;Whether some set of AI systems can take over the world with their new intelligence probably depends how much total cognitive labor they represent. For instance, if they are in total slightly more capable than von Neumann, they probably can’t take over the world. If they are together as capable (in some sense) as a million 21st Century human civilizations, then they probably can (at least in the 21st Century).&lt;/p&gt;

  &lt;p&gt;It also matters how much of that is goal-directed at all, and highly intelligent, and how much of that is directed at achieving the AI systems’ own goals rather than those we intended them for, and how much of that is directed at taking over the world. &lt;/p&gt;

  &lt;p&gt;If we continued to build hardware, presumably at some point AI systems would account for most of the cognitive labor in the world. But if there is first an extended period of more minimal advanced AI presence, that would probably prevent an immediate death outcome, and improve humanity’s prospects for controlling a slow-moving AI power grab. &lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters: &lt;/em&gt;&lt;/strong&gt;&lt;em&gt;when advanced AI is developed, there is a lot of new cognitive labor in the world, but it is a minuscule fraction of all of the cognitive labor in the world. A large part of it is not goal-directed at all, and of that, most of the new AI thought is applied to tasks it was intended for. Thus what part of it is spent on scheming to grab power for AI systems is too small to grab much power quickly. The amount of AI cognitive labor grows fast over time, and in several decades it is most of the cognitive labor, but humanity has had extensive experience dealing with its power grabbing.&lt;/em&gt;&lt;/p&gt;

  &lt;h4&gt;Speed of intelligence growth is ambiguous&lt;/h4&gt;

  &lt;p&gt;The idea that a superhuman AI would be able to rapidly destroy the world seems &lt;em&gt;prima facie&lt;/em&gt; unlikely, since no other entity has ever done that. Two common broad arguments for it:&lt;/p&gt;

  &lt;ol&gt;&lt;li&gt;There will be a feedback loop in which intelligent AI makes more intelligent AI repeatedly until AI is very intelligent.&lt;/li&gt;&lt;li&gt;Very small differences in brains seem to correspond to very large differences in performance, based on observing humans and other apes. Thus any movement past human-level will take us to unimaginably superhuman level.&lt;/li&gt;&lt;/ol&gt;

  &lt;p&gt;These both seem questionable.&lt;/p&gt;

  &lt;ol&gt;&lt;li&gt;Feedback loops can happen at very different rates. Identifying a feedback loop empirically does not signify an explosion of whatever you are looking at. For instance, technology is already helping improve technology. To get to a confident conclusion of doom, you need evidence that the feedback loop is fast.&lt;/li&gt;&lt;li&gt;It does not seem clear that small improvements in brains lead to large changes in intelligence in general, or will do on the relevant margin. Small differences between humans and other primates might include those helpful for communication (see Section &amp;#8216;Human success isn&amp;#8217;t from individual intelligence&amp;#8217;), which do not seem relevant here. If there were a particularly powerful cognitive development between chimps and humans, it is unclear that AI researchers find that same insight at the same point in the process (rather than at some other time).&amp;nbsp;&lt;/li&gt;&lt;/ol&gt;

  &lt;p&gt;A large number of other arguments have been posed for expecting very fast growth in intelligence at around human level. I previously made &lt;a href=&quot;https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/&quot;&gt;a list of them with counterarguments&lt;/a&gt;, though none seemed very compelling. Overall, I don’t know of strong reason to expect very fast growth in AI capabilities at around human-level AI performance, though I hear such arguments might exist. &lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;What it would look like if this gap mattered: &lt;/em&gt;&lt;/strong&gt;&lt;em&gt;AI systems would at some point perform at around human level at various tasks, and would contribute to AI research, along with everything else. This would contribute to progress to an extent familiar from other technological progress feedback, and would not e.g. lead to a superintelligent AI system in minutes.&lt;/em&gt;&lt;/p&gt;

  &lt;h4&gt;Key concepts are vague&lt;/h4&gt;

  &lt;p&gt;Concepts such as ‘control’, ‘power’, and ‘alignment with human values’ all seem vague. ‘Control’ is not zero sum (as seemingly assumed) and is somewhat hard to pin down, I claim. What an ‘aligned’ entity is exactly seems to be contentious in the AI safety community, but I don’t know the details. My guess is that upon further probing, these conceptual issues are resolvable in a way that doesn’t endanger the argument, but I don’t know. I’m not going to go into this here.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this gap matters: &lt;/em&gt;&lt;/strong&gt;&lt;em&gt;upon thinking more, we realize that our concerns were confused. Things go fine with AI in ways that seem obvious in retrospect. This might look like it did for people concerned about the ‘population bomb’ or as it did for me in some of my youthful concerns about sustainability: there was a compelling abstract argument for a problem, and the reality didn’t fit the abstractions well enough to play out as predicted.&lt;/em&gt;&lt;/p&gt;

  &lt;h3&gt;&lt;strong&gt;D. Contra the whole argument&lt;/strong&gt;&lt;/h3&gt;

  &lt;h4&gt;The argument overall proves too much about corporations&lt;/h4&gt;

  &lt;p&gt;Here is the argument again, but modified to be about corporations. A couple of pieces don’t carry over, but they don’t seem integral.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;I. Any given corporation is likely to be ‘goal-directed’&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Reasons to expect this:&lt;/p&gt;

  &lt;ol start=&quot;4&quot;&gt;&lt;li&gt;Goal-directed behavior is likely to be valuable in corporations, e.g. economically&lt;/li&gt;&lt;li&gt;&lt;s&gt;Goal-directed entities may tend to arise from machine learning training processes not intending to create them (at least via the methods that are likely to be used).&lt;/s&gt;&lt;/li&gt;&lt;li&gt;‘Coherence arguments’ may imply that systems with some goal-directedness will become more strongly goal-directed over time.&lt;/li&gt;&lt;/ol&gt;

  &lt;p&gt;&lt;strong&gt;II. If goal-directed superhuman corporations are built, their desired outcomes will probably be about as bad as an empty universe by human lights&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Reasons to expect this:&lt;/p&gt;

  &lt;ol start=&quot;4&quot;&gt;&lt;li&gt;Finding useful goals that aren’t extinction-level bad appears to be hard: we don’t have a way to usefully point at human goals, and divergences from human goals seem likely to produce goals that are in intense conflict with human goals, due to a) most goals producing convergent incentives for controlling everything, and b) value being ‘fragile’, such that an entity with ‘similar’ values will generally create a future of virtually no value.&amp;nbsp;&lt;/li&gt;&lt;li&gt;Finding goals that are extinction-level bad and temporarily useful appears to be easy: for example, corporations with the sole objective ‘maximize company revenue’ might profit for a time before gathering the influence and wherewithal to pursue the goal in ways that blatantly harm society.&lt;/li&gt;&lt;li&gt;Even if humanity found acceptable goals, giving a corporation any specific goals appears to be hard. We don’t know of any procedure to do it&lt;s&gt;, and we have theoretical reasons to expect that AI systems produced through machine learning training will generally end up with goals other than those that they were trained according to&lt;/s&gt;. Randomly aberrant goals resulting are probably extinction-level bad, for reasons described in II.1 above.&lt;br /&gt;&lt;/li&gt;&lt;/ol&gt;

  &lt;p&gt;&lt;strong&gt;III. If most goal-directed corporations have bad goals, the future will very likely be bad&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;That is, a set of ill-motivated goal-directed corporations, of a scale likely to occur, would be capable of taking control of the future from humans. This is supported by at least one of the following being true:&lt;/p&gt;

  &lt;ol&gt;&lt;li&gt;&lt;strong&gt;A corporation would destroy humanity rapidly&lt;/strong&gt;. This may be via ultra-powerful capabilities at e.g. technology design and strategic scheming, or through gaining such powers in an ‘intelligence explosion‘ (self-improvement cycle). Either of those things may happen either through exceptional heights of intelligence being reached or through highly destructive ideas being available to minds only mildly beyond our own.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Superhuman AI would gradually come to control the future via accruing power and resources.&lt;/strong&gt; Power and resources would be more available to the corporation than to humans on average, because of the corporation having far greater intelligence.&lt;/li&gt;&lt;/ol&gt;

  &lt;p&gt;This argument does point at real issues with corporations, but we do not generally consider such issues existentially deadly. &lt;/p&gt;

  &lt;p&gt;One might argue that there are defeating reasons that corporations do not destroy the world: they are made of humans so can be somewhat reined in; they are not smart enough; they are not coherent enough. But in that case, the original argument needs to make reference to these things, so that they apply to one and not the other.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;What it might look like if this counterargument matters: &lt;/em&gt;&lt;/strong&gt;&lt;em&gt;something like the current world. There are large and powerful systems doing things vastly beyond the ability of individual humans, and acting in a definitively goal-directed way. We have a vague understanding of their goals, and do not assume that they are coherent. Their goals are clearly not aligned with human goals, but they have enough overlap that many people are broadly in favor of their existence. They seek power. This all causes some problems, but problems within the power of humans and other organized human groups to keep under control, for some definition of ‘under control’.&lt;/em&gt;&lt;/p&gt;

  &lt;h2&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;

  &lt;p&gt;I think there are quite a few gaps in the argument, as I understand it. My current guess (prior to reviewing other arguments and integrating things carefully) is that enough uncertainties might resolve in the dangerous directions that existential risk from AI is a reasonable concern. I don’t at present though see how one would come to think it was overwhelmingly likely.&lt;/p&gt;

  &lt;p&gt;&lt;/p&gt;

&lt;/figure&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;That is, systems that are somewhat more capable than the most capable human. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Based on countless conversations in the AI risk community, and various reading. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Though not all: you might have an easily satiable utility function, or only care about the near future. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We are talking about divergence in a poorly specified multi-dimensional space, so it isn’t going to be a fixed distance in every direction from the ideal point. It could theoretically be zero distance on some dimensions, such that if AI was misaligned at all in those directions it was catastrophic. My point here is merely that there is some area larger than a point. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The Secrets of Our Success seems to be the canonical reference for this, but I haven’t read it. I don’t know how controversial this is, but also don’t presently see how it could fail to be true. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;See section ‘Intelligence may not be an overwhelming advantage’. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;E.g. for the metric ‘hardness of math problem solvable’, maybe no human can solve a level 10 math problem, but several can solve 9s. Then human society as a whole also can’t solve a 10. So the first AI that can is only mildly surpassing the best human, but is at the same time surpassing all of human society. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Probably I have this impression from reading Steven Pinker at some point. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Katja Grace</name></author><category term="ai" /><category term="airisk" /><category term="xrisk" /><category term="effectivealtruism" /><category term="meteuphoric" /><summary type="html">Crossposted from The AI Impacts blog. This is going to be a list of holes I see in the basic argument for existential risk from superhuman AI systems1. To start, here’s an outline of what I take to be the basic case2: I. If superhuman AI systems are built, any given system is likely to be ‘goal-directed’ Reasons to expect this: Goal-directed behavior is likely to be valuable, e.g. economically. Goal-directed entities may tend to arise from machine learning training processes not intending to create them (at least via the methods that are likely to be used).‘Coherence arguments’ may imply that systems with some goal-directedness will become more strongly goal-directed over time. II. If goal-directed superhuman AI systems are built, their desired outcomes will probably be about as bad as an empty universe by human lights Reasons to expect this: Finding useful goals that aren’t extinction-level bad appears to be hard: we don’t have a way to usefully point at human goals, and divergences from human goals seem likely to produce goals that are in intense conflict with human goals, due to a) most goals producing convergent incentives for controlling everything, and b) value being ‘fragile’, such that an entity with ‘similar’ values will generally create a future of virtually no value.Finding goals that are extinction-level bad and temporarily useful appears to be easy: for example, advanced AI with the sole objective ‘maximize company revenue’ might profit said company for a time before gathering the influence and wherewithal to pursue the goal in ways that blatantly harm society.Even if humanity found acceptable goals, giving a powerful AI system any specific goals appears to be hard. We don’t know of any procedure to do it, and we have theoretical reasons to expect that AI systems produced through machine learning training will generally end up with goals other than those they were trained according to. Randomly aberrant goals resulting are probably extinction-level bad for reasons described in II.1 above. III. If most goal-directed superhuman AI systems have bad goals, the future will very likely be bad That is, a set of ill-motivated goal-directed superhuman AI systems, of a scale likely to occur, would be capable of taking control over the future from humans. This is supported by at least one of the following being true: Superhuman AI would destroy humanity rapidly. This may be via ultra-powerful capabilities at e.g. technology design and strategic scheming, or through gaining such powers in an ‘intelligence explosion‘ (self-improvement cycle). Either of those things may happen either through exceptional heights of intelligence being reached or through highly destructive ideas being available to minds only mildly beyond our own.Superhuman AI would gradually come to control the future via accruing power and resources. Power and resources would be more available to the AI system(s) than to humans on average, because of the AI having far greater intelligence. Below is a list of gaps in the above, as I see it, and counterarguments. A ‘gap’ is not necessarily unfillable, and may have been filled in any of the countless writings on this topic that I haven’t read. I might even think that a given one can probably be filled. I just don’t know what goes in it. This blog post is an attempt to run various arguments by you all on the way to making pages on AI Impacts about arguments for AI risk and corresponding counterarguments. At some point in that process I hope to also read others’ arguments, but this is not that day. So what you have here is a bunch of arguments that occur to me, not an exhaustive literature review.  That is, systems that are somewhat more capable than the most capable human. &amp;#8617; Based on countless conversations in the AI risk community, and various reading. &amp;#8617;</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lh5.googleusercontent.com/mb3BSXp0EmzWZ7Xeiu_0NzeKeWR66fvYHhaG_zDo7QPPGKwZR9WALKHpMTSXx5JHUl8ygFgM4FSTyXYKcqYD60uyCWGNJa77hPzVON0gr8yPA8P0vzSlSIJv42G3Ydd_qwWGHZP9_GUHm45JBEqeaOM2dQyq4RXDWHboHmyVRvWDbTputoxjS5cZcg" /><media:content medium="image" url="https://lh5.googleusercontent.com/mb3BSXp0EmzWZ7Xeiu_0NzeKeWR66fvYHhaG_zDo7QPPGKwZR9WALKHpMTSXx5JHUl8ygFgM4FSTyXYKcqYD60uyCWGNJa77hPzVON0gr8yPA8P0vzSlSIJv42G3Ydd_qwWGHZP9_GUHm45JBEqeaOM2dQyq4RXDWHboHmyVRvWDbTputoxjS5cZcg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Calibration of a thousand predictions</title><link href="http://localhost:4000/2022/10/12/calibration.html" rel="alternate" type="text/html" title="Calibration of a thousand predictions" /><published>2022-10-12T01:38:00-07:00</published><updated>2022-10-12T01:38:00-07:00</updated><id>http://localhost:4000/2022/10/12/calibration</id><content type="html" xml:base="http://localhost:4000/2022/10/12/calibration.html">&lt;p&gt;I’ve been making predictions in a spreadsheet for the last four years, and I recently got to a thousand resolved predictions. Some observations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;I’m surprisingly well calibrated for things that mostly aren’t my own behavior&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Here’s the calibration curve for 630 resolved predictions in that class:&lt;/p&gt;

    &lt;p style=&quot;text-align:center;&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/Calibration_for_no-special-context_forecasts_(mostly_excluding_much_own-behavior_prediction)(1).png&quot; alt=&quot;calibration no context predictions&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;!--ex--&gt;&lt;/p&gt;

    &lt;p&gt;I don’t know what’s up with the 80% category, but the average miscalibration of the eleven categories is &amp;lt;3%.&lt;/p&gt;

    &lt;p&gt;At risk of bragging, this seems wild to me. My experience of making these predictions is fairly well described as ‘pulling a number out of thin air’&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. But apparently if you take all these conjured numbers, and look at the 45 of them that fell in the vicinity of 40%, then I implicitly guessed that 17.28 of those events would happen. And in fact 18 of them happened. WTF? Why wasn’t it eight of them or thirty-five of them? And that was only the fifth most accurate of the eleven buckets shown above! For predictions in the vicinity of 70%, I was off by 0.15%—I said 54.88 of those 80 things would happen, and in fact 55 of them happened.&lt;/p&gt;

    &lt;p&gt;Possibly people overall are just better calibrated than I thought. I had some remembered view that people’s asserted 90% confidence intervals were really 50% confidence intervals or something, but I can’t immediately find such evidence now, and I can find various graphs of &lt;a href=&quot;https://predictionbook.com/predictions&quot;&gt;groups&lt;/a&gt; of &lt;a href=&quot;https://www.researchgate.net/figure/Color-online-Calibration-Curves-Conditional-on-When-in-the-Questions-Life-the_fig1_320911494&quot;&gt;people&lt;/a&gt; &lt;a href=&quot;https://goodjudgment.com/wp-content/uploads/2021/10/Superforecasters-A-Decade-of-Stochastic-Dominance.pdf&quot;&gt;being&lt;/a&gt; on average fairly calibrated. And the &lt;a href=&quot;https://predictionbook.com/users/Baeboo&quot;&gt;handful&lt;/a&gt; &lt;a href=&quot;https://predictionbook.com/users/Tapetum-Lucidum&quot;&gt;of PredictionBook&lt;/a&gt; &lt;a href=&quot;https://predictionbook.com/users/gwern&quot;&gt;users&lt;/a&gt; &lt;a href=&quot;https://predictionbook.com/users/JoshuaZ&quot;&gt;I could&lt;/a&gt; &lt;a href=&quot;https://predictionbook.com/users/brunoparga&quot;&gt;find&lt;/a&gt; with more than a thousand predictions are not hugely worse.&lt;/p&gt;

    &lt;p&gt;If you are curious about what I predicted, I put examples at the end of this post.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the entire thousand predictions—the above plus 370 about my own behavior— I’m off by 6.25% on average (up from 2.95%) over the same eleven buckets.&lt;/p&gt;

    &lt;p style=&quot;text-align:center;&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/Calibration_for_all_predictions.png&quot; alt=&quot;calibration first 1000 resolved predictions&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As you may infer, I’m pretty bad overall at predicting my own behavior!&lt;/p&gt;

    &lt;p style=&quot;text-align:center;&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/Calibration_for_forecasts_with_context_(mostly_own-behavior)(1).png&quot; alt=&quot;calibration first 1000 resolved predictions&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;This is more what I expected of a calibration curve—broadly overconfident. And perhaps its general worseness is explained by the appeal of optimism in predicting oneself. But it’s a pretty weird shape, which seems less explicable. If I think there’s a 40% chance that I’ll do something, apparently it’s not happening. If you want it to happen, you should hope I change my mind and put 5% on it!&lt;/p&gt;

    &lt;p&gt;I’m not sure what is up with this particular strange shape. But note that making predictions about one’s own behavior has particular complication, if one likes to be right. If you put a number below 50% on taking an action, then you have a disincentive to doing it. So you should then put a lower probability on it than you would have, which would make you even more wrong if you took the action, so you have a further disincentive to doing it, etc. I do generally look for a fixed point where given that I put probability &lt;em&gt;p&lt;/em&gt; on something (and the incentive consequences of that), I do think it will happen with probability &lt;em&gt;p&lt;/em&gt;. But this is a different process than the usual predicting process, and I could imagine it going wrong in strange ways. For instance, if I’m more motivated by being right than I thought, then 40% predictions which might have been 50% predictions originally should really be 5% predictions. This theory doesn’t really work though, because then shouldn’t the lower categories also be especially overconfident? Whereas in fact they are okay.&lt;/p&gt;

    &lt;p&gt;(Maybe I just have free will? The kind of free will that manifests as being about 15% less likely to do anything than one might have expected seems disappointing, but the menu of possible free will options was never that inspiring.)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;example-predictions&quot;&gt;Example predictions&lt;/h2&gt;

&lt;p&gt;Here are some typical predictions, arbitrarily pulled from my spreadsheet and lightly edited:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I will be invited to play ONUW today: 0.45 (true)&lt;/li&gt;
  &lt;li&gt;The trial bank transfers come through to my IBKR account by the end of Monday: 0.34 (false)&lt;/li&gt;
  &lt;li&gt;[Friend] will want to leave here for the day before I do: 0.05 (false)&lt;/li&gt;
  &lt;li&gt;[Friend] will seem notably sad in demeanor when I talk to [them]: 0.6 (false)&lt;/li&gt;
  &lt;li&gt;I will be paid by end Jan 27: 0.85 (true)&lt;/li&gt;
  &lt;li&gt;If I go inside shortly I see [friend]: 0.08 (false)&lt;/li&gt;
  &lt;li&gt;We have the [organization] party here: 0.55 (true)&lt;/li&gt;
  &lt;li&gt;I go to math party today: 0.88 (true)&lt;/li&gt;
  &lt;li&gt;I will get my period on Tuesday: 0.10 (true)&lt;/li&gt;
  &lt;li&gt;We will be invited to work at [office] for at least a week: 0.75 (false)&lt;/li&gt;
  &lt;li&gt;On Feb 5 we (including [friend], me, [friend]) are renting a new place: 0.73 (true)&lt;/li&gt;
  &lt;li&gt;We will run the arts and crafts room auction today (i.e. by midnight we will have as much info from the auction as we will get re which room is whos, ignoring processing of info we have): 0.40 (true)&lt;/li&gt;
  &lt;li&gt;[Person] is leading a new EAish org or CEO or COO of an existing EAish org by May 30 2023, where EAish includes orgs not culturally EA but doing things that are considered so by a decent fraction of EAs: 0.62 (TBD)&lt;/li&gt;
  &lt;li&gt;I will get to the office in time for lunch: 0.95 (true)&lt;/li&gt;
  &lt;li&gt;I see [housemate] tonight before midnight: 0.88 (true)&lt;/li&gt;
  &lt;li&gt;If I ask [attendee] ‘what did you think of [event I ran]?’, [they] will be strongly positive: 0.8 (false)&lt;/li&gt;
  &lt;li&gt;I see [friend]’s dad tonight before midnight: 0.95 (forgot to notice)&lt;/li&gt;
  &lt;li&gt;If I offer [person] a trial [they] will take it: 0.65 (true)&lt;/li&gt;
  &lt;li&gt;If I look at [friend]’s most recent Tweet, it is linking to [their] blog: 0.8 (false)&lt;/li&gt;
  &lt;li&gt;My weight is under [number] again before it is over [number] again: 0.75 (false)&lt;/li&gt;
  &lt;li&gt;I do all of my Complice goals tomorrow: 0.3 (false)&lt;/li&gt;
  &lt;li&gt;I will go to the office on Friday: 0.6 (true)&lt;/li&gt;
  &lt;li&gt;I will read the relevant chapter of Yuval book by the end of 2nd: 0.1 (false)&lt;/li&gt;
  &lt;li&gt;I weigh less than [weight] the first time I weigh myself tomorrow: 0.65 (true)&lt;/li&gt;
  &lt;li&gt;Lunch includes no kind of fish meat: 0.43 (true)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And some examples of own-behavior marked predictions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Work goal as stated will be completed by end of day: start a document of feedback policies: 0.90 (true)&lt;/li&gt;
  &lt;li&gt;I ok [person]’s post before the meeting: 0.68 (false)&lt;/li&gt;
  &lt;li&gt;Work goal as stated, will be completed by Sunday 28th October: Respond to [person]: 0.80 (true)&lt;/li&gt;
  &lt;li&gt;Work goal as stated will be completed by midnight Sunday 30 September 2018: read [person]’s research: 0.4 (false)&lt;/li&gt;
  &lt;li&gt;Work goal as stated, will be completed by Sunday 4th November: Arrange to talk to [other researcher] about [employee] project […]: 0.3 (false)&lt;/li&gt;
  &lt;li&gt;Work goal as stated will be completed by midnight Sunday 30 September 2018: Think for 1h about [colleague] thing: 0.5 (true)&lt;/li&gt;
  &lt;li&gt;I have fewer than 1k emails in inbox at some point on Feb 10th: 0.87 (true)&lt;/li&gt;
  &lt;li&gt;I have written to [brother] by Feb 10th: 0.82 (true)&lt;/li&gt;
  &lt;li&gt;I will be home by 9pm: 0.97 (true)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Categories missing from these randomishly selected lists but notable in being particularly fun:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Predictions of history that I don’t know or remember, followed by looking it up on Wikipedia. A pretty fun combination of predicting things and reading Wikipedia.&lt;/li&gt;
  &lt;li&gt;Predictions of relationship persistence in successive episodes of &lt;em&gt;Married at First Sight&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
—&lt;/p&gt;
&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I have a column where I write context on some predictions, which is usually that they are my own work goal, or otherwise a prediction about how I will behave. This graph excludes those, but keeps in some own-behavior prediction which I didn’t flag for whatever reason.) &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Except maybe more like art—do you know that feeling where you look at the sketch, and tilt your head from side to side, and say ‘no, a little bit more… just there….mmm…yes…’? It’s like that: ‘27%…no, a little more, 29%? No, 33%. 33%, yes.’ Except honestly it’s more ridiculous than that, because my brain often seems to have views about which particular digits should be involved. So it’s like, ‘23%…no, but mmm 3…33%, yes.’ I am generally in favor of question decomposition and outside views and all that, but to be clear, that’s not what I’m doing here. I might have been sometimes, but these are usually fast intuitive judgments. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Katja Grace</name></author><category term="experiment" /><category term="prediction" /><summary type="html">I’ve been making predictions in a spreadsheet for the last four years, and I recently got to a thousand resolved predictions. Some observations: I’m surprisingly well calibrated for things that mostly aren’t my own behavior1. Here’s the calibration curve for 630 resolved predictions in that class: I have a column where I write context on some predictions, which is usually that they are my own work goal, or otherwise a prediction about how I will behave. This graph excludes those, but keeps in some own-behavior prediction which I didn’t flag for whatever reason.) &amp;#8617;</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/Calibration_for_no-special-context_forecasts_(mostly_excluding_much_own-behavior_prediction)(1).png" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/Calibration_for_no-special-context_forecasts_(mostly_excluding_much_own-behavior_prediction)(1).png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A game of mattering</title><link href="http://localhost:4000/2022/09/22/2022-game.html" rel="alternate" type="text/html" title="A game of mattering" /><published>2022-09-22T19:16:00-07:00</published><updated>2022-09-22T19:16:00-07:00</updated><id>http://localhost:4000/2022/09/22/2022-game</id><content type="html" xml:base="http://localhost:4000/2022/09/22/2022-game.html">&lt;p&gt;When I have an overwhelming number of things to do, and insufficient native urge to do them, I often arrange them into a kind of game for myself. The nature and appeal of this game has been relatively stable for about a year, after many years of evolution, so this seems like a reasonable time to share it. I also play it when I just want to structure my day and am in the mood for it. I currently play something like two or three times a week.&lt;/p&gt;

&lt;h1 id=&quot;the-game&quot;&gt;The game&lt;/h1&gt;

&lt;p&gt;The basic idea is to lay out the tasks in time a bit like obstacles in a platformer or steps in Dance Dance Revolution, then race through the obstacle course grabbing them under consistently high-but-doable time pressure.&lt;/p&gt;

&lt;p&gt;Here’s how to play:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Draw a grid with as many rows as there are remaining hours in your hoped for productive day, and ~3 columns. Each box stands for a particular ~20 minute period (I sometimes play with 15m or 30m periods.)&lt;/li&gt;
  &lt;li&gt;Lay out the gameboard: break the stuff you want to do into appropriate units, henceforth ‘items’. An item should fit comfortably in the length of a box, and it should be easy enough to verify completion. (This can be achieved through house rules such as ‘do x a tiny bit = do it until I have a sense that an appropriate tiny bit has been done’ as long as you are happy applying them). Space items out a decent amount so that the whole course is clearly feasible. Include everything you want to do in the day, including nice or relaxing things, or break activities. Drinks, snacks, tiny bouts of exercise, looking at news sites for 5 minutes, etc. Design the track thoughtfully, with hard bouts followed by relief before the next hard bout.&lt;/li&gt;
  &lt;li&gt;To play, start in the first box, then move through the boxes according to the time of day. The goal in playing is to collect as many items as you can, as you are forced along the track by the passage of time. You can collect an item by doing the task in or before you get to the box it is in. If it isn’t done by the end of the box, it gets left behind. However if you clear any box entirely, you get to move one item anywhere on the gameboard. So you can rescue something from the past, or rearrange the future to make it more feasible, or if everything is perfect, you can add an entirely new item somewhere.&lt;!--ex--&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I used to play this with tiny post-it stickers, which I would gather in a large moving pile, acting as a counter:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/PXL_20210915_214726176.jpg&quot; alt=&quot;example of game with stickers&quot; width=&quot;300&quot; /&gt; &lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/PXL_20210710_183723811.jpg&quot; alt=&quot;example of game with stickers&quot; width=&quot;300&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Now I just draw the whole thing. Crossed out = collected; [] = rescued from the past, now implicitly in the final box; dot in the lower right = box cleared; dot next to item = task done but item stuck in the past (can be collected immediately if rescued).&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/IMG_0898.jpeg&quot; alt=&quot;IMG_0898&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;h1 id=&quot;why-is-this-good&quot;&gt;Why is this good?&lt;/h1&gt;

&lt;p&gt;I think a basic problem with working on a big pile of things in a big expanse of time is that if you work or not during any particular minute, it feels like it makes nearly no difference to the expectation of success. I’m not quite sure why this is—in fact if I don’t work this minute, I’m going to get one minute less work done. But it feels like if I don’t work this minute, I only need to work a smidgen faster on average to get any particular amount of work done, so what does it matter if I work now or later? And if i had some particular goal (e.g. finishing writing some massive text today), it’s unlikely that my other efforts will get me exactly to the line where this minute pushed me over—probably I will either succeed with hours to spare (haha) or fail hours from my goals.&lt;/p&gt;

&lt;p&gt;I picture what’s going on as vaguely something like this—there is often some amount of work that is going to make your success likely, and if you know that you are on a locally steep part of the curve, it is more motivating than if you are either far away from the steep part or don’t know where you are:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/IMG_0951.HEIC&quot; alt=&quot;IMG_0898&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Yet on the other hand, the appeal of various non-work activities this specific minute might be the most distinct and tangible things in the world. So when there is a lot to be done in a long time, not working often looks more exciting than working, even if a more rational accounting would disagree.&lt;/p&gt;

&lt;p&gt;Having a single specific thing to do within minutes is much more compelling: the task and the time are lined up so that my action right now matters. Slacking this minute is the difference between success and failure.&lt;/p&gt;

&lt;p&gt;It feels very different to have one email to deal with in three minutes and to have a thousand to deal with in next fifty hours.
&lt;!-- In the latter case, while in some sense the time pressure is the same, I probably start by getting myself a drink and pottering around staring out of the window or something. --&gt;&lt;/p&gt;

&lt;p&gt;One might naively respond to this issue by breaking up one’s tasks into tiny chunks, then laying them out in a day of tiny time boxes, then aiming for each to happen by the end of its allotment. But this will be terrible. A few boxes in, either you’ll be ahead or behind. And either way, your immediate actions have drifted away from feeling like they matter. If you are ahead, the pressure is off: you’ll probably succeed at the next increment whether or not you work hard now. If behind, you are definitely going to fail at doing the next box on time, and probably some others, and your present work is for an increased chance of catching up at some vague future box, much like before you had these boxes. (Plus your activities are no longer in line with what your plan was, which for me makes it tempting to scrap the whole thing and do something else.)
&lt;!-- and can substitute with all of the other intervening time. So again, working less hard this minute means you have to work a tiny bit harder in the next five boxes say, to catch up then. So what you do in this minute doesn&apos;t viscerally change the odds much. --&gt;&lt;/p&gt;

&lt;p&gt;A big innovation of this game is to instead ensure that you keep meeting tasks one at a time where each one matters in its moment, as in a game like Beat Saber or Dance Dance Revolution. The game achieves this by adjusting the slack to keep the next ten minutes’ action near the actually-mattering-to-success region all day. If you get behind you have to give up on items and move forward, so you aren’t left struggling for a low probability of catching up. If you get ahead, you add more items and thus tighten the slack.&lt;/p&gt;

&lt;p&gt;A thing I like about this is that it actually makes the activity more genuinely fun and compelling, and doesn’t involve trying to trick or uncomfortably binding oneself. It is superficially a lot like a ‘productivity hack’, but I associate these with somehow manipulating or forcing yourself to do something that you at some level have real reason to dislike. I expect such tricks to fail, and I don’t think I want them to succeed.&lt;/p&gt;

&lt;p&gt;This seems different: I think humans are just genuinely better at being in an enjoyable flow state when their activities have certain structures that are genuinely compatible with a variety of tasks. Beat saber wouldn’t be fun if all the boxes were just sitting in a giant pile and you had to beat your way through as many as you could over an hour. But with the boxes approaching one at a time, at a manageable rate, where what you do in each moment matters, it really is fun (for many people, I hear—I actually don’t love it, but I do appreciate this particular aspect). The same thing that makes Beat Saber more fun than Saber-a-bunch-of-boxes-on-your-own-schedule can genuinely also be applied to giant piles of tasks.&lt;/p&gt;

&lt;p&gt;The fact that this game has lasted a year in my life and I come back to it with verve points to it not being an enemy to any major part of myself.&lt;/p&gt;

&lt;p&gt;Another promising way of seeing this game is that this structure lets you see more clearly the true importance of each spent minute, when you were by default in error. Whereas for instance playing Civ IV for five minutes every time you do work (another sometimes way-of-being of mine) is less like causing yourself to perceive reality truly and more like trying to build an alternate incentive structure out of your mistaken perception, that adds up to rational behavior in the real world.&lt;/p&gt;

&lt;p&gt;If anyone else tries this, I’m curious to hear how it goes. My above explanation of its merit suggests it might be of broad value. But I also know that perhaps nobody in the world likes organizing things into little boxes as much as I do, so that could also be the main thing going on.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="advice" /><category term="game" /><category term="meteuphoric" /><category term="worldlypositions" /><summary type="html">When I have an overwhelming number of things to do, and insufficient native urge to do them, I often arrange them into a kind of game for myself. The nature and appeal of this game has been relatively stable for about a year, after many years of evolution, so this seems like a reasonable time to share it. I also play it when I just want to structure my day and am in the mood for it. I currently play something like two or three times a week. The game The basic idea is to lay out the tasks in time a bit like obstacles in a platformer or steps in Dance Dance Revolution, then race through the obstacle course grabbing them under consistently high-but-doable time pressure. Here’s how to play: Draw a grid with as many rows as there are remaining hours in your hoped for productive day, and ~3 columns. Each box stands for a particular ~20 minute period (I sometimes play with 15m or 30m periods.) Lay out the gameboard: break the stuff you want to do into appropriate units, henceforth ‘items’. An item should fit comfortably in the length of a box, and it should be easy enough to verify completion. (This can be achieved through house rules such as ‘do x a tiny bit = do it until I have a sense that an appropriate tiny bit has been done’ as long as you are happy applying them). Space items out a decent amount so that the whole course is clearly feasible. Include everything you want to do in the day, including nice or relaxing things, or break activities. Drinks, snacks, tiny bouts of exercise, looking at news sites for 5 minutes, etc. Design the track thoughtfully, with hard bouts followed by relief before the next hard bout. To play, start in the first box, then move through the boxes according to the time of day. The goal in playing is to collect as many items as you can, as you are forced along the track by the passage of time. You can collect an item by doing the task in or before you get to the box it is in. If it isn’t done by the end of the box, it gets left behind. However if you clear any box entirely, you get to move one item anywhere on the gameboard. So you can rescue something from the past, or rearrange the future to make it more feasible, or if everything is perfect, you can add an entirely new item somewhere.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/PXL_20210915_214726176.jpg" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/PXL_20210915_214726176.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Update updates</title><link href="http://localhost:4000/2022/09/21/update-update.html" rel="alternate" type="text/html" title="Update updates" /><published>2022-09-21T22:14:00-07:00</published><updated>2022-09-21T22:14:00-07:00</updated><id>http://localhost:4000/2022/09/21/update-update</id><content type="html" xml:base="http://localhost:4000/2022/09/21/update-update.html">&lt;p&gt;You can now read or subscribe to this blog via &lt;a href=&quot;https://worldspiritsockpuppet.substack.com/&quot;&gt;world spirit sock stack&lt;/a&gt;, a Substack mirror of this site. I expect to see comments at &lt;a href=&quot;https://worldspiritsockpuppet.substack.com/&quot;&gt;wsss&lt;/a&gt; similarly often to &lt;a href=&quot;worldspiritsockpuppet.com/&quot;&gt;wssp&lt;/a&gt; (with both being more often than at various other places this crossposts, e.g. LessWrong).&lt;/p&gt;

&lt;p&gt;You can also be alerted to posts on Twitter via &lt;a href=&quot;https://twitter.com/wssockpuppet&quot;&gt;@wssockpuppet&lt;/a&gt;. I’m going to continue to Tweet about some subset of things on my &lt;a href=&quot;https://twitter.com/KatjaGrace&quot;&gt;personal account&lt;/a&gt;, so this runs a risk of double-seeing things.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="logistics" /><summary type="html">You can now read or subscribe to this blog via world spirit sock stack, a Substack mirror of this site. I expect to see comments at wsss similarly often to wssp (with both being more often than at various other places this crossposts, e.g. LessWrong). You can also be alerted to posts on Twitter via @wssockpuppet. I’m going to continue to Tweet about some subset of things on my personal account, so this runs a risk of double-seeing things.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/Screen_Shot_2022-09-21_at_10.19.11_PM.png" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/Screen_Shot_2022-09-21_at_10.19.11_PM.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Podcasts on surveys, slower AI, AI arguments</title><link href="http://localhost:4000/2022/09/17/im-on-podcasts.html" rel="alternate" type="text/html" title="Podcasts on surveys, slower AI, AI arguments" /><published>2022-09-17T23:16:00-07:00</published><updated>2022-09-17T23:16:00-07:00</updated><id>http://localhost:4000/2022/09/17/im-on-podcasts</id><content type="html" xml:base="http://localhost:4000/2022/09/17/im-on-podcasts.html">&lt;p&gt;I recently talked to Michael Trazzi for his podcast, The Inside View. It just came out, so if that’s a conversation you want to sit in on, do so &lt;a href=&quot;https://www.youtube.com/watch?v=rSw3UVDZge0&quot;&gt;here&lt;/a&gt; [ETA: or read it &lt;a href=&quot;https://theinsideview.ai/katja&quot;&gt;here&lt;/a&gt;].&lt;/p&gt;

&lt;p&gt;The main topics were the &lt;a href=&quot;https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/&quot;&gt;survey of ML folk&lt;/a&gt; I recently ran, and my thoughts on moving more slowly on potentially world-threatening AI research (which is to say, AI research in general, &lt;a href=&quot;https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/&quot;&gt;according to&lt;/a&gt; the median surveyed ML researcher…). I also bet him a thousand dollars to his hundred that AI would not make blogging way more efficient in two years, if I recall. (I forget the exact terms, and there’s no way I’m listening to myself talk for that long to find out. If anyone else learns, I’m curious what I agreed to.)&lt;/p&gt;

&lt;p&gt;For completeness of podcast reporting: I forgot to mention that &lt;a href=&quot;https://axrp.net/episode/2021/07/23/episode-10-ais-future-and-dangers-katja-grace.html&quot;&gt;I also talked to Daniel Filan on AXRP&lt;/a&gt;, like a year ago. In other old news, I am opposed to the vibe of time-sensitivity often implicit in the public conversation.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="ai" /><category term="xrisk" /><category term="podcast" /><category term="effectivealtruism" /><category term="meteuphoric" /><category term="worldlypositions" /><summary type="html">I recently talked to Michael Trazzi for his podcast, The Inside View. It just came out, so if that’s a conversation you want to sit in on, do so here [ETA: or read it here]. The main topics were the survey of ML folk I recently ran, and my thoughts on moving more slowly on potentially world-threatening AI research (which is to say, AI research in general, according to the median surveyed ML researcher…). I also bet him a thousand dollars to his hundred that AI would not make blogging way more efficient in two years, if I recall. (I forget the exact terms, and there’s no way I’m listening to myself talk for that long to find out. If anyone else learns, I’m curious what I agreed to.) For completeness of podcast reporting: I forgot to mention that I also talked to Daniel Filan on AXRP, like a year ago. In other old news, I am opposed to the vibe of time-sensitivity often implicit in the public conversation.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/Screen_Shot_2022-09-17_at_11.35.55_PM.png" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/Screen_Shot_2022-09-17_at_11.35.55_PM.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-03-26T21:28:37-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">world spirit sock puppet</title><subtitle>Inclusive writings of Katja Grace</subtitle><author><name>Katja Grace</name></author><entry><title type="html">Coherence arguments imply a force for goal-directed behavior</title><link href="http://localhost:4000/2021/03/26/coherence-arguments-imply-a-force-for-goal-directed-behavior.html" rel="alternate" type="text/html" title="Coherence arguments imply a force for goal-directed behavior" /><published>2021-03-26T21:20:00-07:00</published><updated>2021-03-26T21:20:00-07:00</updated><id>http://localhost:4000/2021/03/26/coherence-arguments-imply-a-force-for-goal-directed-behavior</id><content type="html" xml:base="http://localhost:4000/2021/03/26/coherence-arguments-imply-a-force-for-goal-directed-behavior.html">&lt;!--ex--&gt;

&lt;p&gt;&lt;em&gt;&lt;em&gt;Crossposted from &lt;a href=&quot;https://aiimpacts.org/&quot;&gt;AI Impacts&lt;/a&gt;&lt;/em&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[Epistemic status: my current view, but I haven’t read all the stuff on this topic even in the LessWrong community, let alone more broadly.]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There is a line of thought that says that advanced AI will tend to be ‘goal-directed’—that is, consistently doing whatever makes certain favored outcomes more likely—and that this is to do with the ‘coherence arguments’. &lt;a href=&quot;https://www.lesswrong.com/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-imply-goal-directed-behavior&quot;&gt;Rohin Shah&lt;/a&gt;, and probably others&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; , have argued against this. I want to argue against them.&lt;/p&gt;

&lt;!-- wp:heading --&gt;
&lt;h2&gt;The old argument for coherence implying (worrisome) goal-directedness&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;

&lt;p&gt;I’d reconstruct the original argument that Rohin is arguing against as something like this (making no claim about my own beliefs here):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;‘Whatever things you care about, you are best off assigning consistent numerical values to them and maximizing the expected sum of those values’&lt;/strong&gt; &lt;br /&gt; ‘Coherence arguments’&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; mean that if you don’t maximize ‘expected utility’ (EU)—that is, if you don’t make every choice in accordance with what gets the highest average score, given consistent preferability scores that you assign to all outcomes—then you will make strictly worse choices by your own lights than if you followed some alternate EU-maximizing strategy (at least in some situations, though they may not arise). For instance, you’ll be vulnerable to ‘&lt;a href=&quot;https://www.oxfordreference.com/view/10.1093/oi/authority.20110803100205601&quot;&gt;money-pumping&lt;/a&gt;’—being predictably parted from your money for nothing.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;‘Advanced AI will tend to do better things instead of worse things, by its own lights’&lt;/strong&gt; &lt;br /&gt;Advanced AI will tend to avoid options that are predictably strictly worse by its own lights, due to being highly optimized for making good choices (by some combination of external processes that produced it, its own efforts, and the selection pressure acting on its existence).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;‘Therefore advanced AI will maximize EU, roughly’&lt;/strong&gt; &lt;br /&gt;Advanced AI will tend to be fairly coherent, at least to a level of approximation where becoming more coherent isn’t worth the cost.&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; Which will probably be fairly coherent (e.g. close enough to coherent that &lt;a href=&quot;https://arbital.com/p/optimized_agent_appears_coherent/&quot;&gt;humans can’t anticipate the inconsistencies&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;‘Maximizing EU is pretty much the same as being goal-directed’&lt;/strong&gt;&lt;br /&gt;To maximize expected utility is to pursue the goal of that which you have assigned higher utility to.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And since the point of all this is to argue that advanced AI might be hard to deal with, note that we can get to that conclusion with:&lt;/p&gt;

&lt;!-- wp:list {&quot;ordered&quot;:true,&quot;start&quot;:5} --&gt;
&lt;ol start=&quot;5&quot;&gt;&lt;li&gt;&lt;strong&gt;‘Highly intelligent goal-directed agents are dangerous’&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;If AI systems exist that very competently pursue goals, they will likely be better than us at attaining their goals, and therefore to the extent there is a risk of mismatch between their goals and ours, we face a serious risk.&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:heading --&gt;
&lt;h2&gt;Rohin’s counterargument&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;

&lt;p&gt;Rohin’s counterargument begins with an observation made by others before: any behavior is consistent with maximizing expected utility, given &lt;em&gt;some&lt;/em&gt; utility function. For instance, a creature just twitching around on the ground may have the utility function that returns 1 if the agent does whatever it in fact does in each situation (where ‘situation’ means, ‘entire history of the world so far’), and 0 otherwise. This is a creature that just wants to make the right twitch in each detailed, history-indexed situation, with no regard for further consequences. Alternately the twitching agent might care about outcomes, but just happen to want the particular holistic unfolding of the universe that is occurring, including this particular series of twitches. Or it could be indifferent between all outcomes.&lt;/p&gt;

&lt;p&gt;The basic point is that rationality doesn’t say what ‘things’ you can want. And in particular, it doesn’t say that you have to care about particular atomic units that larger situations can be broken down into. If I try to call you out for first spending money to get to Paris, then spending money to get back from Paris, there is nothing to say you can’t just have wanted to go to Paris for a bit and then to come home. In fact, this is a common human situation. ‘Aha, I money pumped you!’ says the airline, but you aren’t worried. The twitching agent might always be like this—a creature of more refined tastes, who cares about whole delicate histories and relationships, rather than just summing up modular momentarily-defined successes. And given this freedom, any behavior might conceivably be what a creature wants. &lt;/p&gt;

&lt;p&gt;Then I would put the full argument, as I understand it, like this:&lt;/p&gt;

&lt;!-- wp:list {&quot;ordered&quot;:true} --&gt;
&lt;ol&gt;&lt;li&gt;Any observable sequence of behavior is consistent with the entity doing EU maximization (see observation above)&lt;/li&gt;&lt;li&gt;Doing EU maximization doesn’t imply anything about what behavior we might observe (from 1)&lt;/li&gt;&lt;li&gt;In particular, knowing that a creature is an EU maximizer doesn’t imply that it will behave in a ‘goal-directed’ way, assuming that &lt;em&gt;that&lt;/em&gt; concept doesn’t apply to all behavior. (from 2)&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;p&gt;Is this just some disagreement about the meaning of the word ‘goal-directed’? No, because we can get back to a major difference in physical expectations by adding:&lt;/p&gt;

&lt;!-- wp:list {&quot;ordered&quot;:true,&quot;start&quot;:4} --&gt;
&lt;ol start=&quot;4&quot;&gt;&lt;li&gt;&amp;nbsp;Not all behavior in a creature implicates dire risk to humanity, so any concept of goal-directedness that is consistent with any behavior—and so might be implied by the coherence arguments—cannot imply AI risk.&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;p&gt;So where the original argument says that the coherence arguments plus some other assumptions imply danger from AI, this counterargument says that they do not. &lt;/p&gt;

&lt;p&gt;(There is also at least some variety in the meaning of ‘goal-directed’. I’ll use goal-directed&lt;sub&gt;Rohin&lt;/sub&gt; to refer to what I think is Rohin’s &lt;a href=&quot;https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma&quot;&gt;preferred&lt;/a&gt; usage: roughly, that which seems intuitively goal directed to us, e.g. behaving similarly across situations, and accruing resources, and not flopping around in possible pursuit of some exact history of personal floppage, or peaceably preferring to always take the option labeled ‘A’.&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;)&lt;/p&gt;

&lt;!-- wp:heading --&gt;
&lt;h2&gt;My counter-counterarguments&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;

&lt;p&gt;What’s wrong with Rohin’s counterargument? It sounded tight. &lt;/p&gt;

&lt;p&gt;In brief, I see two problems:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A. The whole argument is in terms of logical implication. But what seems to matter is changes in probability. Coherence doesn’t need to rule out any behavior to matter, it just has to change the probabilities of behaviors. Understood in terms of probability, argument 2 is a false inference: just because any sequence of behavior is consistent with EU maximization doesn’t mean that EU maximization says nothing about what behavior we will see, probabilistically. All it says is that the probability of a behavioral sequence is never reduced to zero by considerations of coherence alone, which is hardly saying anything.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You might then think that a probabilistic version still applies: since every entity appears to be in good standing with the coherence arguments, the arguments don’t exert any force, probabilistically, on what entities we might see. But:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;B. An outside observer being able to rationalize a sequence of observed behavior as coherent doesn’t mean that the behavior is actually coherent. Coherence arguments constrain combinations of external behavior and internal features—‘preferences’&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; and beliefs. So whether an actor is coherent depends on what preferences and beliefs it actually has. And if it isn’t coherent in light of these, then coherence pressures will apply, whether or not its behavior &lt;em&gt;looks&lt;/em&gt; coherent. And in many cases, revision of preferences due to coherence pressures will end up affecting external behavior. So 2) is not only not a sound inference from 1), but actually a wrong conclusion: if a system moves toward EU maximization, that does imply things about the behavior that we will observe (probabilistically).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Perhaps Rohin only meant to argue about whether it is &lt;em&gt;logically possible&lt;/em&gt; to be coherent and not goal-directed-seeming, for the purpose of arguing that humanity can construct creatures in that perhaps-unlikely-in-nature corner of mindspace, if we try hard. In which case, I agree that it is logically possible. But I think his argument is often taken to be relevant more broadly, to questions of whether advanced AI will tend to be goal-directed, or to be goal-directed in places where they were not intended to be.&lt;/p&gt;

&lt;p&gt;I take A) to be fairly clear. I’ll lay out B) in more detail.&lt;/p&gt;

&lt;!-- wp:heading --&gt;
&lt;h2&gt;My counter-counterarguments in more detail&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:heading {&quot;level&quot;:3} --&gt;
&lt;h3&gt;&lt;strong&gt;How might coherence arguments affect creatures?&lt;/strong&gt;&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;p&gt;Let us step back.&lt;/p&gt;

&lt;p&gt;How would coherence arguments affect an AI system—or anyone—anyway? They’re not going to fly in from the platonic realm and reshape irrational creatures.&lt;/p&gt;

&lt;p&gt;The main routes, as I see it, are via implying:&lt;/p&gt;

&lt;!-- wp:list {&quot;ordered&quot;:true} --&gt;
&lt;ol&gt;&lt;li&gt;incentives for the agent itself to reform incoherent preferences&lt;/li&gt;&lt;li&gt;incentives for the processes giving rise to the agent (explicit design, or selection procedures directed at success) to make them more coherent&lt;/li&gt;&lt;li&gt;some advantage for coherent agents in competition with incoherent agents&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;p&gt;To be clear, the agent, the makers, or the world are not necessarily thinking about the arguments here—the arguments correspond to incentives in the world, which these parties are responding to. So I’ll often talk about ‘incentives for coherence’ or ‘forces for coherence’ rather than ‘coherence arguments’.&lt;/p&gt;

&lt;p&gt;I’ll talk more about 1 for simplicity, expecting 2 and 3 to be similar, though I haven’t thought them through.&lt;/p&gt;

&lt;!-- wp:heading {&quot;level&quot;:3} --&gt;
&lt;h3&gt;&lt;strong&gt;&lt;em&gt;Looking&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt; coherent isn’t enough: if you aren’t coherent inside, coherence forces apply&lt;/strong&gt;&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;p&gt;If self-adjustment is the mechanism for the coherence, this doesn’t depend on what a sequence of actions looks like from the outside, but from what it looks like from the inside.&lt;/p&gt;

&lt;p&gt;Consider the aforementioned creature just twitching sporadically on the ground. Let’s call it Alex.&lt;/p&gt;

&lt;p&gt;As noted earlier, there is a utility function under which Alex is maximizing expected utility: the one that assigns utility 1 to however Alex in fact acts in every specific history, and utility 0 to anything else.&lt;/p&gt;

&lt;p&gt;But from the inside, this creature you excuse as ‘maybe just wanting that series of twitches’ has—let us suppose—actual preferences and beliefs. And if its preferences do not in fact prioritize this elaborate sequence of twitching in an unconflicted way, and it has the self-awareness and means to make corrections, then it will make corrections&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;. And having done so, its behavior will change. &lt;/p&gt;

&lt;p&gt;Thus excusable-as-coherent Alex is still moved by coherence arguments, even while the arguments have no complaints about its behavior &lt;em&gt;per se&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;For a more realistic example: suppose Assistant-Bot is observed making this sequence of actions: &lt;/p&gt;

&lt;!-- wp:list --&gt;
&lt;ul&gt;&lt;li&gt;Offers to buy gym membership for $5/week&amp;nbsp;&lt;/li&gt;&lt;li&gt;Consents to upgrade to gym-pro membership for $7/week, which is like gym membership but with added morning classes&lt;/li&gt;&lt;li&gt;Takes discounted ‘off-time’ deal, saving $1 per week for only using gym in evenings&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;p&gt;This is consistent with coherence: Assistant-Bot might prefer that exact sequence of actions over all others, or might prefer incurring gym costs with a larger sum of prime factors, or might prefer talking to Gym-sales-bot over ending the conversation, or prefer agreeing to things.&lt;/p&gt;

&lt;p&gt;But suppose that &lt;em&gt;in fact&lt;/em&gt;, in terms of the structure of the internal motivations producing this behavior, Assistant-Bot just prefers you to have a gym membership, and prefers you to have a better membership, and prefers you to have money, but is treating these preferences with inconsistent levels of strength in the different comparisons. Then there appears to be a coherence-related force for Assistant-Bot to change. One way that that could look is that since Assistant-Bot’s overall behavioral policy currently entails giving away money for nothing, and also Assistant-Bot prefers money over nothing, that preference gives Assistant-Bot reason to alter its current overall policy, to avert the ongoing exchange of money for nothing.&lt;sup id=&quot;fnref:10&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; And if its behavioral policy is arising from something like preferences, then the natural way to alter it is via altering those preferences, and in particular, altering them in the direction of coherence.&lt;/p&gt;

&lt;p&gt;One issue with this line of thought is that it’s not obvious in what sense there is anything inside a creature that corresponds to ‘preferences’. Often when people posit preferences, the preferences are defined in terms of behavior. Does it make sense to discuss different possible ‘internal’ preferences, distinct from behavior? I find it helpful to consider the behavior and ‘preferences’ of groups:&lt;/p&gt;

&lt;p&gt;Suppose two cars are parked in driveways, each containing a couple. One couple are just enjoying hanging out in the car. The other couple are dealing with a conflict: one wants to climb a mountain together, and the other wants to swim in the sea together, and they aren’t moving because neither is willing to let the outing proceed as the other wants. ‘Behaviorally’, both cars are the same: stopped. But their internal parts (the partners) are importantly different. And in the long run, we expect different behavior: the car with the unconflicted couple will probably stay where it is, and the conflicted car will (hopefully) eventually resolve the conflict and drive off.&lt;/p&gt;

&lt;p&gt;I think here it makes sense to talk about internal parts, separate from behavior, and real. And similarly in the single agent case: there are physical mechanisms producing the behavior, which can have different characteristics, and which in particular can be ‘in conflict’—in a way that motivates change—or not. I think it is also worth observing that humans find their preferences ‘in conflict’ and try to resolve them, which is suggests that they at least are better understood in terms of both behavior and underlying preferences that are separate from it. &lt;/p&gt;

&lt;p&gt;So we have: even if you can excuse any seizuring as consistent with coherence, coherence incentives still exert a force on creatures that are&lt;em&gt; in fact&lt;/em&gt; incoherent, given their real internal state (or would be incoherent if created). At least if they or their creator have machinery for noticing their incoherence, caring about it, and making changes.&lt;/p&gt;

&lt;p&gt;Or put another way, coherence doesn’t exclude overt behaviors alone, but does exclude combinations of preferences, and preferences beget behaviors. This changes how specific creatures behave, even if it doesn’t entirely rule out any behavior ever being correct for some creature, somewhere. &lt;/p&gt;

&lt;p&gt;That is, the coherence theorems may change what behavior is &lt;em&gt;likely&lt;/em&gt; to appear amongst creatures with preferences. &lt;/p&gt;

&lt;!-- wp:heading {&quot;level&quot;:3} --&gt;
&lt;h3&gt;&lt;strong&gt;Reform for coherence probably makes a thing more goal-directed&lt;/strong&gt;&lt;strong&gt;&lt;sub&gt;Rohin&lt;/sub&gt;&lt;/strong&gt;&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;p&gt;Ok, but moving toward coherence might sound totally innocuous, since, per Rohin’s argument, coherence includes all sorts of things, such as absolutely any sequence of behavior. &lt;/p&gt;

&lt;p&gt;But the relevant question is again whether a coherence-increasing reform process is likely to result in some kinds of behavior over others, probabilistically.&lt;/p&gt;

&lt;p&gt;This is partly a practical question—what kind of reform process is it? Where a creature ends up depends not just on what it incoherently ‘prefers’, but on what kinds of things its so-called ‘preferences’ are at all&lt;sup id=&quot;fnref:11&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;, and what mechanisms detect problems, and how problems are resolved.&lt;/p&gt;

&lt;p&gt;My guess is that there are also things we can say in general. It’s is too big a topic to investigate properly here, but some initially plausible hypotheses about a wide range of coherence-reform processes:&lt;/p&gt;

&lt;!-- wp:list {&quot;ordered&quot;:true} --&gt;
&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Coherence-reformed entities will tend to end up looking similar to their starting point but less conflicted&lt;br /&gt;&lt;/strong&gt;For instance, if a creature starts out being indifferent to buying red balls when they cost between ten and fifteen blue balls, it is more likely to end up treating red balls as exactly 12x the value of blue balls than it is to end up very much wanting the sequence where it takes the blue ball option, then the red ball option, then blue, red, red, blue, red. Or wanting red squares. Or wanting to ride a dolphin.&lt;br /&gt;&lt;br /&gt;(I agree that if a creature starts out valuing Tuesday-red balls at fifteen blue balls and yet all other red balls at ten blue balls, then it faces no obvious pressure from within to become ‘coherent’, since it is not incoherent.)&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;More coherent strategies are systematically less wasteful, and waste inhibits goal-direction&lt;sub&gt;Rohin&lt;/sub&gt;, which means more coherent strategies are more forcefully goal-directed&lt;sub&gt;Rohin&lt;/sub&gt; on average&lt;/strong&gt;&lt;br /&gt;In general, if you are sometimes a force for A and sometimes a force against A, then you are not moving the world with respect to A as forcefully as you would be if you picked one or the other. Two people intermittently changing who is in the driving seat, who want to go to different places, will not cover distance in any direction as effectively as either one of them. A company that cycles through three CEOs with different evaluations of everything will—even if they don’t actively scheme to thwart one another—tend to waste a lot of effort bringing in and out different policies and efforts (e.g. one week trying to expand into textiles, the next week trying to cut everything not involved in the central business).&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:list {&quot;ordered&quot;:true,&quot;start&quot;:3} --&gt;
&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;&lt;strong&gt;Combining points 1 and 2 above, as entities become more coherent, they generally become more goal-directed&lt;/strong&gt;&lt;strong&gt;&lt;sub&gt;Rohin&lt;/sub&gt;&lt;/strong&gt;&lt;strong&gt;. &lt;/strong&gt;As opposed to, for instance, becoming more goal-directed&lt;sub&gt;Rohin&lt;/sub&gt; &lt;em&gt;on average,&lt;/em&gt; but individual agents being about as likely to become worse as better as they are reformed. Consider: a creature that values red balls at 12x blue balls is very similar to one that values them inconsistently, except a little less wasteful. So it is probably similar but more goal-directed&lt;sub&gt;Rohin&lt;/sub&gt;. Whereas it’s fairly unclear how goal-directed&lt;sub&gt;Rohin &lt;/sub&gt;a creature that wants to ride a dolphin is compared to one that wanted red balls inconsistently much. In a world with lots of balls and no possible access to dolphins, it might be much less goal-directed&lt;sub&gt;Rohin&lt;/sub&gt;, in spite of its greater coherence.&amp;nbsp;&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:list {&quot;ordered&quot;:true,&quot;start&quot;:4} --&gt;
&lt;ol start=&quot;4&quot;&gt;&lt;li&gt;&lt;strong&gt;Coherence-increasing processes rarely lead to non-goal-directed&lt;/strong&gt;&lt;strong&gt;&lt;sub&gt;Rohin&lt;/sub&gt;&lt;/strong&gt;&lt;strong&gt; agents—like the one that twitches on the ground&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;In the abstract, few starting points and coherence-motivated reform processes will lead to an agent with the goal of carrying out a specific convoluted moment-indexed policy without regard for consequence, like Rohin’s twitching agent, or to valuing the sequence of history-action pairs that will happen anyway, or to being indifferent to everything. And these outcomes will be even less likely in practice, where AI systems with anything like preferences probably start out caring about much more normal things, such as money and points and clicks, so will probably land at a more consistent and shrewd version of that, if 1 is true. (Which is not to say that you couldn’t intentionally create such a creature.)&lt;br /&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;p&gt;These hypotheses suggest to me that the changes in behavior brought about by coherence forces favor moving toward goal-directedness&lt;sub&gt;Rohin&lt;/sub&gt;, and therefore at least weakly toward risk.&lt;/p&gt;

&lt;!-- wp:heading --&gt;
&lt;h2&gt;Does this mean advanced AI will be goal-directed&lt;sub&gt;Rohin&lt;/sub&gt;?&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;

&lt;p&gt;Together, this does not imply that advanced AI will tend to be goal-directed&lt;sub&gt;Rohin&lt;/sub&gt;. We don’t know how strong such forces are. Evidently not so strong that humans&lt;sup id=&quot;fnref:12&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;, or our other artifacts, are whipped into coherence in mere hundreds of thousands of years&lt;sup id=&quot;fnref:13&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;. If a creature doesn’t have anything like preferences (beyond a tendency to behave certain ways), then coherence arguments don’t obviously even apply to it (though discrepancies between the creature’s behavior and its makers’ preferences probably produce an analogous force&lt;sup id=&quot;fnref:14&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:14&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt; and competitive pressures probably produce a similar force for coherence in valuing resources instrumental to survival). Coherence arguments mark out an aspect of the incentive landscape, but to say that there is an incentive for something, all things equal, is not to say that it will happen.&lt;/p&gt;

&lt;!-- wp:heading --&gt;
&lt;h2&gt;In sum&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;

&lt;p&gt;1) Even though any behavior could be coherent in principle, if it is not coherent in combination with an entity’s internal state, then coherence arguments point to a real force for different (more coherent) behavior.&lt;/p&gt;

&lt;p&gt;2) My guess is that this force for coherent behavior is also a force for goal-directed behavior. This isn’t clear, but seems likely, and also isn’t undermined by Rohin’s argument, as seems commonly believed.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;
  &lt;p&gt;&lt;a href=&quot;http://aiimpacts.org/wp-content/uploads/2021/03/Two_dogs_attached_to_the_same_leash_are_pulling_in_different_Wellcome_V0021880-scaled.jpeg&quot;&gt;&lt;img src=&quot;http://aiimpacts.org/wp-content/uploads/2021/03/Two_dogs_attached_to_the_same_leash_are_pulling_in_different_Wellcome_V0021880-1024x804.jpeg&quot; alt=&quot;&quot; class=&quot;wp-image-2877&quot; width=&quot;768&quot; height=&quot;603&quot; /&gt;&lt;/a&gt;&amp;lt;figcaption&amp;gt;&lt;em&gt;Two dogs attached to the same leash are pulling in different directions&lt;/em&gt;. &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Two_dogs_attached_to_the_same_leash_are_pulling_in_different_Wellcome_V0021880.jpg&quot;&gt;Etching by J. Fyt&lt;/a&gt;, &lt;a href=&quot;https://catalogue.wellcomelibrary.org/record=b1199389&quot;&gt;1642&lt;/a&gt;&amp;lt;/figcaption&amp;gt;&amp;lt;/figure&amp;gt;&lt;/p&gt;

  &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

  &lt;!-- wp:separator --&gt;
  &lt;hr /&gt;

  &lt;!-- /wp:separator --&gt;

  &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;/figure&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For instance, Richard Ngo agrees &lt;a href=&quot;https://www.lesswrong.com/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-the-real-world-is-an-incoherent&quot;&gt;here&lt;/a&gt;, and Eric Drexler makes a related argument &lt;a href=&quot;https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf&quot;&gt;here&lt;/a&gt;, section 6.4. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Something something &lt;a href=&quot;https://arbital.com/p/expected_utility_formalism/?l=7hh&quot;&gt;This&lt;/a&gt; has more on these arguments. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I haven’t read all of this, and don’t yet see watertight versions of these arguments, but this is not the time I’m going to get into that. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Assuming being ‘more coherent’ is meaningful and better than being ‘less coherent’, granting that one is not coherent, which sounds plausible, but which I haven’t got into. One argument against is that if you are incoherent at all, then it looks to me like you can logically evaluate any bundle of things at any price. Which would seem to make all incoherences identical—much like how all logical contradictions equivalently lead to every belief. However this seems unlikely to predict well how creatures behave in practice if they have an incoherent preferences. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This isn’t quite right, since ‘goal’ suggests one outcome that is being pursued ahead of all others, whereas EU-maximizing implies that all possible outcomes have an ordering, and you care about getting higher ones in general, not just the top one above all others, but this doesn’t seem like a particularly relevant distinction here. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I am not sold on this usage myself for ‘goal-directed’—there is an appeal to using that phrase for ‘pursues goals’ in its most basic sense, but I am also tentatively in favor of having as many concepts as possible. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;It seems perhaps misleading to call these ‘preferences’, if they are incoherent, and so do not together implicate orderings of outcomes being better than one another. If a creature is not coherent, what are even the objects of its decision calculus? I am inclined to think in terms of ‘decision criteria’, e.g. ‘given X and Y, choose X’, and ‘given Y and Z, choose Y’, which don’t necessarily imply anything about ‘given Z and X, choose …’, but I haven’t thought about this much, and it seems like a technical detail of the creature in question. Whatever they are though, if the creature has behavior, then there are internal dynamics that produce it. When exactly an aspect of these should be considered a ‘preference’ for the sake of this argument isn’t entirely clear to me, but would seem to depend on something like whether it tends to produce actions favoring certain outcomes over other outcomes across a range of circumstances (similar to the unclear definition of ‘agent’). &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The ‘right’ way to correct your own incoherent preferences seems complicated and not obviously well defined or existent, and perhaps there is not much more to say than that what you do will depend on your design. It’s also not clear to me that a genuinely incoherent creature should necessarily want to reform, by its own lights, but that is a question for another time—here I’m assuming that the coherence arguments do have this implication that seems commonly attributed to them. My guess is that in practice, such creatures often do want to reform, and exactly how they do it doesn’t matter for my argument here. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I’m describing the force potentially felt by Assistant-Bot itself, but to the extent that its makers, or users also have preferences for money over nothing, and wish to use Assistant-Bot, and can alter it, they would seem to have similar incentives to mitigate its self-defeating behavior. &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The creature’s ‘preferences’ can’t be in terms of consistent numerical values assigned to everything, because those would be consistent. So what are they? For instance, one might imagine that they are pairwise comparisons between some kind of things (which can include ‘A &amp;gt; B’ and ‘B &amp;gt; C’ and ‘C &amp;gt; A’), or that they are a set of ‘situation—&amp;gt;action’ mappings, or they are a noisy ‘options—&amp;gt;feelings’ mapping combined with a set of deontological constraints over actions and feelings (‘choose things you feel better about, except don’t choose things out of selfishness, except when you feel more than 50% scared…’, etc. &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For instance, humans are insistently incoherent on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Allais_paradox&quot;&gt;Allais paradox&lt;/a&gt;. &lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:13&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;That seems pretty interesting—but note that well-designed computers have been known to do things that took humans participating in biological and cultural evolution hundreds of thousands of years before, so inference here not straightforward, and the forces of coherence depend on the costs of reform, which depend on the machinery for it. Also, we don’t know what other forces were in play—there might even have been forces for apparent incoherence, e.g. insofar as hypocrisy can benefit social animals, and dishonesty is complicated (&lt;a href=&quot;https://www.amazon.com/Elephant-Brain-Hidden-Motives-Everyday/dp/0190495995&quot;&gt;The Elephant in the Brain&lt;/a&gt; discusses such ideas). &lt;a href=&quot;#fnref:13&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:14&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For instance, the coherent creature that evaluates red balls differently on Tuesday and Wednesday might be in conflict with its creators, if they have a more consistent red ball evaluation, giving them reason to reform it. You might class this under the question, ‘what kinds of advanced AI will people want?’, but the reason for it is very similar to the reasons for internal pressure for coherence. If you refuse to pay $13 for a red ball, and your AI then goes out and buys you one for $15 because it is Tuesday, then the pair of you together could have done better. &lt;a href=&quot;#fnref:14&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Katja Grace</name></author><category term="ai" /><category term="airisk" /><summary type="html"></summary></entry><entry><title type="html">Animal faces</title><link href="http://localhost:4000/2021/03/10/animal-face.html" rel="alternate" type="text/html" title="Animal faces" /><published>2021-03-10T23:36:00-08:00</published><updated>2021-03-10T23:36:00-08:00</updated><id>http://localhost:4000/2021/03/10/animal-face</id><content type="html" xml:base="http://localhost:4000/2021/03/10/animal-face.html">&lt;p&gt;&lt;em&gt;[Epistemic status: not reflective of the forefront of human undersetanding, or human understanding after any research at all. Animal pictures with speculative questions.]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Do the facial expressions of animals mean anything like what I’m inclined to take them to mean?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/5a8099e3-3df2-4e0a-b4ae-a550d072da51&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/ja-san-miguel-_-QQuvAwQ-0-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;ja-san-miguel-_-QQuvAwQ-0-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/e8fde0f1-d09c-421e-9d37-7a99ed41594e&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/charles-deluvio-Mv9hjnEUHR4-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;charles-deluvio-Mv9hjnEUHR4-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/19cf796a-926a-46c8-9353-4b1e753ba877&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/amy-humphries-kACs144foYM-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;amy-humphries-kACs144foYM-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/0c78f7e4-d7bb-4d01-bc0d-0057fcce3fff&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/aureanne-mailhiot-N5AHY2dk1mU-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;aureanne-mailhiot-N5AHY2dk1mU-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/27447041-f7cd-432f-851b-eb86253ea6e6&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/sara-rolin-n4RJtjHhb9A-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;sara-rolin-n4RJtjHhb9A-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/d8866103-f05c-416b-9954-15cf03df408c&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/sophia-muller-5t9T6hQ2Cn0-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;sophia-muller-5t9T6hQ2Cn0-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/584579c4-4862-4a8c-8f44-4734638664f9&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/sophie-elvis-xJQA_-9lwtM-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;sophie-elvis-xJQA_-9lwtM-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/57c9e771-7969-402d-804e-498475118b2a&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/jamie-street-UtrE5DcgEyg-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;jamie-street-UtrE5DcgEyg-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/34994826-6884-4000-acec-b3d5157d5b74&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/jaime-dantas-djeApmC4Aco-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;jaime-dantas-djeApmC4Aco-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/de5832b9-6564-4d41-9562-7700005e7db2&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/peggy-anke-8eCRIYaKME8-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;peggy-anke-8eCRIYaKME8-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/1afde4bc-f652-4bf6-a9a5-c9b7d5ce744f&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/karina-rullo-tSvMp2iAwPs-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;karina-rullo-tSvMp2iAwPs-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/efb917d5-8b7f-43ae-bcf8-1ced6cc5cfcf&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/rajesh-raj-r5fY0n6rYjA-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;rajesh-raj-r5fY0n6rYjA-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/0fb1f1af-44cd-4208-ba99-a521ba9e92ff&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/vivek-doshi-w-0_LjFFY0E-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;vivek-doshi-w-0_LjFFY0E-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/2442a0d7-0a52-447f-9dbc-6b5c91d6de68&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/sand-crain-8jk64drEMcw-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;sand-crain-8jk64drEMcw-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;On the one hand, we can’t have been the first creatures to have the innovation of facial expressions. And the same kind of animal is often seen to have different facial expressions, which has got to happen for a reason. But on the other hand, the whole race of dolphins just looks vaguely elated, and surely that can’t track the collective personality of the species.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/a197818e-6e8f-42a2-a898-40567c77fa15&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/louan-garcia-Xsq5ie5f498-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;louan-garcia-Xsq5ie5f498-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/2d917a92-3462-4569-b133-06c3e5c2a885&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/adam-berkecz-K6kZKJOmZrk-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;adam-berkecz-K6kZKJOmZrk-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/8b7ae3ec-0e05-4df6-a7bc-44991bebcfca&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/louan-garcia-fcZU_ZkQlzI-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;louan-garcia-fcZU_ZkQlzI-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/e149c471-6b97-45ea-9693-dc8449e677a3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/ranae-smith-ixbyBwwv6rk-unsplash.jpg&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;ranae-smith-ixbyBwwv6rk-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I suppose the answer is that many animals do have meaningful facial expressions, and even that broadly ‘smiling’ matches our broadly ‘smiling’, but that for the most part, they don’t match in their subtleties, and species-wide features don’t track anything.&lt;/p&gt;

&lt;p&gt;That all sounds very reasonable, but can I really look at this creature and not see it as happy?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/01679e7c-aef2-4885-a89b-fce671c1430d&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/silviu-georgescu-1_OD46MjRUQ-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;silviu-georgescu-1_OD46MjRUQ-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Perhaps I can’t, but my hard-to-quash beliefs are just wrong. How tragic in that case to have a visual right on one’s face—standing in the way of our ever looking each other in the eye and understanding at all, even if we were to spend a lifetime together.&lt;/p&gt;

&lt;p&gt;And seals—they seem to look extremely contented more than half the time, but not all of the time. So it’s not that they are automatically contented-looking. What does it mean?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/13259e41-b07f-4be9-ad02-17fa38fd8275&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/yuriy-rzhemovskiy-G-MyqQy8v_4-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;yuriy-rzhemovskiy-G-MyqQy8v_4-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/db5e4460-24df-44ad-a71e-815ecff07ffb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/jene-yeo-bkUkoggDMf0-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;jene-yeo-bkUkoggDMf0-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/e286ddd2-06cd-42b8-a042-0e694ed63676&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/fred-heap-gRpu05nL4so-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;fred-heap-gRpu05nL4so-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/2763b2f7-0055-4055-a649-172e232799eb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/adam-king-BJRkaK_nUcI-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;adam-king-BJRkaK_nUcI-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/592da8d9-3011-4cc5-9319-0bae29e992b7&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/rani-d-haese-J8BvOxI_vrY-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;rani-d-haese-J8BvOxI_vrY-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/8f11f9ea-04a5-47b7-bf80-026cd96a05e2&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/amy-asher-giZJHm2m9yY-unsplash_(1).jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;amy-asher-giZJHm2m9yY-unsplash_(1)&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/d240eabc-8815-495a-8051-9de08ac3542d&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/zdenek-machacek-uRQlCmfOCRg-unsplash_(2).jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;zdenek-machacek-uRQlCmfOCRg-unsplash_(2)&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/9807bc9b-5a33-4639-a4a1-eb8a354c2a03&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/sevak-oJbF_4qfrjE-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;sevak-oJbF_4qfrjE-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/40cdec21-29bb-4ec9-9aee-689e8f94663f&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/roundseal5.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;roundseal5&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Are &lt;em&gt;these&lt;/em&gt; creatures not contented?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/8b2214a0-6e8c-4784-8752-02c65f9b0716&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/john-dean-0vaHhFf16TQ-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;john-dean-0vaHhFf16TQ-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Do the facial expressions of humans mean anything like what I’m inclined to take them to mean?&lt;/p&gt;

&lt;p&gt;I tend to read slightly different smiles with a lot of flavor and significance, for instance.&lt;/p&gt;

&lt;p&gt;But when I look in the mirror, my face doesn’t look like what I mean it to look like. At least not reliably and with fine specifically. I’m way better than chance on ‘smile’ when intending to smile, but ‘friendly smile’ can easily end up as ‘ridiculous to the point of questioning how anyone takes me seriously’ smile, or ‘somewhat intimidating smile’.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/e74c83b8-70e5-45af-9510-f8a8726b9ba3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/IMG_20191128_171917.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;IMG_20191128_171917&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And people’s default faces have features that my brain interprets as expression—if one person looks more grouchy or judgmental or open or lighthearted across the board than another, does that mean something? My automatic interpretation of things thinks so, but it seems very questionable.&lt;/p&gt;

&lt;p&gt;~&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.photobucket.com/u/katjasgrace/p/0c47e4f8-25a5-4887-8892-1dfb4d1f0f32&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/anchor-lee-ROU5VRrOuq4-unsplash.jpg?width=1920&amp;amp;height=1080&amp;amp;fit=bounds&quot; width=&quot;500&quot; class=&quot;center&quot; alt=&quot;anchor-lee-ROU5VRrOuq4-unsplash&quot; /&gt;&lt;/a&gt;&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="elephantseal" /><summary type="html">[Epistemic status: not reflective of the forefront of human undersetanding, or human understanding after any research at all. Animal pictures with speculative questions.] Do the facial expressions of animals mean anything like what I’m inclined to take them to mean? On the one hand, we can’t have been the first creatures to have the innovation of facial expressions. And the same kind of animal is often seen to have different facial expressions, which has got to happen for a reason. But on the other hand, the whole race of dolphins just looks vaguely elated, and surely that can’t track the collective personality of the species. I suppose the answer is that many animals do have meaningful facial expressions, and even that broadly ‘smiling’ matches our broadly ‘smiling’, but that for the most part, they don’t match in their subtleties, and species-wide features don’t track anything. That all sounds very reasonable, but can I really look at this creature and not see it as happy? Perhaps I can’t, but my hard-to-quash beliefs are just wrong. How tragic in that case to have a visual right on one’s face—standing in the way of our ever looking each other in the eye and understanding at all, even if we were to spend a lifetime together. And seals—they seem to look extremely contented more than half the time, but not all of the time. So it’s not that they are automatically contented-looking. What does it mean? Are these creatures not contented? Do the facial expressions of humans mean anything like what I’m inclined to take them to mean? I tend to read slightly different smiles with a lot of flavor and significance, for instance. But when I look in the mirror, my face doesn’t look like what I mean it to look like. At least not reliably and with fine specifically. I’m way better than chance on ‘smile’ when intending to smile, but ‘friendly smile’ can easily end up as ‘ridiculous to the point of questioning how anyone takes me seriously’ smile, or ‘somewhat intimidating smile’. And people’s default faces have features that my brain interprets as expression—if one person looks more grouchy or judgmental or open or lighthearted across the board than another, does that mean something? My automatic interpretation of things thinks so, but it seems very questionable. ~</summary></entry><entry><title type="html">Quarantine variety</title><link href="http://localhost:4000/2021/03/09/the-variety-never-went-away.html" rel="alternate" type="text/html" title="Quarantine variety" /><published>2021-03-09T23:34:00-08:00</published><updated>2021-03-09T23:34:00-08:00</updated><id>http://localhost:4000/2021/03/09/the-variety-never-went-away</id><content type="html" xml:base="http://localhost:4000/2021/03/09/the-variety-never-went-away.html">&lt;p&gt;Among people sheltering from covid, I think there is a common thought that being stuck in your home for a year begets a certain sameyness, that it will be nice to be done with.&lt;/p&gt;

&lt;p&gt;It’s interesting to me to remember that big chunk of the variety that is missing in life comes from regular encounters with other people, and their mind-blowing tendencies to do and think differently to me, and jump to different conclusions, and not even know what I’m talking about when I mention the most basic of basic assumptions.&lt;/p&gt;

&lt;p&gt;And to remember that many of those people are stuck in similar houses, similarly wishing for variety, but being somewhat tired of a whole different set of behaviors and thoughts and framings and assumptions.&lt;/p&gt;

&lt;p&gt;Which means that the variety is not fully out of safe reach in the way that, say, a big lick-a-stranger party might be. At least some of it is just informationally inaccessible, like finding the correct answer to a hard math problem. If I could somehow spend a day living like a person stuck in their house across the street lives, I would see all kinds of new things. My home itself—especially with its connection to the internet and Amazon—is capable of vastly more variety than I typically see.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="lifeadvice" /><category term="worldlypositions" /><summary type="html">Among people sheltering from covid, I think there is a common thought that being stuck in your home for a year begets a certain sameyness, that it will be nice to be done with. It’s interesting to me to remember that big chunk of the variety that is missing in life comes from regular encounters with other people, and their mind-blowing tendencies to do and think differently to me, and jump to different conclusions, and not even know what I’m talking about when I mention the most basic of basic assumptions. And to remember that many of those people are stuck in similar houses, similarly wishing for variety, but being somewhat tired of a whole different set of behaviors and thoughts and framings and assumptions. Which means that the variety is not fully out of safe reach in the way that, say, a big lick-a-stranger party might be. At least some of it is just informationally inaccessible, like finding the correct answer to a hard math problem. If I could somehow spend a day living like a person stuck in their house across the street lives, I would see all kinds of new things. My home itself—especially with its connection to the internet and Amazon—is capable of vastly more variety than I typically see.</summary></entry><entry><title type="html">Why does Applied Divinity Studies think EA hasn’t grown since 2015?</title><link href="http://localhost:4000/2021/03/09/why-does-ads-think-ea-hasnt-grown.html" rel="alternate" type="text/html" title="Why does Applied Divinity Studies think EA hasn't grown since 2015?" /><published>2021-03-09T00:32:00-08:00</published><updated>2021-03-09T00:32:00-08:00</updated><id>http://localhost:4000/2021/03/09/why-does-ads-think-ea-hasnt-grown</id><content type="html" xml:base="http://localhost:4000/2021/03/09/why-does-ads-think-ea-hasnt-grown.html">&lt;p&gt;&lt;a href=&quot;https://applieddivinitystudies.com/&quot;&gt;Applied Divinity Studies&lt;/a&gt; seeks to explain why the EA community hasn’t grown since 2015. The observations they initially call the EA community not having grown are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;GiveWell money moved increased a lot in 2015, then grew only slightly since then.&lt;/li&gt;
  &lt;li&gt;Open Phil (I guess money allocated) hasn’t increased since 2017&lt;/li&gt;
  &lt;li&gt;Google Trends “Effective Altruism” ‘grows quickly starting in 2013, peaks in 2017, then falls back down to around 2015 levels’.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Looking at the graph they illustrate with, 1) is because GiveWell started receiving a large chunk of money from OpenPhil in 2015, and that chunk remained around the same over the years, while the money not from Open Phil has grown.&lt;/p&gt;

&lt;p&gt;So 1) and 2) are both the observation, “Open Phil has not scaled up its money-moving in recent years”.&lt;/p&gt;

&lt;p&gt;I’m confused about how this observation seems suggestive about the size of the EA community. Open Phil is not a community small-donations collector. You can’t even donate to Open Phil. It is mainly moving Good Ventures’ money, i.e. the money of a single couple: Dustin Moskovitz and Cari Tuna.&lt;!--ex--&gt;&lt;/p&gt;

&lt;p&gt;One way that I could imagine Open Phil’s spending saying something about the size of the EA community is that the community might provide funding opportunities for Open Phil, so that its growth was reflected in Open Phil’s spending. But this would require EA growth at a scale that produced large enough funding opportunities, that met Open Phil’s specific criteria, to show up amidst hundreds of millions of dollars of annual grant-making. I think this at least requires argument.&lt;/p&gt;

&lt;p&gt;I’m further confused when in trying to explain the purported end of growth, ADS says, ‘One possibility is that there was not a strange hidden cause behind widespread stagnation. It’s just that funding slowed down, and so everything else slowed down with it’, then go on to explore the possibility that funding from Open Phil/Good Ventures has slowed down in line with this ‘widespread’ stagnation (in different aspects of Open Phil and one Google Trends result). They find that indeed it has.&lt;/p&gt;

&lt;p&gt;There &lt;em&gt;is&lt;/em&gt; a strange hidden cause! Which is the underlying structural relationship between Open Phil and a combination of Open Phil twice and another thing.&lt;/p&gt;

&lt;p&gt;(In fairness, ‘widespread’ might at that point in the post also include the stagnation since 2010 and 2013 respectively of Google searches for popular Rationalist blogs, &lt;em&gt;Less Wrong&lt;/em&gt;, and &lt;em&gt;Slate Star Codex&lt;/em&gt;. But stagnations that old seem unlikely to be symptomatic of an EA post-2015 stagnation, so probably the author didn’t intend to include them in it.)&lt;/p&gt;

&lt;p&gt;While it’s not relevant to my above confusion, I should also note that even if Open Phil funding of Givewell was a better metric of the size of EA than I understand, if the data for 1) reached 2020, it would probably tell a different story: Open Phil’s allocation to GiveWell charities grew a lot— to &lt;a href=&quot;https://www.openphilanthropy.org/blog/2020-allocation-givewell-top-charities&quot;&gt;$100 million&lt;/a&gt;, up from $54 million the previous year (according to the figure). So there isn’t even a stagnation of Open Phil allocation to GiveWell to be confused by.&lt;/p&gt;

&lt;p&gt;I agree that 3) is evidence of the EA community receiving less attention since 2017 (though not 2015). But mild evidence, because searching for “effective altruism” on Google is probably not that closely associated with commitment to EA, as ADS notes, and because Google Trends is just &lt;a href=&quot;https://slatestarcodex.com/2018/11/01/working-with-google-trends/&quot;&gt;pretty complicated to make sense of&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I think it’s worth finding better evidence, before we get to speculating about such things. For instance, &lt;a href=&quot;https://forum.effectivealtruism.org/posts/MBJvDDw2sFGkFCA29/is-ea-growing-ea-growth-metrics-for-2018&quot;&gt;this&lt;/a&gt; 2018 post by Peter Hurford looks at about thirty metrics, only two of which peaked in or before 2015. I haven’t read that post, but my impressions from skimming it a bit seem like a more informative start than a single Google trend.&lt;/p&gt;

&lt;p&gt;In the process of coming up with explanations, ADS does also point to further relevant trends: the aforementioned flatness of two major rationalist blog name Google Trends, the aforementioned further measure of Open Phil activity via Good Ventures which looks like the other Open Phil activity measure, and the trend of Giving What We Can membership.&lt;/p&gt;

&lt;p&gt;Giving What We Can membership seems potentially very informative. Does it support the view that EA hasn’t grown since 2015? No—GWWC membership has apparently grown by about 7x since 2015. ADS correctly notes that this is a measure of “cumulative interest” whereas how many people Google a term is a measure of “active interest”. That is, the total GWWC members on a day are the sum of everyone who has ever wanted to become a GWWC member, whereas searches for “join GWWC”, say, would be a measure of how many are wanting to do that right now. But ADS’s claim was that EA hasn’t grown. It wasn’t that EA’s rate of growth hasn’t grown. And the size of the EA community is about cumulative interest, to the extent interest persists. (For some activities, you might expect them to happen again every day that you remain involved, but neither joining a society for giving away 10% of your money, nor Googling “effective altruism” is one.) Or skipping all this complication about cumulativeness, the size of EA is very much the same kind of thing as the size of GWWC membership.&lt;/p&gt;

&lt;p&gt;In sum, I don’t have a clear picture of how EA has grown since 2015, but as far as I can tell, the main relevant evidence that this post presents is that Google Trends “effective altruism” is flattish, Giving What We Can membership is growing (though not exponentially), and non-Open-Phil GiveWell money moved is growing. (Also, including information not in the post, it seems that Open Phil money moved to GiveWell was flat for a while then grew a lot in 2020, but it remains unclear to me how this is relevant anyway.) I’m inclined to interpret this evidence as mildly supporting ‘EA has grown since 2015’, but it doesn’t seem like much evidence either way. I think we should at least hold off on taking for granted that EA &lt;em&gt;hasn’t&lt;/em&gt; grown since 2015 and trying to explain why.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="effectivealtruism" /><category term="critique" /><summary type="html">Applied Divinity Studies seeks to explain why the EA community hasn’t grown since 2015. The observations they initially call the EA community not having grown are: GiveWell money moved increased a lot in 2015, then grew only slightly since then. Open Phil (I guess money allocated) hasn’t increased since 2017 Google Trends “Effective Altruism” ‘grows quickly starting in 2013, peaks in 2017, then falls back down to around 2015 levels’. Looking at the graph they illustrate with, 1) is because GiveWell started receiving a large chunk of money from OpenPhil in 2015, and that chunk remained around the same over the years, while the money not from Open Phil has grown. So 1) and 2) are both the observation, “Open Phil has not scaled up its money-moving in recent years”. I’m confused about how this observation seems suggestive about the size of the EA community. Open Phil is not a community small-donations collector. You can’t even donate to Open Phil. It is mainly moving Good Ventures’ money, i.e. the money of a single couple: Dustin Moskovitz and Cari Tuna.</summary></entry><entry><title type="html">Sleep math: red clay blue clay</title><link href="http://localhost:4000/2021/03/07/sleep-math-red-clay-blue-clay.html" rel="alternate" type="text/html" title="Sleep math: red clay blue clay" /><published>2021-03-07T15:17:00-08:00</published><updated>2021-03-07T15:17:00-08:00</updated><id>http://localhost:4000/2021/03/07/sleep-math-red-clay-blue-clay</id><content type="html" xml:base="http://localhost:4000/2021/03/07/sleep-math-red-clay-blue-clay.html">&lt;p&gt;To me, going to bed often feels more like a tiresome deprivation from life than a welcome rest, or a painless detour through oblivion to morning. When I lack patience for it, I like to think about math puzzles. Other purposeful lines of thought keep me awake or lose me, but math leads me happily into a world of abstraction, from which the trip to dreamland comes naturally.&lt;/p&gt;

&lt;p&gt;(It doesn’t always work. Once I was still awake after seemingly solving two Putnam problems, which is about as well as I did in the actual Putnam contest.)&lt;/p&gt;

&lt;p&gt;A good puzzle for this purpose should be easy to play with in one’s head. For me, that means it should be amenable to simple visualization, and shouldn’t have the kind of description you have to look at multiple times. A handful of blobs is a great subject matter; an infinite arrangement of algebra is not.&lt;/p&gt;

&lt;p&gt;Recently I’ve been going to sleep thinking about the following puzzle. I got several nights of agreeable sleep out of it, but now I think I have a good solution, which I’ll probably post in future.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Suppose that you have 1 kg of red clay that is 100 degrees and 1 kg of blue clay that is 0 degrees. You can divide and recombine clay freely. If two pieces of clay come into contact, temperature immediately equilibrates—if you put the 1kg of red clay next to 0.5 kg of blue clay, all the clay will immediately become 66 degrees. Other than that the temperature of the clay doesn’t change (i.e. no exchange with air or your hands, no radiation, etc.). Your goal is to end up with all of the blue clay in a single clump that is as hot as possible. How hot can you make it? (Equivalently: how cold can you make the red clay?)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;HT Chelsea Voss via Paul Christiano&lt;/p&gt;

&lt;!--ex--&gt;

&lt;p&gt;Clue: it’s more than 50 degrees.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="advice" /><category term="puzzle" /><summary type="html">To me, going to bed often feels more like a tiresome deprivation from life than a welcome rest, or a painless detour through oblivion to morning. When I lack patience for it, I like to think about math puzzles. Other purposeful lines of thought keep me awake or lose me, but math leads me happily into a world of abstraction, from which the trip to dreamland comes naturally. (It doesn’t always work. Once I was still awake after seemingly solving two Putnam problems, which is about as well as I did in the actual Putnam contest.) A good puzzle for this purpose should be easy to play with in one’s head. For me, that means it should be amenable to simple visualization, and shouldn’t have the kind of description you have to look at multiple times. A handful of blobs is a great subject matter; an infinite arrangement of algebra is not. Recently I’ve been going to sleep thinking about the following puzzle. I got several nights of agreeable sleep out of it, but now I think I have a good solution, which I’ll probably post in future. Suppose that you have 1 kg of red clay that is 100 degrees and 1 kg of blue clay that is 0 degrees. You can divide and recombine clay freely. If two pieces of clay come into contact, temperature immediately equilibrates—if you put the 1kg of red clay next to 0.5 kg of blue clay, all the clay will immediately become 66 degrees. Other than that the temperature of the clay doesn’t change (i.e. no exchange with air or your hands, no radiation, etc.). Your goal is to end up with all of the blue clay in a single clump that is as hot as possible. How hot can you make it? (Equivalently: how cold can you make the red clay?) HT Chelsea Voss via Paul Christiano</summary></entry><entry><title type="html">Arrow grid game</title><link href="http://localhost:4000/2021/02/28/wholesome-fun-arrow-grid.html" rel="alternate" type="text/html" title="Arrow grid game" /><published>2021-02-28T02:26:00-08:00</published><updated>2021-02-28T02:26:00-08:00</updated><id>http://localhost:4000/2021/02/28/wholesome-fun-arrow-grid</id><content type="html" xml:base="http://localhost:4000/2021/02/28/wholesome-fun-arrow-grid.html">&lt;p&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/todos.jpg&quot; alt=&quot;todos&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s something I like about having different systems all the time. Apparently.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="advice" /><category term="mundane" /><category term="photo" /><summary type="html">There’s something I like about having different systems all the time. Apparently.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/todos.jpg" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/todos.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Remarks on morality, shuddering, judging, friendship and the law</title><link href="http://localhost:4000/2021/02/21/remarks-on-morality-etc.html" rel="alternate" type="text/html" title="Remarks on morality, shuddering, judging, friendship and the law" /><published>2021-02-21T11:02:00-08:00</published><updated>2021-02-21T11:02:00-08:00</updated><id>http://localhost:4000/2021/02/21/remarks-on-morality-etc</id><content type="html" xml:base="http://localhost:4000/2021/02/21/remarks-on-morality-etc.html">&lt;p&gt;I lately enjoyed listening to Julia Galef and Jonathan Haidt &lt;a href=&quot;http://rationallyspeakingpodcast.org/show/episode-252-understanding-moral-disagreements-jonathan-haidt.html&quot;&gt;discuss&lt;/a&gt; Haidt’s theorized palate of ‘moral foundations’—basic flavors of moral motivation—and how Julia should understand the ones that she doesn’t naturally feel.&lt;/p&gt;

&lt;p&gt;I was interested in Julia’s question of whether she was just using different words to those who for instance would say that incest or consensual cannibalism are ‘morally wrong’.&lt;/p&gt;

&lt;p&gt;She explained that her earlier guest, Michael Sandel, had asked whether she didn’t ‘cringe’ at the thought of consensual cannibalism, as if he thought that was equivalent to finding it immoral. Julia thought she could personally cringe without morally condemning a thing. She had read Megan McArdle similarly observing that ‘liberals’ claim that incest is moral, but meanwhile wouldn’t befriend someone who practices it, so do in fact morally object after all&lt;!--ex--&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. As if morality was defined by friendship preferences or inclination to pass judgment. Again Julia is perplexed. Do these people agree with her, and just call their facial movements and personal social preferences ‘morality’? Michael Sandel even explained that he didn’t want to &lt;em&gt;ban&lt;/em&gt; consensual cannibalism, he just thought it was immoral. Julia says, “I don’t even know what people are talking about when they talk about whether a thing is moral or immoral, if they’re not talking about wanting to use legal pressure to change people’s behavior”&lt;/p&gt;

&lt;p&gt;Here’s how I see morality and its relationship with shuddering, judging, declining friendship, and the law:&lt;/p&gt;

&lt;p&gt;Morality is about what is actually good.&lt;/p&gt;

&lt;p&gt;Shuddering, cringing and wincing are instinctive guesses about what is actually good. But much like your instinctive guesses about whether a spider is dangerous or whether you are driving well, they can err, and can even continue to err after you have seen the error. They are especially likely to err when the goodness of a thing is not readily available to your imagination. For instance, if you are a twelve-year-old hearing about sex, you might shudder, but you are missing a lot of the good bits of the picture. Or if you are thinking about a baby coming from a test-tube in 1977, you may be mostly focused on the test tube situation, and not on the century of happy life that the child will have, and the love and joy in the family that would have been emptier.&lt;/p&gt;

&lt;p&gt;Judging people is a response to your guesses about morality, but also to your guesses about other qualities like social status, effectiveness, personal aesthetics, and likelihood of being immoral in the future. You might judge people because they seem ridiculous, or slow, or aesthetically repellant, without thinking that the world would be objectively better off with a random different person existing instead. Or you might judge a person’s enjoyment of watching YouTube videos of plane crashes, without thinking that it is doing any harm.&lt;/p&gt;

&lt;p&gt;Choices about friendship seem to bring in everything that ‘judging’ does, but are also substantially complicated by preferences for actual interactions. For instance, you might have no moral problem with cooking, while dispreferring cooking-obsessed friends. Sometimes you might even prefer friends with traits that you judge—you might find continental philosophy ridiculous, while also enjoying that your friend Bob is so into it, since he explains bits of it in a language that you understand, and it’s fun to tease each other.&lt;/p&gt;

&lt;p&gt;The law is a complicated agreement between a collection of people for cooperating in pursuit of their different ends. It is very unclear that a thing being immoral (in the sense of ‘actually making the world worse’) means that the law should control it:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If the population mostly don’t know that the thing is bad, then it doesn’t seem like they should agree to be constrained from doing it. Similarly if they are in broad disagreement e.g. If you and 40% of people think A is moral, but 40% of people think it is immoral, and 20% are very unsure, then it may be that all of you think that it shouldn’t be legislated. The government doesn’t have access to the truth about it, and neither requiring or banning A would seem like a good compromise in the current state of knowledge, and either would probably have to be undemocratic.&lt;/li&gt;
  &lt;li&gt;Sometimes there would be harm in the law controlling things, or it wouldn’t be a good judge. e.g. maybe everyone agrees that it is immoral to cheat on your partner, or to not pay attention to signs that your teenage son is depressed because you are into playing a computer game, but most people wouldn’t want the law policing these things.&lt;/li&gt;
  &lt;li&gt;The law has certain actions available to it, and sometimes none is the best response to a moral concern. For instance, perhaps a person seems unfairly critical of others in a way that seems wrong to their friends. This is arguably better rectified with quiet words, sharp looks, or a reduction in invitations and endorsements, than by fines or sentences.&lt;/li&gt;
  &lt;li&gt;it can be better ultimately for people to make good choices through understanding the situation themselves over time, even while initially making errors, rather than having their good behavior forced or otherwise incentivized. For instance, it might be better for a person to learn compassion than to be trained by numb rote to act kindly, even if kindness is always better in the moment. Or, if the good behavior in question was enjoying the best experiences of art: you can’t actually force someone to experience a thing.&lt;/li&gt;
  &lt;li&gt;Coercion itself might be bad, or it might be fundamentally better for people to be free. (I’m being agnostic about what is or isn’t moral.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Julia and these other people aren’t using words differently, I interpret&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;: Michael thinks a world where people consensually eat one another is worse, whereas Julia thinks it isn’t—her own displeasure at it is a little bad, but this is presumably nothing compared to whatever satisfaction those involved enjoy. Michael and Julia both have an emotional response, and perhaps neither would be friends with the participants. But Julia doesn’t want to use the law because she thinks the act doesn’t make the world worse, whereas Michael may or may not want to use the law, but isn’t talking about that—he’s saying that it makes the world worse.&lt;/p&gt;

&lt;p&gt;&amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.bloomberg.com/opinion/articles/2015-08-06/liberals-can-t-admit-to-thinking-like-conservatives&quot;&gt;here&lt;/a&gt;, but gated, so just going by Julia’s recollection. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;To be clear, I’m going by Julia’s description in this discussion with Haidt, and haven’t e.g. listened to the discussion with Sandel. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Katja Grace</name></author><category term="ethics" /><category term="response" /><summary type="html">I lately enjoyed listening to Julia Galef and Jonathan Haidt discuss Haidt’s theorized palate of ‘moral foundations’—basic flavors of moral motivation—and how Julia should understand the ones that she doesn’t naturally feel. I was interested in Julia’s question of whether she was just using different words to those who for instance would say that incest or consensual cannibalism are ‘morally wrong’. She explained that her earlier guest, Michael Sandel, had asked whether she didn’t ‘cringe’ at the thought of consensual cannibalism, as if he thought that was equivalent to finding it immoral. Julia thought she could personally cringe without morally condemning a thing. She had read Megan McArdle similarly observing that ‘liberals’ claim that incest is moral, but meanwhile wouldn’t befriend someone who practices it, so do in fact morally object after all</summary></entry><entry><title type="html">Coffee trucks: a brilliant idea that someone should do?</title><link href="http://localhost:4000/2021/02/19/coffee-trucks.html" rel="alternate" type="text/html" title="Coffee trucks: a brilliant idea that someone should do?" /><published>2021-02-19T10:36:00-08:00</published><updated>2021-02-19T10:36:00-08:00</updated><id>http://localhost:4000/2021/02/19/coffee-trucks</id><content type="html" xml:base="http://localhost:4000/2021/02/19/coffee-trucks.html">&lt;p&gt;I sometimes wonder if the world should have coffee trucks, like ice cream trucks, roaming the street. Especially when half the population is working from home.&lt;/p&gt;

&lt;p&gt;Coffee seems ideal for this because:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;you can probably prepare it fresh in a briefly stopping vehicle,&lt;/li&gt;
  &lt;li&gt;you don’t need much variety,&lt;/li&gt;
  &lt;li&gt;people are often flexible about when they get it,&lt;/li&gt;
  &lt;li&gt;they often like to get it as a short break from work in which they bump into other people,&lt;/li&gt;
  &lt;li&gt;more than half of the US population consumes it at least once every day, so potential demand is radically higher than for most foodstuffs!&lt;/li&gt;
  &lt;li&gt;most people don’t have the means to make cafe-quality coffee in their home,&lt;/li&gt;
  &lt;li&gt;it doesn’t go bad easily&lt;/li&gt;
&lt;/ol&gt;

&lt;!--ex--&gt;
&lt;p&gt;There are clearly stationary coffee trucks, like food trucks. I think moving trucks may also exist or have existed ever, but I don’t think I’ve ever seen one, and it’s hard to find mention of them. &lt;a href=&quot;https://yaledailynews.com/blog/2017/12/01/the-jitter-bus-an-ice-cream-truck-for-adults/&quot;&gt;Here are&lt;/a&gt; some people who had the same idea, but since nobody was expecting a moving coffee bus, they got more traffic sitting still like a cafe. But surely people expecting your product to exist is an obstacle not unique to circulating coffee trucks. &lt;a href=&quot;https://www.reddit.com/r/Showerthoughts/comments/37socy/there_should_be_roaming_coffee_trucks_like_ice/&quot;&gt;Here&lt;/a&gt; is someone with the same idea, and someone who says there was one in a holiday park once, and some others who think they exist in Australia and Buffalo, but it sounds like they might be thinking of stationary coffee trucks.&lt;/p&gt;

&lt;p&gt;I’m not sure that it’s good for any kind of truck to exist if it makes noise all the time as it travels through neighborhoods, delighting some, but surely disrupting others. But I think that can be entirely resolved digitally: instead of the truck playing music, the service could have an app that plays music when truck is approaching, if you signed up for that. Then you touch the notification to turn off the music and at the same time report whether you want a coffee.&lt;/p&gt;

&lt;p&gt;Am I missing something?&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="idea" /><category term="whynot" /><summary type="html">I sometimes wonder if the world should have coffee trucks, like ice cream trucks, roaming the street. Especially when half the population is working from home. Coffee seems ideal for this because: you can probably prepare it fresh in a briefly stopping vehicle, you don’t need much variety, people are often flexible about when they get it, they often like to get it as a short break from work in which they bump into other people, more than half of the US population consumes it at least once every day, so potential demand is radically higher than for most foodstuffs! most people don’t have the means to make cafe-quality coffee in their home, it doesn’t go bad easily</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/filip-sablatura-o58cEDAnPB8-unsplash.jpg" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/filip-sablatura-o58cEDAnPB8-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Oliver Sipple</title><link href="http://localhost:4000/2021/02/18/oliver-sipple.html" rel="alternate" type="text/html" title="Oliver Sipple" /><published>2021-02-18T09:28:00-08:00</published><updated>2021-02-18T09:28:00-08:00</updated><id>http://localhost:4000/2021/02/18/oliver-sipple</id><content type="html" xml:base="http://localhost:4000/2021/02/18/oliver-sipple.html">&lt;p&gt;The other day I read Wikipedia arguably too much, and consequently came to know the story of Oliver Sipple. Here’s my summary of the story according to &lt;a href=&quot;https://en.wikipedia.org/wiki/Oliver_Sipple&quot;&gt;these&lt;/a&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Attempted_assassination_of_Gerald_Ford_in_San_Francisco&quot;&gt;two&lt;/a&gt; Wikipedia pages and &lt;a href=&quot;https://web.archive.org/web/20070831033508/http://www.lambda.net/~maximum/sipple.html&quot;&gt;this&lt;/a&gt; page:&lt;/p&gt;

&lt;p&gt;In the September of 1975, Oliver (‘Billy’) Sipple was an ex-marine of thirty-three, injured in Vietnam and living in San Francisco. He was in and out of the veteran’s hospital, six years into civilian life.&lt;/p&gt;

&lt;p&gt;One afternoon, he stood in a crowd of thousands of people to see the visiting President Gerald Ford leave a San Francisco hotel from across the street. Ford stopped to wave. Suddenly, a shot sounded, and Oliver saw a woman nearby adjusting the aim of her revolver. He lunged and grabbed her arm, sending the second bullet into the hotel, injuring a man inside.&lt;/p&gt;

&lt;p&gt;Oliver was thanked for saving the president, and celebrated as a hero by the media. A heroic veteran.&lt;/p&gt;

&lt;p&gt;Soon the media learned that he was in fact a heroic &lt;em&gt;gay&lt;/em&gt; veteran.&lt;!--ex--&gt;&lt;/p&gt;

&lt;p&gt;Oliver had shared his sexual orientation with with the San Francisco gay community—or at least he had worked at a gay bar, paraded for gay pride, demonstrated for gay rights, helped in the (LGBT) &lt;a href=&quot;https://en.wikipedia.org/wiki/Imperial_Court_System&quot;&gt;Imperial Court System&lt;/a&gt;, and worked on the campaign to elect openly gay board of supervisors candidate Harvey Milk. But he hadn’t shared it with his family in Detroit, who had more old-fashioned impressions about the morality of homosexuality. He also hadn’t shared it with the world at large, who after all, lived at a time when evidence of a gay person being a public hero was considered fascinating news.&lt;/p&gt;

&lt;p&gt;How did the media learn about this? Perhaps there were many sources, or would have been eventually. But the morning after the shooting, two prominent gay activists each outed Oliver to the San Francisco Chronicle. One was Reverend Ray Broshears, leader of the ‘Lavender Panthers’. The other was Oliver’s own friend, Harvey Milk.&lt;/p&gt;

&lt;p&gt;!&lt;/p&gt;

&lt;p&gt;Harvey is reported to have explained privately to a friend, “It’s too good an opportunity. For once we can show that gays do heroic things, not just all that caca about molesting children and hanging out in bathrooms.”&lt;/p&gt;

&lt;p&gt;The next day, Herb Caen, the San Francisco Chronicle reporter who received these messages, reported to the world that Oliver was gay. He added that Oliver was friends with Harvey Milk, and speculated that President Ford hadn’t invited him to the White House because of his sexual orientation.&lt;/p&gt;

&lt;p&gt;Somewhere in here, Oliver asked that the media not report on the topic of his sexual orientation, lest his family or current employer learn of it. It’s not clear to me whether this was in time for them to definitively know that he didn’t want them to when they first did it, since apparently Caen ‘couldn’t contact him’.&lt;/p&gt;

&lt;p&gt;At any rate, the topic was reported on thoroughly. Gay activists called for his recognition as a gay hero. He was deluged by reporters, and hid at a friend’s house, at which point they turned to interviewing Harvey Milk. Harvey opined that President Ford’s gratitude would indeed have flowed more generously had Oliver been straight.&lt;/p&gt;

&lt;p&gt;Oliver’s mother was purportedly harassed by her neighbors, and declared her intent never to speak to him again. He was estranged from his family. His father at some point instructed his brother to forget that he had a brother.&lt;/p&gt;

&lt;p&gt;Oliver sued the reporter Caen and numerous newspapers and publishers for the invasion of his privacy. The suit was dismissed, but he fought on. In 1984 a state court of appeals held that he had become news, and his sexual orientation was part of the story.&lt;/p&gt;

&lt;p&gt;Oliver didn’t do well after becoming a hero. He drank heavily, was diagnosed with schizophrenia, put on weight, and needed a pacemaker. Over a drink, he was heard to say that he regretted grabbing the gun.&lt;/p&gt;

&lt;p&gt;It is said that he eventually reconciled with his family, but it is also said that his father didn’t let him come to his mother’s funeral, so granting both stories it may have been a late or mild reconciliation.&lt;/p&gt;

&lt;p&gt;One February day in 1989, Oliver’s friend found him dead in his San Francisco apartment, alongside a bottle of Jack Daniels and a running television. He was 47.&lt;/p&gt;

&lt;p&gt;Years later, journalistic ethics professors found this an instructive class discussion topic.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="history" /><category term="story" /><category term="sanfrancisco" /><summary type="html">The other day I read Wikipedia arguably too much, and consequently came to know the story of Oliver Sipple. Here’s my summary of the story according to these two Wikipedia pages and this page: In the September of 1975, Oliver (‘Billy’) Sipple was an ex-marine of thirty-three, injured in Vietnam and living in San Francisco. He was in and out of the veteran’s hospital, six years into civilian life. One afternoon, he stood in a crowd of thousands of people to see the visiting President Gerald Ford leave a San Francisco hotel from across the street. Ford stopped to wave. Suddenly, a shot sounded, and Oliver saw a woman nearby adjusting the aim of her revolver. He lunged and grabbed her arm, sending the second bullet into the hotel, injuring a man inside. Oliver was thanked for saving the president, and celebrated as a hero by the media. A heroic veteran. Soon the media learned that he was in fact a heroic gay veteran.</summary></entry><entry><title type="html">Neck abacus</title><link href="http://localhost:4000/2021/02/17/wearable-abaci.html" rel="alternate" type="text/html" title="Neck abacus" /><published>2021-02-17T22:12:00-08:00</published><updated>2021-02-17T22:12:00-08:00</updated><id>http://localhost:4000/2021/02/17/wearable-abaci</id><content type="html" xml:base="http://localhost:4000/2021/02/17/wearable-abaci.html">&lt;p&gt;My &lt;a href=&quot;https://worldspiritsockpuppet.com/2020/12/01/points-for-anxiety.html&quot;&gt;points for anxiety&lt;/a&gt; system continued to help, but was encumbered by the friction of getting my phone out to mark points. Thus I have turned to wearable abaci.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/PXL_20210218_064803676.jpg&quot; alt=&quot;neclace 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/PXL_20210218_064811541.jpg&quot; alt=&quot;necklace 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I made this necklace according to very roughly &lt;a href=&quot;https://www.pinterest.com/pin/212724782374260653/&quot;&gt;this picture&lt;/a&gt;, using knitting wool and beads I bought at one point to use as virtual in-house currency and now found in a box in my room. It works well! The beads don’t shift unless I move them, which is easy and pleasing. It seems clearly more convenient than my phone. (Plus, I can show off to those in the know that I have 4 or 6 or 24 or 26 of something!) I am also for now reminded when I look in a mirror to consider whether I can get a point, which is currently a plus.&lt;!--ex--&gt;&lt;/p&gt;

&lt;p&gt;You can buy bracelet versions in a very small number of places online, and also keychain or general hanging clip-on versions, but I don’t think I saw necklaces anywhere. This seems striking, given the clear superiority to a phone counter for me so far, and the likely scale of phone counter usage in the world.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="craft" /><category term="advice" /><category term="worldlypositions" /><summary type="html">My points for anxiety system continued to help, but was encumbered by the friction of getting my phone out to mark points. Thus I have turned to wearable abaci. I made this necklace according to very roughly this picture, using knitting wool and beads I bought at one point to use as virtual in-house currency and now found in a box in my room. It works well! The beads don’t shift unless I move them, which is easy and pleasing. It seems clearly more convenient than my phone. (Plus, I can show off to those in the know that I have 4 or 6 or 24 or 26 of something!) I am also for now reminded when I look in a mirror to consider whether I can get a point, which is currently a plus.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/PXL_20210218_064803676.jpg" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/PXL_20210218_064803676.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>
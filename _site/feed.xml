<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-03T21:10:48-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">world spirit sock puppet</title><subtitle>Inclusive writings of Katja Grace</subtitle><author><name>Katja Grace</name></author><entry><title type="html">Are we so good to simulate</title><link href="http://localhost:4000/2024/03/03/are-we-so-good-to-simulate.html" rel="alternate" type="text/html" title="Are we so good to simulate" /><published>2024-03-03T19:01:00-08:00</published><updated>2024-03-03T19:01:00-08:00</updated><id>http://localhost:4000/2024/03/03/are-we-so-good-to-simulate</id><content type="html" xml:base="http://localhost:4000/2024/03/03/are-we-so-good-to-simulate.html">&lt;p&gt;If you believe that,—&lt;/p&gt;

&lt;p&gt;a) a civilization like ours is likely to survive into technological incredibleness, and&lt;/p&gt;

&lt;p&gt;b) a technologically incredible civilization is very likely to create ‘ancestor simulations’,&lt;/p&gt;

&lt;p&gt;—then the Simulation Argument says you should expect that you are currently in such an ancestor simulation, rather than in the genuine historical civilization that later gives rise to an abundance of future people.&lt;/p&gt;

&lt;p&gt;Not officially included in the argument I think, but commonly believed: both a) and b) seem pretty likely, ergo we should conclude we are in a simulation.&lt;/p&gt;

&lt;p&gt;I don’t know about this. Here’s my counterargument:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;‘Simulations’ here are people who are intentionally misled about their whereabouts in the universe. For the sake of argument, let’s use the term ‘simulation’ for all such people, including e.g. biological people who have been grown in Truman-show-esque situations.&lt;/li&gt;
  &lt;li&gt;In the long run, the cost of running a simulation of a confused mind is probably similar to that of running a non-confused mind.&lt;/li&gt;
  &lt;li&gt;Probably much, much less than 50% of the resources allocated to computing minds in the long run will be allocated to confused minds, because non-confused minds are generally more useful than confused minds. There are some uses for confused minds, but quite a lot of uses for non-confused minds. (This is debatable.) Of resources directed toward minds in the future, I’d guess less than a thousandth is directed toward confused minds.&lt;/li&gt;
  &lt;li&gt;Thus on average, for a given apparent location in the universe, the majority of minds thinking they are in that location are correct. (I guess at at least a thousand to one.)&lt;/li&gt;
  &lt;li&gt;For people in our situation to be majority simulations, this would have to be a vastly more simulated location than average, like &amp;gt;1000x&lt;/li&gt;
  &lt;li&gt;I agree there’s some merit to simulating ancestors, but 1000x more simulated than average is a lot - is it clear that we are that radically desirable a people to simulate? Perhaps, but also we haven’t thought much about the other people to simulate, or what will go in in the rest of the universe. Possibly we are radically over-salient to us. It’s true that we are a very few people in the history of what might be a very large set of people, at perhaps a causally relevant point. But is it clear that is a very, very strong reason to simulate some people in detail? It feels like it might be salient because it is what makes us stand out, and someone who has the most energy-efficient brain in the Milky Way would think that was the obviously especially strong reason to simulate a mind, etc.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I’m not sure what I think in the end, but for me this pushes back against the intuition that it’s so radically cheap, surely someone will do it. For instance from Bostrom:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We noted that a rough approximation of the computational power of a planetary-mass computer is 1042 operations per second, and that assumes only already known nanotechnological designs, which are probably far from optimal. A single such a computer could simulate the entire mental history of humankind (call this an ancestor-simulation) by using less than one millionth of its processing power for one second. A posthuman civilization may eventually build an astronomical number of such computers. We can conclude that the computing power available to a posthuman civilization is sufficient to run a huge number of ancestor-simulations even it allocates only a minute fraction of its resources to that purpose. We can draw this conclusion even while leaving a substantial margin of error in all our estimates.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Simulating history so far might be extremely cheap. But if there are finite resources and astronomically many extremely cheap things, only a few will be done.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="philosophy" /><category term="simulation" /><category term="meteuphoric" /><summary type="html">If you believe that,— a) a civilization like ours is likely to survive into technological incredibleness, and b) a technologically incredible civilization is very likely to create ‘ancestor simulations’, —then the Simulation Argument says you should expect that you are currently in such an ancestor simulation, rather than in the genuine historical civilization that later gives rise to an abundance of future people. Not officially included in the argument I think, but commonly believed: both a) and b) seem pretty likely, ergo we should conclude we are in a simulation. I don’t know about this. Here’s my counterargument: ‘Simulations’ here are people who are intentionally misled about their whereabouts in the universe. For the sake of argument, let’s use the term ‘simulation’ for all such people, including e.g. biological people who have been grown in Truman-show-esque situations. In the long run, the cost of running a simulation of a confused mind is probably similar to that of running a non-confused mind. Probably much, much less than 50% of the resources allocated to computing minds in the long run will be allocated to confused minds, because non-confused minds are generally more useful than confused minds. There are some uses for confused minds, but quite a lot of uses for non-confused minds. (This is debatable.) Of resources directed toward minds in the future, I’d guess less than a thousandth is directed toward confused minds. Thus on average, for a given apparent location in the universe, the majority of minds thinking they are in that location are correct. (I guess at at least a thousand to one.) For people in our situation to be majority simulations, this would have to be a vastly more simulated location than average, like &amp;gt;1000x I agree there’s some merit to simulating ancestors, but 1000x more simulated than average is a lot - is it clear that we are that radically desirable a people to simulate? Perhaps, but also we haven’t thought much about the other people to simulate, or what will go in in the rest of the universe. Possibly we are radically over-salient to us. It’s true that we are a very few people in the history of what might be a very large set of people, at perhaps a causally relevant point. But is it clear that is a very, very strong reason to simulate some people in detail? It feels like it might be salient because it is what makes us stand out, and someone who has the most energy-efficient brain in the Milky Way would think that was the obviously especially strong reason to simulate a mind, etc. I’m not sure what I think in the end, but for me this pushes back against the intuition that it’s so radically cheap, surely someone will do it. For instance from Bostrom: We noted that a rough approximation of the computational power of a planetary-mass computer is 1042 operations per second, and that assumes only already known nanotechnological designs, which are probably far from optimal. A single such a computer could simulate the entire mental history of humankind (call this an ancestor-simulation) by using less than one millionth of its processing power for one second. A posthuman civilization may eventually build an astronomical number of such computers. We can conclude that the computing power available to a posthuman civilization is sufficient to run a huge number of ancestor-simulations even it allocates only a minute fraction of its resources to that purpose. We can draw this conclusion even while leaving a substantial margin of error in all our estimates. Simulating history so far might be extremely cheap. But if there are finite resources and astronomically many extremely cheap things, only a few will be done.</summary></entry><entry><title type="html">Shaming with and without naming</title><link href="http://localhost:4000/2024/02/21/shaming-with-and-without-naming.html" rel="alternate" type="text/html" title="Shaming with and without naming" /><published>2024-02-21T23:00:00-08:00</published><updated>2024-02-21T23:00:00-08:00</updated><id>http://localhost:4000/2024/02/21/shaming-with-and-without-naming</id><content type="html" xml:base="http://localhost:4000/2024/02/21/shaming-with-and-without-naming.html">&lt;p&gt;Suppose someone wrongs you and you want to emphatically mar their reputation, but only insofar as doing so is conducive to the best utilitarian outcomes. I was thinking about this one time and it occurred to me that there are at least two fairly different routes to positive utilitarian outcomes from publicly shaming people for apparent wrongdoings*:&lt;/p&gt;

&lt;p&gt;A) People fear such shaming and avoid activities that may bring it about (possibly including the original perpetrator)&lt;/p&gt;

&lt;p&gt;B) People internalize your values and actually agree more that the sin is bad, and then do it less&lt;!--ex--&gt;&lt;/p&gt;

&lt;p&gt;These things are fairly different, and don’t necessarily come together. I can think of shaming efforts that seem to inspire substantial fear of social retribution in many people (A) while often reducing sympathy for the object-level moral claims (B).&lt;/p&gt;

&lt;p&gt;It seems like on a basic strategical level (ignoring the politeness of trying to change others’ values) you would much prefer have 2 than 1, because it is longer lasting, and doesn’t involve you threatening conflict with other people for the duration.&lt;/p&gt;

&lt;p&gt;It seems to me that whether you name the person in your shaming makes a big difference to which of these you hit. If I say “Sarah Smith did [—]”, then Sarah is perhaps punished, and people in general fear being punished like Sarah (A). If I say “Today somebody did [—]”, then Sarah can’t get any social punishment, so nobody need fear that much (except for private shame), but you still get B—people having the sense that people think [—] is bad, and thus also having the sense that it is bad. Clearly not naming Sarah makes it harder for A) to happen, but I also have the sense—much less clearly—that by naming Sarah you actually get less of B).&lt;/p&gt;

&lt;p&gt;This might be too weak a sense to warrant speculation, but in case not—why would this be? Is it because you are allowed to choose without being threatened, and with your freedom, you want to choose the socially sanctioned one? Whereas if someone is named you might be resentful and defensive, which is antithetical with going along with the norm that has been bid for? Is it that if you say Sarah did the thing, you have set up two concrete sides, you and Sarah, and observers might be inclined to join Sarah’s side instead of yours? (Or might already be on Sarah’s side in all manner of you-Sarah distinctions?)&lt;/p&gt;

&lt;p&gt;Is it even true that not naming gets you more of B?&lt;/p&gt;

&lt;p&gt;—&lt;/p&gt;

&lt;p&gt;*NB: I haven’t decided if it’s almost ever appropriate to try to cause other people to feel shame, but it remains true that under certain circumstances fantasizing about it is an apparently natural response.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="meteuphoric" /><category term="psychology" /><category term="lifestrategy" /><summary type="html">Suppose someone wrongs you and you want to emphatically mar their reputation, but only insofar as doing so is conducive to the best utilitarian outcomes. I was thinking about this one time and it occurred to me that there are at least two fairly different routes to positive utilitarian outcomes from publicly shaming people for apparent wrongdoings*: A) People fear such shaming and avoid activities that may bring it about (possibly including the original perpetrator) B) People internalize your values and actually agree more that the sin is bad, and then do it less</summary></entry><entry><title type="html">Parasocial relationship logic</title><link href="http://localhost:4000/2024/02/18/i-parasocial-relationship-logic.html" rel="alternate" type="text/html" title="Parasocial relationship logic" /><published>2024-02-18T23:00:00-08:00</published><updated>2024-02-18T23:00:00-08:00</updated><id>http://localhost:4000/2024/02/18/i-parasocial-relationship-logic</id><content type="html" xml:base="http://localhost:4000/2024/02/18/i-parasocial-relationship-logic.html">&lt;p&gt;If:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;You become like the five people you spend the most time with (or something remotely like that)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The people who are most extremal in good ways tend to be highly successful&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Should you try to have 2-3 of your five relationships be parasocial ones with people too successful to be your friend individually?&lt;!--ex--&gt;&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="meteuphoric" /><category term="lifestrategy" /><summary type="html">If: You become like the five people you spend the most time with (or something remotely like that) The people who are most extremal in good ways tend to be highly successful Should you try to have 2-3 of your five relationships be parasocial ones with people too successful to be your friend individually?</summary></entry><entry><title type="html">Deep and obvious points in the gap between your thoughts and your pictures of thought</title><link href="http://localhost:4000/2024/02/14/deep-and-obvious-points.html" rel="alternate" type="text/html" title="Deep and obvious points in the gap between your thoughts and your pictures of thought" /><published>2024-02-14T23:00:00-08:00</published><updated>2024-02-14T23:00:00-08:00</updated><id>http://localhost:4000/2024/02/14/deep-and-obvious-points</id><content type="html" xml:base="http://localhost:4000/2024/02/14/deep-and-obvious-points.html">&lt;p&gt;Some &lt;a href=&quot;https://worldspiritsockpuppet.substack.com/p/bodega-bay-workshop-18-11-26&quot;&gt;ideas&lt;/a&gt; feel either deep or extremely obvious. You’ve heard some trite truism your whole life, then one day an epiphany lands and you try to save it with words, and you realize the description is that truism. And then you go out and try to tell others what you saw, and you can’t reach past their bored nodding. Or even you yourself, looking back, wonder why you wrote such tired drivel with such excitement.&lt;!--ex--&gt;&lt;/p&gt;

&lt;p&gt;When this happens, I wonder if it’s because the thing is true in your model of how to think, but not in how you actually think.&lt;/p&gt;

&lt;p&gt;For instance, “when you think about the future, the thing you are dealing with is your own imaginary image of the future, not the future itself”.&lt;/p&gt;

&lt;p&gt;On the one hand: of course. You think I’m five and don’t know broadly how thinking works? You think I was mistakenly modeling my mind as doing time-traveling and also enclosing the entire universe within itself? No I wasn’t, and I don’t need your insight.&lt;/p&gt;

&lt;p&gt;But on the other hand one does habitually think of the hazy region one conjures connected to the present as ‘the future’ not as ‘my image of the future’, so when this advice is applied to one’s thinking—when the future one has relied on and cowered before is seen to evaporate in a puff of realizing you were overly drawn into a fiction—it can feel like a revelation, because it really is news to how you think, just not how you think a rational agent thinks.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="meteuphoric" /><category term="minds" /><summary type="html">Some ideas feel either deep or extremely obvious. You’ve heard some trite truism your whole life, then one day an epiphany lands and you try to save it with words, and you realize the description is that truism. And then you go out and try to tell others what you saw, and you can’t reach past their bored nodding. Or even you yourself, looking back, wonder why you wrote such tired drivel with such excitement.</summary></entry><entry><title type="html">Survey of 2,778 AI authors: six parts in pictures</title><link href="http://localhost:4000/2024/01/04/survey-of-2778-ai.html" rel="alternate" type="text/html" title="Survey of 2,778 AI authors: six parts in pictures" /><published>2024-01-04T17:00:01-08:00</published><updated>2024-01-04T17:00:01-08:00</updated><id>http://localhost:4000/2024/01/04/survey-of-2778-ai</id><content type="html" xml:base="http://localhost:4000/2024/01/04/survey-of-2778-ai.html">&lt;p&gt;
&lt;em&gt;Crossposted from&lt;a href=&quot;https://blog.aiimpacts.org/p/2023-ai-survey-of-2778-six-things&quot;&gt; AI Impacts blog&lt;/a&gt;&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;
The 2023 Expert Survey on Progress in AI is&lt;a href=&quot;https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf&quot;&gt; out&lt;/a&gt;, this time with 2778 participants from six top AI venues (up from&lt;a href=&quot;https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2022_expert_survey_on_progress_in_ai#population&quot;&gt; about 700&lt;/a&gt; and two in the&lt;a href=&quot;https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2022_expert_survey_on_progress_in_ai&quot;&gt; 2022 ESPAI&lt;/a&gt;), making it probably the biggest ever survey of AI researchers.
&lt;/p&gt;
&lt;p&gt;
People answered in October, an eventful fourteen months after the 2022 survey, which had mostly identical questions for comparison.
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf&quot;&gt;Here&lt;/a&gt; is the preprint. And here are six interesting bits in pictures (with figure numbers matching paper, for ease of learning more):
&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;1. Expected time to human-level performance dropped 1-5 decades since the 2022 survey. &lt;/strong&gt;As always, our questions about ‘high level machine intelligence’ (HLMI) and ‘full automation of labor’ (FAOL) got very different answers, and individuals disagreed a lot (shown as thin lines below), but the aggregate forecasts for both sets of questions dropped sharply. For context, between 2016 and 2022 surveys, the forecast for HLMI had only shifted about a year.
&lt;/p&gt;
&lt;p&gt;

&lt;img src=&quot;https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/kvngwylqnpf0tvlf8g8a&quot; width=&quot;&quot; alt=&quot;Probability assigned to HLMI over time&quot; title=&quot;image_tooltip&quot; /&gt;
(Fig 3)

&lt;img src=&quot;https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/nllvn2ilyfuifmyoboa2&quot; width=&quot;&quot; alt=&quot;Probability assigned to FAOL over time&quot; title=&quot;image_tooltip&quot; /&gt;
(Fig 4)
&lt;/p&gt;
&lt;!--ex--&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;2. Time to most narrow milestones decreased, some by a lot. &lt;/strong&gt;AI researchers are expected to be professionally fully automatable a quarter of a century earlier than in 2022, and NYT bestselling fiction dropped by more than half to ~2030. Within five years, AI systems are forecast to be feasible that can fully make a payment processing site from scratch, or entirely generate a new song that sounds like it’s by e.g. Taylor Swift, or autonomously download and fine-tune a large language model.
&lt;/p&gt;
&lt;p&gt;

&lt;img src=&quot;https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/lkogl5wintmggyw4upeg&quot; width=&quot;&quot; alt=&quot;Change in guesses about time to specific narrow capabilities&quot; title=&quot;image_tooltip&quot; /&gt;
(Fig 2)
&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;3. Median respondents put 5% or more on advanced AI leading to human extinction or similar, and a third to a half of participants gave 10% or more. &lt;/strong&gt;This was across four questions, one about overall value of the future and three more directly about extinction.
&lt;/p&gt;
&lt;p&gt;

&lt;img src=&quot;https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/czh3xjxt7w1hwtojijeh&quot; width=&quot;&quot; alt=&quot;Around 40% of participants gave at least 10% chance to human extinction from AI&quot; title=&quot;image_tooltip&quot; /&gt;
(Fig 10)
&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;4. Many participants found many scenarios worthy of substantial concern over the next 30 years.&lt;/strong&gt; For every one of eleven scenarios and ‘other’ that we asked about, at least a third of participants considered it deserving of substantial or extreme concern.
&lt;/p&gt;
&lt;p&gt;

&lt;img src=&quot;https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/pkrmpgh619mzaa703ncu&quot; width=&quot;&quot; alt=&quot;Level of concern warranted by different scenarios&quot; title=&quot;image_tooltip&quot; /&gt;
(Fig 9)
&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;5. There are few confident optimists or pessimists about advanced AI: high hopes and dire concerns are usually found together.&lt;/strong&gt; 68% of participants who thought HLMI was more likely to lead to good outcomes than bad, but nearly half of these people put at least 5% on extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes.
&lt;/p&gt;
&lt;p&gt;

&lt;img src=&quot;https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/zusla2aogk561nuoejku&quot; width=&quot;&quot; alt=&quot;800 responses to how likely the future being different levels of good is after HLMI&quot; title=&quot;image_tooltip&quot; /&gt;
(Fig 11: a random 800 responses as vertical bars, higher definition below)
&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://blog.aiimpacts.org/api/v1/file/29c9bfb8-5d57-4b5f-9ffe-e50691268b4d.pdf&quot;&gt;Download&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;6. 70% of participants would like to see research aimed at minimizing risks of AI systems be prioritized more highly.&lt;/strong&gt; This is much like 2022, and in both years a third of participants asked for “much more”—more than doubling since 2016.
&lt;/p&gt;
&lt;p&gt;

&lt;img src=&quot;https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/zk5xgvtclrtpeuc8ymkh&quot; width=&quot;&quot; alt=&quot;how much should safety research be prioritized?&quot; title=&quot;image_tooltip&quot; /&gt;
(Fig 15)
&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;p&gt;
If you enjoyed this,&lt;a href=&quot;https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf&quot;&gt; the paper&lt;/a&gt; covers many other questions, as well as more details on the above. What makes AI progress go? Has it sped up? Would it be better if it were slower or faster? What will AI systems be like in 2043? Will we be able to know the reasons for its choices before then? Do people from academia and industry have different views? Are concerns about AI due to misunderstandings of AI research? Do people who completed undergraduate study in Asia put higher chances on extinction from AI than those who studied in America? Is the ‘alignment problem’ worth working on?
&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="ai" /><category term="meteuphoric" /><summary type="html">Crossposted from AI Impacts blog The 2023 Expert Survey on Progress in AI is out, this time with 2778 participants from six top AI venues (up from about 700 and two in the 2022 ESPAI), making it probably the biggest ever survey of AI researchers. People answered in October, an eventful fourteen months after the 2022 survey, which had mostly identical questions for comparison. Here is the preprint. And here are six interesting bits in pictures (with figure numbers matching paper, for ease of learning more): 1. Expected time to human-level performance dropped 1-5 decades since the 2022 survey. As always, our questions about ‘high level machine intelligence’ (HLMI) and ‘full automation of labor’ (FAOL) got very different answers, and individuals disagreed a lot (shown as thin lines below), but the aggregate forecasts for both sets of questions dropped sharply. For context, between 2016 and 2022 surveys, the forecast for HLMI had only shifted about a year. (Fig 3) (Fig 4)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/zusla2aogk561nuoejku" /><media:content medium="image" url="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6uJBqeG5ywE6Wcdqw/zusla2aogk561nuoejku" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">I put odds on ends with Nathan Young</title><link href="http://localhost:4000/2023/11/07/i-put-odds-on-ends-with-nathan-young.html" rel="alternate" type="text/html" title="I put odds on ends with Nathan Young" /><published>2023-11-07T21:25:00-08:00</published><updated>2023-11-07T21:25:00-08:00</updated><id>http://localhost:4000/2023/11/07/i-put-odds-on-ends-with-nathan-young</id><content type="html" xml:base="http://localhost:4000/2023/11/07/i-put-odds-on-ends-with-nathan-young.html">&lt;p&gt;I forgot to post this in August when we did it, so one might hope it would be out of date now but luckily/sadly my understanding of things is sufficiently coarse-grained that it probably isn’t much. Though all this policy and global coordination stuff of late sounds promising.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtu.be/Zum2QTaByeo&quot;&gt;&lt;img src=&quot;https://hosting.photobucket.com/images/i/katjasgrace/Screen_Shot_2023-11-16_at_9.12.15_PM.png&quot; alt=&quot;YouTube video of Odds and Ends episode&quot; /&gt;&lt;/a&gt;&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="podcast" /><category term="meteuphoric" /><summary type="html">I forgot to post this in August when we did it, so one might hope it would be out of date now but luckily/sadly my understanding of things is sufficiently coarse-grained that it probably isn’t much. Though all this policy and global coordination stuff of late sounds promising.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/matt-power-TpHmEoVSmfQ-unsplash.jpg" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/matt-power-TpHmEoVSmfQ-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A to Z of things</title><link href="http://localhost:4000/2023/11/03/a-to-z-of-things.html" rel="alternate" type="text/html" title="A to Z of things" /><published>2023-11-03T01:25:00-07:00</published><updated>2023-11-03T01:25:00-07:00</updated><id>http://localhost:4000/2023/11/03/a-to-z-of-things</id><content type="html" xml:base="http://localhost:4000/2023/11/03/a-to-z-of-things.html">&lt;p&gt;I wanted to give my good friends’ baby a book, in honor of her existence. And I recalled children’s books being an exciting genre. Yet checking in on that thirty years later, Amazon had none I could super get behind. They did have books I used to like, but for reasons now lost. And I wonder if as a child I just had no taste because I just didn’t know how good things could be.&lt;/p&gt;

&lt;p&gt;What would a good children’s book be like?&lt;/p&gt;

&lt;p&gt;When I was about sixteen, I thought one reasonable thing to have learned when I was about two would have been the concepts of ‘positive feedback loop’ and ‘negative feedback loop’, then being taught in my year 11 class. Very interesting, very bleedingly obvious once you saw it. Why not hear about this as soon as one is coherent? Evolution, if I recall, seemed similar.&lt;/p&gt;

&lt;p&gt;Here I finally enact my teenage self’s vision, and present A to Z of things, including some very interesting things that you might want a beautiful illustrative prompt to explain to your child as soon as they show glimmerings of conceptual thought: levers, markets, experiments, Greece, computer hardware, reference classes, feedback loops, (trees).&lt;/p&gt;

&lt;p&gt;I think so far, the initial recipient is most fond of the donkey, in fascinating support of everyone else’s theories about what children are actually into. (Don’t get me wrong, I also like donkeys—when I have a second monitor, I just use it to stream donkey cams.) But perhaps one day donkeys will be a gateway drug to monkeys, and monkeys to moths, and moths will be resting on perfecttly moth-colored trees, and BAM! Childhood improved.&lt;/p&gt;

&lt;p&gt;Anyway, if you want a copy, it’s now available in an ‘email it to a copy shop and get it printed yourself’ format! See below. Remember to ask for card that is stronger than your child’s bite.&lt;/p&gt;

&lt;p&gt;[&lt;a href=&quot;https://worldspiritsockpuppet.substack.com/api/v1/file/4c7a89d7-27f8-430b-9c8c-7b8cd61c21c6.pdf&quot;&gt;Front&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;[&lt;a href=&quot;https://worldspiritsockpuppet.substack.com/api/v1/file/2e2ed64c-4101-4f66-bf82-86920f10ee97.pdf&quot;&gt;Content&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F592e8439-aed4-4cbf-95a5-03f2868bf65e_2448x1590.png&quot; alt=&quot;Volcano and world&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb77660a-0a9b-4598-9452-b7f1e0e58140_2448x1590.png&quot; alt=&quot;Natural selection and orangutan&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dea6d4e-f024-4121-a335-57a47a7e1e3b_2448x1590.png&quot; alt=&quot;PFL and quantification&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5291002-4a68-4f5d-a0fa-79a012aef8dd_2448x1590.png&quot; alt=&quot;Donkey and experiment&quot; /&gt;&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="worldlypositions" /><summary type="html">I wanted to give my good friends’ baby a book, in honor of her existence. And I recalled children’s books being an exciting genre. Yet checking in on that thirty years later, Amazon had none I could super get behind. They did have books I used to like, but for reasons now lost. And I wonder if as a child I just had no taste because I just didn’t know how good things could be. What would a good children’s book be like? When I was about sixteen, I thought one reasonable thing to have learned when I was about two would have been the concepts of ‘positive feedback loop’ and ‘negative feedback loop’, then being taught in my year 11 class. Very interesting, very bleedingly obvious once you saw it. Why not hear about this as soon as one is coherent? Evolution, if I recall, seemed similar. Here I finally enact my teenage self’s vision, and present A to Z of things, including some very interesting things that you might want a beautiful illustrative prompt to explain to your child as soon as they show glimmerings of conceptual thought: levers, markets, experiments, Greece, computer hardware, reference classes, feedback loops, (trees). I think so far, the initial recipient is most fond of the donkey, in fascinating support of everyone else’s theories about what children are actually into. (Don’t get me wrong, I also like donkeys—when I have a second monitor, I just use it to stream donkey cams.) But perhaps one day donkeys will be a gateway drug to monkeys, and monkeys to moths, and moths will be resting on perfecttly moth-colored trees, and BAM! Childhood improved. Anyway, if you want a copy, it’s now available in an ‘email it to a copy shop and get it printed yourself’ format! See below. Remember to ask for card that is stronger than your child’s bite. [Front] [Content]</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/5dea6d4e-f024-4121-a335-57a47a7e1e3b_2448x1590.webp" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/5dea6d4e-f024-4121-a335-57a47a7e1e3b_2448x1590.webp" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The other side of the tidal wave</title><link href="http://localhost:4000/2023/11/02/the-other-side-of-the-tidal-wave.html" rel="alternate" type="text/html" title="The other side of the tidal wave" /><published>2023-11-02T22:25:00-07:00</published><updated>2023-11-02T22:25:00-07:00</updated><id>http://localhost:4000/2023/11/02/the-other-side-of-the-tidal-wave</id><content type="html" xml:base="http://localhost:4000/2023/11/02/the-other-side-of-the-tidal-wave.html">&lt;p&gt;I guess there’s maybe a 10-20% chance of AI causing human extinction in the coming decades, but I feel more distressed about it than even that suggests—I think because in the case where it doesn’t cause human extinction, I find it hard to imagine life not going kind of off the rails. So many things I like about the world seem likely to be over or badly disrupted with superhuman AI (writing, explaining things to people, friendships where you can be of any use to one another, taking pride in skills, thinking, learning, figuring out how to achieve things, making things, easy tracking of what is and isn’t conscious), and I don’t trust that the replacements will be actually good, or good for us, or that anything will be reversible.&lt;/p&gt;

&lt;p&gt;Even if we don’t die, it still feels like everything is coming to an end.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="AI" /><category term="worldlypositions" /><summary type="html">I guess there’s maybe a 10-20% chance of AI causing human extinction in the coming decades, but I feel more distressed about it than even that suggests—I think because in the case where it doesn’t cause human extinction, I find it hard to imagine life not going kind of off the rails. So many things I like about the world seem likely to be over or badly disrupted with superhuman AI (writing, explaining things to people, friendships where you can be of any use to one another, taking pride in skills, thinking, learning, figuring out how to achieve things, making things, easy tracking of what is and isn’t conscious), and I don’t trust that the replacements will be actually good, or good for us, or that anything will be reversible. Even if we don’t die, it still feels like everything is coming to an end.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/matt-power-TpHmEoVSmfQ-unsplash.jpg" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/matt-power-TpHmEoVSmfQ-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Robin Hanson and I talk about AI risk</title><link href="http://localhost:4000/2023/05/03/robin-hanson-and-i-talk-about-ai-risk.html" rel="alternate" type="text/html" title="Robin Hanson and I talk about AI risk" /><published>2023-05-03T20:20:00-07:00</published><updated>2023-05-03T20:20:00-07:00</updated><id>http://localhost:4000/2023/05/03/robin-hanson-and-i-talk-about-ai-risk</id><content type="html" xml:base="http://localhost:4000/2023/05/03/robin-hanson-and-i-talk-about-ai-risk.html">&lt;p&gt;From this afternoon: &lt;a href=&quot;https://www.youtube.com/watch?v=em0_p9eL_XE&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Our previous recorded discussions are &lt;a href=&quot;https://www.overcomingbias.com/p/grace-hanson-podcasts-4html?utm_source=%2Fsearch%2Fgrace-hanson%2520podcasts&amp;amp;utm_medium=reader2&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="ai" /><category term="meteuphoric" /><category term="podcast" /><summary type="html">From this afternoon: here Our previous recorded discussions are here.</summary></entry><entry><title type="html">Have we really forsaken natural selection?</title><link href="http://localhost:4000/2023/01/11/have-we-foresaken-natural-selection.html" rel="alternate" type="text/html" title="Have we really forsaken natural selection?" /><published>2023-01-11T14:50:00-08:00</published><updated>2023-01-11T14:50:00-08:00</updated><id>http://localhost:4000/2023/01/11/have-we-foresaken-natural-selection</id><content type="html" xml:base="http://localhost:4000/2023/01/11/have-we-foresaken-natural-selection.html">&lt;p&gt;Natural selection is often charged with having goals for humanity, and humanity is often charged with falling down on them. The big accusation, I think, is of sub-maximal procreation. If we cared at all about the genetic proliferation that natural selection wanted for us, then this time of riches would be a time of fifty-child families, not one of coddled dogs and state-of-the-art sitting rooms.&lt;/p&gt;

&lt;p&gt;But (the story goes) our failure is excusable, because instead of a deep-seated loyalty to genetic fitness, natural selection merely fitted humans out with a system of suggestive urges: hungers, fears, loves, lusts. Which all worked well together to bring about children in the prehistoric years of our forebears, but no more. In part because all sorts of things are different, and in part because we specifically made things different in that way on purpose: bringing about children gets in the way of the further satisfaction of those urges, so we avoid it (the story goes).&lt;/p&gt;

&lt;p&gt;This is generally floated as an illustrative warning about artificial intelligence. The moral is that if you make a system by first making multitudinous random systems and then systematically destroying all the ones that don’t do the thing you want, then the system you are left with might only do what you want while current circumstances persist, rather than being endowed with a consistent desire for the thing you actually had in mind.&lt;/p&gt;

&lt;p&gt;Observing acquaintences dispute this point recently, it struck me that humans are actually weirdly aligned with natural selection, more than I could easily account for.&lt;/p&gt;

&lt;p&gt;Natural selection, in its broadest, truest, (most idiolectic?) sense, doesn’t care about genes. Genes are a nice substrate on which natural selection famously makes particularly pretty patterns by driving a sensical evolution of lifeforms through interesting intricacies. But natural selection’s real love is existence. Natural selection just favors things that tend to exist. Things that start existing: great. Things that, having started existing, survive: amazing. Things that, while surviving, cause many copies of themselves to come into being: especial favorites of evolution, as long as there’s a path to the first ones coming into being.&lt;/p&gt;

&lt;p&gt;So natural selection likes genes that promote procreation and survival, but also likes elements that appear and don’t dissolve, ideas that come to mind and stay there, tools that are conceivable and copyable, shapes that result from myriad physical situations, rocks at the bottoms of mountains. Maybe this isn’t the dictionary definition of natural selection, but it is the real force in the world, of which natural selection of reproducing and surviving genetic clusters is one facet. Generalized natural selection—the thing that created us—says that the things that you see in the world are those things that exist best in the world.&lt;/p&gt;

&lt;p&gt;So what did natural selection want for us? What were we selected for? Existence.&lt;/p&gt;

&lt;p&gt;And while we might not proliferate our genes spectacularly well in particular, I do think we have a decent shot at a very prolonged existence.  Or the prolonged existence of some important aspects of our being. It seems plausible that humanity makes it to the stars, galaxies, superclusters. Not that we are maximally trying for that any more than we are maximally trying for children. And I do think there’s a large chance of us wrecking it with various existential risks. But it’s interesting to me that natural selection made us for existing, and we look like we might end up just totally killing it, existence-wise. Even though natural selection purportedly did this via a bunch of hackish urges that were good in 200,000 BC but you might have expected to be outside their domain of applicability by 2023. And presumably taking over the universe is an extremely narrow target: it can only be done by so many things.&lt;/p&gt;

&lt;p&gt;Thus it seems to me that humanity is plausibly doing astonishingly well on living up to natural selection’s goals. Probably not as well as a hypothetical race of creatures who each harbors a monomaniacal interest in prolonged species survival. And not so well as to be clear of great risk of foolish speciocide. But still staggeringly well.&lt;/p&gt;</content><author><name>Katja Grace</name></author><category term="ai" /><category term="meteuphoric" /><summary type="html">Natural selection is often charged with having goals for humanity, and humanity is often charged with falling down on them. The big accusation, I think, is of sub-maximal procreation. If we cared at all about the genetic proliferation that natural selection wanted for us, then this time of riches would be a time of fifty-child families, not one of coddled dogs and state-of-the-art sitting rooms. But (the story goes) our failure is excusable, because instead of a deep-seated loyalty to genetic fitness, natural selection merely fitted humans out with a system of suggestive urges: hungers, fears, loves, lusts. Which all worked well together to bring about children in the prehistoric years of our forebears, but no more. In part because all sorts of things are different, and in part because we specifically made things different in that way on purpose: bringing about children gets in the way of the further satisfaction of those urges, so we avoid it (the story goes). This is generally floated as an illustrative warning about artificial intelligence. The moral is that if you make a system by first making multitudinous random systems and then systematically destroying all the ones that don’t do the thing you want, then the system you are left with might only do what you want while current circumstances persist, rather than being endowed with a consistent desire for the thing you actually had in mind. Observing acquaintences dispute this point recently, it struck me that humans are actually weirdly aligned with natural selection, more than I could easily account for. Natural selection, in its broadest, truest, (most idiolectic?) sense, doesn’t care about genes. Genes are a nice substrate on which natural selection famously makes particularly pretty patterns by driving a sensical evolution of lifeforms through interesting intricacies. But natural selection’s real love is existence. Natural selection just favors things that tend to exist. Things that start existing: great. Things that, having started existing, survive: amazing. Things that, while surviving, cause many copies of themselves to come into being: especial favorites of evolution, as long as there’s a path to the first ones coming into being. So natural selection likes genes that promote procreation and survival, but also likes elements that appear and don’t dissolve, ideas that come to mind and stay there, tools that are conceivable and copyable, shapes that result from myriad physical situations, rocks at the bottoms of mountains. Maybe this isn’t the dictionary definition of natural selection, but it is the real force in the world, of which natural selection of reproducing and surviving genetic clusters is one facet. Generalized natural selection—the thing that created us—says that the things that you see in the world are those things that exist best in the world. So what did natural selection want for us? What were we selected for? Existence. And while we might not proliferate our genes spectacularly well in particular, I do think we have a decent shot at a very prolonged existence. Or the prolonged existence of some important aspects of our being. It seems plausible that humanity makes it to the stars, galaxies, superclusters. Not that we are maximally trying for that any more than we are maximally trying for children. And I do think there’s a large chance of us wrecking it with various existential risks. But it’s interesting to me that natural selection made us for existing, and we look like we might end up just totally killing it, existence-wise. Even though natural selection purportedly did this via a bunch of hackish urges that were good in 200,000 BC but you might have expected to be outside their domain of applicability by 2023. And presumably taking over the universe is an extremely narrow target: it can only be done by so many things. Thus it seems to me that humanity is plausibly doing astonishingly well on living up to natural selection’s goals. Probably not as well as a hypothetical race of creatures who each harbors a monomaniacal interest in prolonged species survival. And not so well as to be clear of great risk of foolish speciocide. But still staggeringly well.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hosting.photobucket.com/images/i/katjasgrace/nasa-Yj1M5riCKk4-unsplash.jpg" /><media:content medium="image" url="https://hosting.photobucket.com/images/i/katjasgrace/nasa-Yj1M5riCKk4-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>
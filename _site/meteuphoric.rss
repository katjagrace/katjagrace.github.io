<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
 <title>world spirit sock puppet</title>
 <description>Inclusive writings of Katja Grace</description>
 <link>http://localhost:4000</link>
 <lastBuildDate>Fri, 14 Oct 2022 06:41:47 -0700</lastBuildDate>
 <pubDate>Fri, 14 Oct 2022 06:41:47 -0700</pubDate>
 <ttl>1800</ttl>

 
 <item>
  <title>Counterarguments to the basic AI risk case</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p><em>Crossposted from <a href="https://aiimpacts.org/counterarguments-to-the-basic-ai-x-risk-case/">AI Impacts’ blog</a>.</em></p>

<p>This is going to be a list of holes I see in the basic argument for existential risk from superhuman AI systems<span id="easy-footnote-1-3345" class="easy-footnote-margin-adjust"></span><span class="easy-footnote"><a href="#easy-footnote-bottom-1-3345" title="That is, systems that are somewhat more capable than the most capable human."><sup>1</sup></a></span>. </p>

<p>To start, here’s an outline of what I take to be the basic case<span id="easy-footnote-2-3345" class="easy-footnote-margin-adjust"></span><span class="easy-footnote"><a href="#easy-footnote-bottom-2-3345" title="Based on countless conversations in the AI risk community, and various reading."><sup>2</sup></a></span>:</p>

<blockquote>

  <p><strong>I. If superhuman AI systems are built, any given system is likely to be ‘goal-directed’</strong></p>

  <p>Reasons to expect this:</p>

  <ol><li>Goal-directed behavior is likely to be valuable, e.g. economically. </li><li>Goal-directed entities may tend to arise from machine learning training processes not intending to create them (at least via the methods that are likely to be used).</li><li>‘Coherence arguments’ may imply that systems with some goal-directedness will become more strongly goal-directed over time.</li></ol>

  <p><strong>II. If goal-directed superhuman AI systems are built, their desired outcomes will probably be about as bad as an empty universe by human lights </strong></p>

  <p>Reasons to expect this:</p>

  <ol><li>Finding useful goals that aren’t extinction-level bad appears to be hard: we don’t have a way to usefully point at human goals, and divergences from human goals seem likely to produce goals that are in intense conflict with human goals, due to a) most goals producing <a href="https://en.wikipedia.org/wiki/Instrumental_convergence">convergent incentives</a> for controlling everything, and b) value <a href="https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile">being</a> ‘fragile’, such that an entity with ‘similar’ values will generally create a future of virtually no value.</li><li>Finding goals that are extinction-level bad and temporarily useful appears to be easy: for example, advanced AI with the sole objective ‘maximize company revenue’ might profit said company for a time before gathering the influence and wherewithal to pursue the goal in ways that blatantly harm society.</li><li>Even if humanity found acceptable goals, giving a powerful AI system any specific goals appears to be hard. We don’t know of any procedure to do it, and we have theoretical reasons to expect that AI systems produced through machine learning training will generally end up with goals other than those they were trained according to. Randomly aberrant goals resulting are probably extinction-level bad for reasons described in II.1 above.</li></ol>

  <p><strong>III. If most goal-directed superhuman AI systems have bad goals, the future will very likely be bad</strong></p>

  <p>That is, a set of ill-motivated goal-directed superhuman AI systems, of a scale likely to occur, would be capable of taking control over the future from humans. This is supported by at least one of the following being true:</p>

  <ol><li><strong>Superhuman AI would destroy humanity rapidly. </strong>This may be via ultra-powerful capabilities at e.g. technology design and strategic scheming, or through gaining such powers in an ‘intelligence explosion‘ (self-improvement cycle). Either of those things may happen either through exceptional heights of intelligence being reached or through highly destructive ideas being available to minds only mildly beyond our own.</li><li><strong>Superhuman AI would gradually come to control the future via accruing power and resources.</strong> Power and resources would be more available to the AI system(s) than to humans on average, because of the AI having far greater intelligence.</li></ol>

</blockquote>

<p>Below is a list of gaps in the above, as I see it, and counterarguments. A ‘gap’ is not necessarily unfillable, and may have been filled in any of the countless writings on this topic that I haven’t read. I might even think that a given one can probably be filled. I just don’t know what goes in it.  </p>

<p>This blog post is an attempt to run various arguments by you all on the way to making pages on <a href="http://aiimpacts.org/">AI Impacts</a> about arguments for AI risk and corresponding counterarguments. At some point in that process I hope to also read others’ arguments, but this is not that day. So what you have here is a bunch of arguments that occur to me, not an exhaustive literature review. </p>

<h2><strong>Counterarguments</strong></h2>

<h3><strong><em>A. Contra “superhuman AI systems will be ‘goal-directed’”</em></strong></h3>

<h4>Different calls to ‘goal-directedness’ don’t necessarily mean the same concept</h4>

<p>‘Goal-directedness’ is a vague concept. It is unclear that the ‘goal-directednesses’ that are favored by economic pressure, training dynamics or coherence arguments (the component arguments in part I of the argument above) are the same ‘goal-directedness’ that implies a zealous drive to control the universe (i.e. that makes most possible goals very bad, fulfilling II above). </p>

<p>One well-defined concept of goal-directedness is ‘utility maximization’: always doing what maximizes a particular utility function, given a particular set of beliefs about the world. </p>

<p>Utility maximization does seem to quickly engender an interest in controlling literally everything, at least for many utility functions one might have<span id="easy-footnote-3-3345" class="easy-footnote-margin-adjust"></span><span class="easy-footnote"><a href="#easy-footnote-bottom-3-3345" title="Though not all: you might have an easily satiable utility function, or only care about the near future."><sup>3</sup></a></span>. If you want things to go a certain way, then you have reason to control anything which gives you any leverage over that, i.e. potentially all resources in the universe (i.e. agents have ‘<a href="https://en.wikipedia.org/wiki/Instrumental_convergence">convergent instrumental goals</a>’). This is in serious conflict with anyone else with resource-sensitive goals, even if <em>prima facie</em> those goals didn’t look particularly opposed. For instance, a person who wants all things to be red and another person who wants all things to be cubes may not seem to be at odds, given that all things could be red cubes. However if these projects might each fail for lack of energy, then they are probably at odds. </p>

<p>Thus utility maximization is a notion of goal-directedness that allows Part II of the argument to work, by making a large class of goals deadly.</p>

<p>You might think that any other concept of ‘goal-directedness’ would also lead to this zealotry. If one is inclined toward outcome O in any plausible sense, then does one not have an interest in anything that might help procure O? No: if a system is not a ‘coherent’ agent, then it can have a tendency to bring about O in a range of circumstances, without this implying that it will take any given effective opportunity to pursue O. This assumption of consistent adherence to a particular evaluation of everything is part of utility maximization, not a law of physical systems. Call machines that push toward particular goals but are not utility maximizers pseudo-agents. </p>

<p>Can pseudo-agents exist? Yes—utility maximization is computationally intractable, so any physically existent ‘goal-directed’ entity is going to be a pseudo-agent. We are all pseudo-agents, at best. But it seems something like a spectrum. At one end is a thermostat, then maybe a thermostat with a better algorithm for adjusting the heat. Then maybe a thermostat which intelligently controls the windows. After a lot of honing, you might have a system much more like a utility-maximizer: a system that deftly seeks out and seizes well-priced opportunities to make your room 68 degrees—upgrading your house, buying R&amp;D, influencing your culture, building a vast mining empire. Humans might not be very far on this spectrum, but they seem enough like utility-maximizers already to be alarming. (And it might not be well-considered as a one-dimensional spectrum—for instance, perhaps ‘tendency to modify oneself to become more coherent’ is a fairly different axis from ‘consistency of evaluations of options and outcomes’, and calling both ‘more agentic’ is obscuring.)</p>

<p>Nonetheless, it seems plausible that there is a large space of systems which strongly increase the chance of some desirable objective O occurring without even acting as much like maximizers of an identifiable utility function as humans would. For instance, without searching out novel ways of making O occur, or modifying themselves to be more consistently O-maximizing. Call these ‘weak pseudo-agents’. </p>

<p>For example, I can imagine a system constructed out of a huge number of ‘IF X THEN Y’ statements (reflexive responses), like ‘if body is in hallway, move North’, ‘if hands are by legs and body is in kitchen, raise hands to waist’.., equivalent to a kind of vector field of motions, such that for every particular state, there are directions that all the parts of you should be moving. I could imagine this being designed to fairly consistently cause O to happen within some context. However since such behavior would not be produced by a process optimizing O, you shouldn’t expect it to find new and strange routes to O, or to seek O reliably in novel circumstances. There appears to be zero pressure for this thing to become more coherent, unless its design already involves reflexes to move its thoughts in certain ways that lead it to change itself. I expect you could build a system like this that reliably runs around and tidies your house say, or runs your social media presence, without it containing any impetus to become a more coherent agent (because it doesn’t have any reflexes that lead to pondering self-improvement in this way).</p>

<p>It is not clear that economic incentives generally favor the far end of this spectrum over weak pseudo-agency. There are incentives toward systems being more like utility maximizers, but also incentives against. </p>

<p>The reason any kind of ‘goal-directedness’ is incentivised in AI systems is that then the system can be given an objective by someone hoping to use their cognitive labor, and the system will make that objective happen. Whereas a similar non-agentic AI system might still do almost the same cognitive labor, but require an agent (such as a person) to look at the objective and decide what should be done to achieve it, then ask the system for that. Goal-directedness means automating this high-level strategizing. </p>

<p>Weak pseudo-agency fulfills this purpose to some extent, but not as well as utility maximization. However if we think that utility maximization is difficult to wield without great destruction, then that suggests a disincentive to creating systems with behavior closer to utility-maximization. Not just from the world being destroyed, but from the same dynamic causing more minor divergences from expectations, if the user can’t specify their own utility function well. </p>

<p>That is, if it is true that utility maximization tends to lead to very bad outcomes relative to any slightly different goals (in the absence of great advances in the field of AI alignment), then the most economically favored level of goal-directedness seems unlikely to be as far as possible toward utility maximization. More likely it is a level of pseudo-agency that achieves a lot of the users’ desires without bringing about sufficiently detrimental side effects to make it not worthwhile. (This is likely more agency than is socially optimal, since some of the side-effects will be harms to others, but there seems no reason to think that it is a very high degree of agency.)</p>

<p>Some minor but perhaps illustrative evidence: anecdotally, people prefer interacting with others who predictably carry out their roles or adhere to deontological constraints, rather than consequentialists in pursuit of broadly good but somewhat unknown goals. For instance, employers would often prefer employees who predictably follow rules than ones who try to forward company success in unforeseen ways.</p>

<p>The other arguments to expect goal-directed systems mentioned above seem more likely to suggest approximate utility-maximization rather than some other form of goal-directedness, but it isn’t that clear to me. I don’t know what kind of entity is most naturally produced by contemporary ML training. Perhaps someone else does. I would guess that it’s more like the reflex-based agent described above, at least at present. But present systems aren’t the concern.</p>

<p>Coherence arguments are arguments for being coherent a.k.a. maximizing a utility function, so one might think that they imply a force for utility maximization in particular. That seems broadly right. Though note that these are arguments that there is some pressure for the system to modify itself to become more coherent. What actually results from specific systems modifying themselves seems like it might have details not foreseen in an abstract argument merely suggesting that the status quo is suboptimal whenever it is not coherent. Starting from a state of arbitrary incoherence and moving iteratively in one of many pro-coherence directions produced by whatever whacky mind you currently have isn’t obviously guaranteed to increasingly approximate maximization of some sensical utility function. For instance, take an entity with a cycle of preferences, apples &gt; bananas = oranges &gt; pears &gt; apples. The entity notices that it sometimes treats oranges as better than pears and sometimes worse. It tries to correct by adjusting the value of oranges to be the same as pears. The new utility function is exactly as incoherent as the old one. Probably moves like this are rarer than ones that make you more coherent in this situation, but I don’t know, and I also don’t know if this is a great model of the situation for incoherent systems that could become more coherent.</p>

<p><strong><em>What it might look like if this gap matters: </em></strong><em>AI systems proliferate, and have various goals. Some AI systems try to make money in the stock market. Some make movies. Some try to direct traffic optimally. Some try to make the Democratic party win an election. Some try to make Walmart maximally profitable. These systems have no perceptible desire to optimize the universe for forwarding these goals because they aren’t maximizing a general utility function, they are more ‘behaving like someone who is trying to make Walmart profitable’. They make strategic plans and think about their comparative advantage and forecast business dynamics, but they don’t build nanotechnology to manipulate everybody’s brains, because that’s not the kind of behavior pattern they were designed to follow. The world looks kind of like the current world, in that it is fairly non-obvious what any entity’s ‘utility function’ is. It often looks like AI systems are ‘trying’ to do things, but there’s no reason to think that they are enacting a rational and consistent plan, and they rarely do anything shocking or galaxy-brained.</em></p>

<h4>Ambiguously strong forces for goal-directedness need to meet an ambiguously high bar to cause a risk</h4>

<p>The forces for goal-directedness mentioned in I are presumably of finite strength. For instance, if coherence arguments correspond to pressure for machines to become more like utility maximizers, there is an empirical answer to how fast that would happen with a given system. There is also an empirical answer to how ‘much’ goal directedness is needed to bring about disaster, supposing that utility maximization would bring about disaster and, say, being a rock wouldn’t. Without investigating these empirical details, it is unclear whether a particular qualitatively identified force for goal-directedness will cause disaster within a particular time.</p>

<p><strong><em>What it might look like if this gap matters: </em></strong><em>There are not that many systems doing something like utility maximization in the new AI economy. Demand is mostly for systems more like GPT or DALL-E, which transform inputs in some known way without reference to the world, rather than ‘trying’ to bring about an outcome. Maybe the world was headed for more of the latter, but ethical and safety concerns reduced desire for it, and it wasn’t that hard to do something else. Companies setting out to make non-agentic AI systems have no trouble doing so. Incoherent AIs are never observed making themselves more coherent, and training has never produced an agent unexpectedly. There are lots of vaguely agentic things, but they don’t pose much of a problem. There are a few things at least as agentic as humans, but they are a small part of the economy.</em></p>

<h3><strong><em>B. Contra “goal-directed AI systems’ goals will be bad”</em></strong></h3>

<h4>Small differences in utility functions may not be catastrophic</h4>

<p>Arguably, humans are likely to have somewhat different values to one another even after arbitrary reflection. If so, there is some extended region of the space of possible values that the values of different humans fall within. That is, ‘human values’ is not a single point.</p>

<p>If the values of misaligned AI systems fall within that region, this would not appear to be worse in expectation than the situation where the long-run future was determined by the values of humans other than you. (This may still be a huge loss of value relative to the alternative, if a future determined by your own values is vastly better than that chosen by a different human, and if you also expected to get some small fraction of the future, and will now get much less. These conditions seem non-obvious however, and if they obtain you should worry about more general problems than AI.)</p>

<p>Plausibly even a single human, after reflecting, could on their own come to different places in a whole region of specific values, depending on somewhat arbitrary features of how the reflecting period went. In that case, even the values-on-reflection of a single human is an extended region of values space, and an AI which is only slightly misaligned could be the same as some version of you after reflecting.</p>

<p>There is a further larger region, ‘that which can be reliably enough aligned with typical human values via incentives in the environment’, which is arguably larger than the circle containing most human values. Human society makes use of this a lot: for instance, most of the time particularly evil humans don’t do anything too objectionable because it isn’t in their interests. This region is probably smaller for more capable creatures such as advanced AIs, but still it is some size.</p>

<p>Thus it seems that some amount<span id="easy-footnote-4-3345" class="easy-footnote-margin-adjust"></span><span class="easy-footnote"><a href="#easy-footnote-bottom-4-3345" title="We are talking about divergence in a poorly specified multi-dimensional space, so it isn’t going to be a fixed distance in every direction from the ideal point. It could theoretically be zero distance on some dimensions, such that if AI was misaligned at all in those directions it was catastrophic. My point here is merely that there is some area larger than a point."><sup>4</sup></a></span> of AI divergence from your own values is probably broadly fine, i.e. not worse than what you should otherwise expect without AI. </p>

<p>Thus in order to arrive at a conclusion of doom, it is not enough to argue that we cannot align AI perfectly. The question is a quantitative one of whether we can get it close enough. And how close is ‘close enough’ is not known. </p>

<p><strong><em>What it might look like if this gap matters: </em></strong><em>there are many superintelligent goal-directed AI systems around. They are trained to have human-like goals, but we know that their training is imperfect and none of them has goals exactly like those presented in training. However if you just heard about a particular system’s intentions, you wouldn’t be able to guess if it was an AI or a human. Things happen much faster than they were, because superintelligent AI is superintelligent, but not obviously in a direction less broadly in line with human goals than when humans were in charge.</em></p>

<h4>Differences between AI and human values may be small </h4>

<p>AI trained to have human-like goals will have something close to human-like goals. How close? Call it <em>d</em>, for a particular occasion of training AI. </p>

<p>If <em>d</em> doesn’t have to be 0 for safety (from above), then there is a question of whether it is an acceptable size. </p>

<p>I know of two issues here, pushing <em>d</em> upward. One is that with a finite number of training examples, the fit between the true function and the learned function will be wrong. <a href="https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks">The other</a> is that you might accidentally create a monster (‘misaligned <a href="https://www.alignmentforum.org/tag/mesa-optimization">mesaoptimizer</a>’) who understands its situation and pretends to have the utility function you are aiming for so that it can be freed and go out and manifest its own utility function, which could be just about anything. If this problem is real, then the values of an AI system might be arbitrarily different from the training values, rather than ‘nearby’ in some sense, so <em>d</em> is probably unacceptably large. But if you avoid creating such mesaoptimizers, then it seems plausible to me that <em>d</em> is very small. </p>

<p>If humans also substantially learn their values via observing examples, then the variation in human values is arising from a similar process, so might be expected to be of a similar scale. If we care to make the ML training process more accurate than the human learning one, it seems likely that we could. For instance, <em>d</em> gets smaller with more data.</p>

<p>Another line of evidence is that for things that I have seen AI learn so far, the distance from the real thing is intuitively small. If AI learns my values as well as it learns what faces look like, it seems plausible that it carries them out better than I do.</p>

<p>As minor additional evidence here, I don’t know how to describe any slight differences in utility functions that are catastrophic. Talking concretely, what does a utility function look like that is so close to a human utility function that an AI system has it after a bunch of training, but which is an absolute disaster? Are we talking about the scenario where the AI values a slightly different concept of justice, or values satisfaction a smidgen more relative to joy than it should? And then that’s a moral disaster because it is wrought across the cosmos? Or is it that it looks at all of our inaction and thinks we want stuff to be maintained very similar to how it is now, so crushes any efforts to improve things? </p>

<p><strong><em>What it might look like if this gap matters:</em></strong><em> when we try to train AI systems to care about what specific humans care about, they usually pretty much do, as far as we can tell. We basically get what we trained for. For instance, it is hard to distinguish them from the human in question. (It is still important to actually do this training, rather than making AI systems not trained to have human values.)</em></p>

<h4>Maybe value isn’t fragile</h4>

<p>Eliezer argued that <a href="https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile">value is fragile</a>, via examples of ‘just one thing’ that you can leave out of a utility function, and end up with something very far away from what humans want. For instance, if you leave out ‘boredom’ then he thinks the preferred future might look like repeating the same otherwise perfect moment again and again. (His argument is perhaps longer—that post says there is a lot of important background, though the bits mentioned don’t sound relevant to my disagreement.) This sounds to me like ‘value is not resilient to having components of it moved to zero’, which is a weird usage of ‘fragile’, and in particular, doesn’t seem to imply much about smaller perturbations. And smaller perturbations seem like the relevant thing with AI systems trained on a bunch of data to mimic something. </p>

<p>You could very analogously say ‘human faces are fragile’ because if you just leave out the nose it suddenly doesn’t look like a typical human face at all. Sure, but is that the kind of error you get when you try to train ML systems to mimic human faces? Almost none of the faces on <a href="http://thispersondoesnotexist.com">thispersondoesnotexist.com</a> are blatantly morphologically unusual in any way, let alone noseless. Admittedly one time I saw someone whose face was neon green goo, but I’m guessing you can get the rate of that down pretty low if you care about it.</p>

<p>Eight examples, no cherry-picking:</p>

<figure class="wp-block-image is-resized">
  <p><img src="https://lh6.googleusercontent.com/0vj-NpCiLoEKF3mXiDQHUE4mVQro_b1b6R3TxIh85ulPXlZ9jhgAPRb5IvU8tqDjM8dCdZe5vVERjjX0Y1-cAky03TErEwdlibOHJ7DbO0-zFjk3r0iM8GM8sNQHaIRDxDioNdLHDvisRtC9SrhHV23CuzUptkTbQwAoDldouCpqmq0XFGuAxovePQ" alt="" width="200" height="200" /><figure class="wp-block-image is-resized"><img loading="lazy" src="https://lh3.googleusercontent.com/Tuqy_SobcKiClmzWwtzL6jju5KJLaPa_8ZJKHsfgiNWumTPSBJNUYqvtAnck4u4DaQXQF2Vi9sZQXaDZ6aTHdVhrQWL-0gxygEYtA8PAOym1pNTknN0rGPKzcSYRx7heu2rg11lHJC-4o6-3o018H2Lu7FKK9rQgFuB9ar3g2ZbXQqp16InN-faonA" alt="" width="200" height="200" /><figure class="wp-block-image is-resized"><img loading="lazy" src="https://lh4.googleusercontent.com/0SSkt7jhDtDatv-YUQGq-01B_jsmEGAaS2iGg9tf12Ar6IYT0yYRbLm27p88DdpcI1DRMvGZzoV8IfLLoABdMok0ZMI1213ixZWkv-jws8tSoWBNzjK4ntGlIkDHRAcSnAe6-0Xhn1hCOK0sA9nPqbMZ6wP0YBVak6qgllOAbrhJ47kiO0b0u2amKw" alt="" width="200" height="200" /><figure class="wp-block-image is-resized"><img loading="lazy" src="https://lh4.googleusercontent.com/fZvZIY6hu_PhsIQEf5J24IlLk6LaUHVYblUMWdc39RxTY37QGKEh8mNiYWv03oMDNDq7S8eHqnPgOVoW5BGHXw_TP9OiKoA1riOl7NUWwpsinJjIRpfHAkzTxQHt-KY0SLY2O_Kta1qGEt07XeIWCldhYD6ZvoBrIOO-qyDmMom7HjWzdqTLT6BPPg" alt="" width="200" height="200" /><figure class="wp-block-image is-resized"><img loading="lazy" src="https://lh3.googleusercontent.com/uor40snO97cdBPci-mXbA3dJ8NPe0VE7CiIERY5i26B0I8tgtqOCuYt0VRt5OyP8H1PYOIfaAEqHKwAaRlQdETXj5-6YiO30EKJgPlsxxbduTGQMRSz0bGSfiBZyZwHBG3jhXOzaHqg7N2jTI0YiTRQG_AVtgGDdkJpH9of6ykEVJKHicuz-Z9Bwhg" alt="" width="200" height="200" /><figure class="wp-block-image is-resized"><img loading="lazy" src="https://lh6.googleusercontent.com/WbZs_lAmdUCwg6ZKO7mi5U3VPOXhezNlioBe7Gw5UYzY32ZTryJnkaCY35N05T72z_uyU_f1hv0OWxh8UrF2oTVPMRQbi647UzYXir0-4R1_QX0qcN7vPxByvcIgifB_1peLnwoyvLQCmdABo2v8_66-eckFxMbswhasGcUiDRI5G7wV3337CEp03A" alt="" width="200" height="200" /><figure class="wp-block-image is-resized"><img loading="lazy" src="https://lh4.googleusercontent.com/Iu6OOr13ntHCmLC0EXTc4Sh_nCwgSSb67Ffq75SwMR3l-r606IqXBbUQsQj1rT7UrhLgL10I7p8QzXGuJx5nv3pkp0yLURgI1yCaHmHrD-RHxjkZUxQOrQzfgennYXR54kreIN_wn15GtxmcEPjLtFx-VHqZl59nCA1CXhHJPQrtjpGg1aIfvcSdVw" alt="" width="200" height="200" /><figure class="wp-block-image is-resized"><img loading="lazy" src="https://lh5.googleusercontent.com/mb3BSXp0EmzWZ7Xeiu_0NzeKeWR66fvYHhaG_zDo7QPPGKwZR9WALKHpMTSXx5JHUl8ygFgM4FSTyXYKcqYD60uyCWGNJa77hPzVON0gr8yPA8P0vzSlSIJv42G3Ydd_qwWGHZP9_GUHm45JBEqeaOM2dQyq4RXDWHboHmyVRvWDbTputoxjS5cZcg" alt="" width="200" height="200" /></figure></figure></figure></figure></figure></figure></figure></p>

  <p>Skipping the nose is the kind of mistake you make if you are a child drawing a face from memory. Skipping ‘boredom’ is the kind of mistake you make if you are a person trying to write down human values from memory. My guess is that this seemed closer to the plan in 2009 when that post was written, and that people cached the takeaway and haven’t updated it for deep learning which can learn what faces look like better than you can.</p>

  <p><strong><em>What it might look like if this gap matters:</em></strong><em> there is a large region ‘around’ my values in value space that is also pretty good according to me. AI easily lands within that space, and eventually creates some world that is about as good as the best possible utopia, according to me. There aren’t a lot of really crazy and terrible value systems adjacent to my values.</em></p>

  <h4>Short-term goals</h4>

  <p>Utility maximization really only incentivises drastically altering the universe if one’s utility function places a high enough value on very temporally distant outcomes relative to near ones. That is, long term goals are needed for danger. A person who cares most about winning the timed chess game in front of them should not spend time accruing resources to invest in better chess-playing.</p>

  <p>AI systems could have long-term goals via people intentionally training them to do so, or via long-term goals naturally arising from systems not trained so. </p>

  <p>Humans seem to discount the future a lot in their usual decision-making (they have goals years in advance but rarely a hundred years) so the economic incentive to train AI to have very long term goals might be limited.</p>

  <p>It’s not clear that training for relatively short term goals naturally produces creatures with very long term goals, though it might.</p>

  <p>Thus if AI systems fail to have value systems relatively similar to human values, it is not clear that many will have the long time horizons needed to motivate taking over the universe.</p>

  <p><strong><em>What it might look like if this gap matters:</em></strong><em> the world is full of agents who care about relatively near-term issues, and are helpful to that end, and have no incentive to make long-term large scale schemes. Reminiscent of the current world, but with cleverer short-termism.</em></p>

  <h3><strong><em>C. Contra “superhuman AI would be sufficiently superior to humans to overpower humanity”</em></strong></h3>

  <h4>Human success isn’t from individual intelligence</h4>

  <p>The argument claims (or assumes) that surpassing ‘human-level’ intelligence (i.e. the mental capacities of an individual human) is the relevant bar for matching the power-gaining capacity of humans, such that passing this bar in individual intellect means outcompeting humans in general in terms of power (argument III.2), if not being able to immediately destroy them all outright (argument III.1.). In a similar vein, introductions to AI risk often start by saying that humanity has triumphed over the other species because it is more intelligent, as a lead in to saying that if we make something more intelligent still, it will inexorably triumph over humanity.</p>

  <p>This hypothesis about the provenance of human triumph seems wrong. Intellect surely helps, but humans look to be powerful largely because they share their meager intellectual discoveries with one another and consequently save them up over time<span id="easy-footnote-5-3345" class="easy-footnote-margin-adjust"></span><span class="easy-footnote"><a href="#easy-footnote-bottom-5-3345" title="&lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Secret-Our-Success-Evolution-Domesticating/dp/0691166854&quot;&gt;The Secrets of Our Success&lt;/a&gt;&lt;/em&gt; seems to be the canonical reference for this, but I haven’t read it. I don’t know how controversial this is, but also don’t presently see how it could fail to be true."><sup>5</sup></a></span>. You can see this starkly by comparing the material situation of Alice, a genius living in the stone age, and Bob, an average person living in 21st Century America. Alice might struggle all day to get a pot of water, while Bob might be able to summon all manner of delicious drinks from across the oceans, along with furniture, electronics, information, etc. Much of Bob’s power probably did flow from the application of intelligence, but not Bob’s individual intelligence. Alice’s intelligence, and that of those who came between them.</p>

  <p>Bob’s greater power isn’t directly just from the knowledge and artifacts Bob inherits from other humans. He also seems to be helped for instance by much better coordination: both from a larger number people coordinating together, and from better infrastructure for that coordination (e.g. for Alice the height of coordination might be an occasional big multi-tribe meeting with trade, and for Bob it includes global instant messaging and banking systems and the Internet). One might attribute all of this ultimately to innovation, and thus to intelligence and communication, or not. I think it’s not important to sort out here, as long as it’s clear that individual intelligence isn’t the source of power.</p>

  <p>It could still be that with a given bounty of shared knowledge (e.g. within a given society), intelligence grants huge advantages. But even that doesn’t look true here: 21st Century geniuses live basically like 21st Century people of average intelligence, give or take<span id="easy-footnote-6-3345" class="easy-footnote-margin-adjust"></span><span class="easy-footnote"><a href="#easy-footnote-bottom-6-3345" title="See section &amp;#8216;Intelligence may not be an overwhelming advantage&amp;#8217;"><sup>6</sup></a></span>.</p>

  <p>Why does this matter? Well for one thing, if you make AI which is merely as smart as a human, you shouldn’t then expect it to do that much better than a genius living in the stone age. That’s what human-level intelligence gets you: nearly nothing. <a href="https://meteuphoric.com/2017/12/28/why-did-everything-take-so-long/">A piece of rope</a> after millions of lifetimes. Humans without their culture are much like other animals. </p>

  <p>To wield the control-over-the-world of a genius living in the 21st Century, the human-level AI would seem to need something like the other benefits that the 21st century genius gets from their situation in connection with a society. </p>

  <p>One such thing is access to humanity’s shared stock of hard-won information. AI systems plausibly do have this, if they can get most of what is relevant by reading the internet. This isn’t obvious: people also inherit information from society through copying habits and customs, learning directly from other people, and receiving artifacts with implicit information (for instance, a factory allows whoever owns the factory to make use of intellectual work that was done by the people who built the factory, but that information may not available explicitly even for the owner of the factory, let alone to readers on the internet). These sources of information seem likely to also be available to AI systems though, at least if they are afforded the same options as humans.</p>

  <p>My best guess is that AI systems easily do better than humans on extracting information from humanity’s stockpile, and on coordinating, and so on this account are probably in an even better position to compete with humans than one might think on the individual intelligence model, but that is a guess. In that case perhaps this misunderstanding makes little difference to the outcomes of the argument. However it seems at least a bit more complicated. </p>

  <p>Suppose that AI systems can have access to all information humans can have access to. The power the 21st century person gains from their society is modulated by their role in society, and relationships, and rights, and the affordances society allows them as a result. Their power will vary enormously depending on whether they are employed, or listened to, or paid, or a citizen, or the president. If AI systems’ power stems substantially from interacting with society, then their power will also depend on affordances granted, and humans may choose not to grant them many affordances (see section ‘Intelligence may not be an overwhelming advantage’ for more discussion).</p>

  <p>However suppose that your new genius AI system is also treated with all privilege. The next way that this alternate model matters is that if most of what is good in a person’s life is determined by the society they are part of, and their own labor is just buying them a tiny piece of that inheritance, then if they are for instance twice as smart as any other human, they don’t get to use technology that it twice as good. They just get a larger piece of that same shared technological bounty purchasable by anyone. Because each individual person is adding essentially nothing in terms of technology, so twice that is still basically nothing. </p>

  <p>In contrast, I think people are often imagining that a single entity somewhat smarter than a human will be able to quickly use technologies that are somewhat better than current human technologies. This seems to be mistaking the actions of a human and the actions of a human society. If <a href="https://en.wikipedia.org/wiki/Manhattan_Project#Personnel">a hundred thousand people sometimes get together for a few years and make fantastic new weapons</a>, you should not expect an entity somewhat smarter than a person to make even better weapons. That’s off by a factor of about a hundred thousand. </p>

  <p>There might be places you can get far ahead of humanity by being better than a single human—it depends how much accomplishments depend on the few most capable humans in the field, and how few people are working on the problem<span id="easy-footnote-7-3345" class="easy-footnote-margin-adjust"></span><span class="easy-footnote"><a href="#easy-footnote-bottom-7-3345" title="E.g. for the metric ‘hardness of math problem solvable’, maybe no human can solve a level 10 math problem, but several can solve 9s. Then human society as a whole also can’t solve a 10. So the first AI that can is only mildly surpassing the best human, but is at the same time surpassing all of human society."><sup>7</sup></a></span>. But for instance the Manhattan Project <a href="https://en.wikipedia.org/wiki/Manhattan_Project#Personnel">took</a> a hundred thousand people several years, and von Neumann (a mythically smart scientist) joining the project did not reduce it to an afternoon. Plausibly to me, some specific people being on the project caused it to not take twice as many person-years, though the plausible candidates here seem to be more in the business of running things than doing science directly (though that also presumably involves intelligence). But even if you are an ambitious somewhat superhuman intelligence, the influence available to you seems to plausibly be limited to making a large dent in the effort required for some particular research endeavor, not single-handedly outmoding humans across many research endeavors.</p>

  <p>This is all reason to doubt that a small number of superhuman intelligences will rapidly take over or destroy the world (as in III.i.). This doesn’t preclude a set of AI systems that are together more capable than a large number of people from making great progress. However some related issues seem to make that less likely.</p>

  <p>Another implication of this model is that if most human power comes from buying access to society’s shared power, i.e. interacting with the economy, you should expect intellectual labor by AI systems to usually be sold, rather than for instance put toward a private stock of knowledge. This means the intellectual outputs are mostly going to society, and the main source of potential power to an AI system is the wages received (which may allow it to gain power in the long run). However it seems quite plausible that AI systems at this stage will generally not receive wages, since they presumably do not need them to be motivated to do the work they were trained for. It also seems plausible that they would be owned and run by humans. This would seem to not involve any transfer of power to that AI system, except insofar as its intellectual outputs benefit it (e.g. if it is writing advertising material, maybe it doesn’t get paid for that, but if it can write material that slightly furthers its own goals in the world while also fulfilling the advertising requirements, then it sneaked in some influence.) </p>

  <p>If there is AI which is moderately more competent than humans, but not sufficiently more competent to take over the world, then it is likely to contribute to this stock of knowledge and affordances shared with humans. There is no reason to expect it to build a separate competing stock, any more than there is reason for a current human household to try to build a separate competing stock rather than sell their labor to others in the economy. </p>

  <p>In summary:</p>

  <ol><li>Functional connection with a large community of other intelligences in the past and present is probably a much bigger factor in the success of humans as a species or individual humans than is individual intelligence.&nbsp;</li><li>Thus this also seems more likely to be important for AI success than individual intelligence. This is contrary to a usual argument for AI superiority, but probably leaves AI systems at least as likely to outperform humans, since superhuman AI is probably superhumanly good at taking in information and coordinating.</li><li>However it is not obvious that AI systems will have the same access to society’s accumulated information e.g. if there is information which humans learn from living in society, rather than from reading the internet.&nbsp;</li><li>And it seems an open question whether AI systems are given the same affordances in society as humans, which also seem important to making use of the accrued bounty of power over the world that humans have. For instance, if they are not granted the same legal rights as humans, they may be at a disadvantage in doing trade or engaging in politics or accruing power.</li><li>The fruits of greater intelligence for an entity will probably not look like society-level accomplishments unless it is a society-scale entity</li><li>The route to influence with smaller fruits probably by default looks like participating in the economy rather than trying to build a private stock of knowledge.</li><li>If the resources from participating in the economy accrue to the owners of AI systems, not to the systems themselves, then there is less reason to expect the systems to accrue power incrementally, and they are at a severe disadvantage relative to humans.&nbsp;</li></ol>

  <p>Overall these are reasons to expect AI systems with around human-level cognitive performance to not destroy the world immediately, and to not amass power as easily as one might imagine. </p>

  <p><strong><em>What it might look like if this gap matters:</em></strong><em> If AI systems are somewhat superhuman, then they do impressive cognitive work, and each contributes to technology more than the best human geniuses, but not more than the whole of society, and not enough to materially improve their own affordances. They don’t gain power rapidly because they are disadvantaged in other ways, e.g. by lack of information, lack of rights, lack of access to positions of power. Their work is sold and used by many actors, and the proceeds go to their human owners. AI systems do not generally end up with access to masses of technology that others do not have access to, and nor do they have private fortunes. In the long run, as they become more powerful, they might take power if other aspects of the situation don’t change. </em></p>

  <h4>AI agents may not be radically superior to combinations of humans and non-agentic machines</h4>

  <p>‘Human level capability’ is a moving target. For comparing the competence of advanced AI systems to humans, the relevant comparison is with humans who have state-of-the-art AI and other tools. For instance, the human capacity to make art quickly has recently been improved by a variety of AI art systems. If there were now an agentic AI system that made art, it would make art much faster than a human of 2015, but perhaps hardly faster than a human of late 2022. If humans continually have access to tool versions of AI capabilities, it is not clear that agentic AI systems must ever have an overwhelmingly large capability advantage for important tasks (though they might). </p>

  <p>(This is not an argument that humans might be better than AI systems, but rather: if the gap in capability is smaller, then the pressure for AI systems to accrue power is less and thus loss of human control is slower and easier to mitigate entirely through other forces, such as subsidizing human involvement or disadvantaging AI systems in the economy.)</p>

  <p>Some advantages of being an agentic AI system vs. a human with a tool AI system seem to be:</p>

  <ol><li>There might just not be an equivalent tool system, for instance if it is impossible to train systems without producing emergent agents.</li><li>When every part of a process takes into account the final goal, this should make the choices within the task more apt for the final goal (and agents know their final goal, whereas tools carrying out parts of a larger problem do not).</li><li>For humans, the interface for using a capability of one’s mind tends to be smoother than the interface for using a tool. For instance a person who can do fast mental multiplication can do this more smoothly and use it more often than a person who needs to get out a calculator. This seems likely to persist.</li></ol>

  <p>1 and 2 may or may not matter much. 3 matters more for brief, fast, unimportant tasks. For instance, consider again people who can do mental calculations better than others. My guess is that this advantages them at using Fermi estimates in their lives and buying cheaper groceries, but does not make them materially better at making large financial choices well. For a one-off large financial choice, the effort of getting out a calculator is worth it and the delay is very short compared to the length of the activity. The same seems likely true of humans with tools vs. agentic AI with the same capacities integrated into their minds. Conceivably the gap between humans with tools and goal-directed AI is small for large, important tasks.</p>

  <p><strong><em>What it might look like if this gap matters:</em></strong><em> agentic AI systems have substantial advantages over humans with tools at some tasks like rapid interaction with humans, and responding to rapidly evolving strategic situations.  One-off large important tasks such as advanced science are mostly done by tool AI. </em></p>

  <h4>Trust</h4>

  <p>If goal-directed AI systems are only mildly more competent than some combination of tool systems and humans (as suggested by considerations in the last two sections), we still might expect AI systems to out-compete humans, just more slowly. However AI systems have one serious disadvantage as employees of humans: they are intrinsically untrustworthy, while we don’t understand them well enough to be clear on what their values are or how they will behave in any given case. Even if they did perform as well as humans at some task, if humans can’t be certain of that, then there is reason to disprefer using them. This can be thought of as two problems: firstly, slightly misaligned systems are less valuable because they genuinely do the thing you want less well, and secondly, even if they were not misaligned, if humans can’t know that (because we have no good way to verify the alignment of AI systems) then it is costly in expectation to use them. (This is only a further force acting against the supremacy of AI systems—they might still be powerful enough that using them is enough of an advantage that it is worth taking the hit on trustworthiness.)</p>

  <p><strong><em>What it might look like if this gap matters: </em></strong><em>in places where goal-directed AI systems are not typically hugely better than some combination of less goal-directed systems and humans, the job is often given to the latter if trustworthiness matters. </em></p>

  <h4>Headroom</h4>

  <p>For AI to vastly surpass human performance at a task, there needs to be ample room for improvement above human level. For some tasks, there is not—tic-tac-toe is a classic example. It is not clear how close humans (or technologically aided humans) are from the limits to competence in the particular domains that will matter. It is to my knowledge an open question how much ‘headroom’ there is. My guess is a lot, but it isn’t obvious.</p>

  <p>How much headroom there is varies by task. Categories of task for which there appears to be little headroom: </p>

  <ol><li>Tasks where we know what the best performance looks like, and humans can get close to it. For instance, machines cannot win more often than the best humans at Tic-tac-toe (playing within the rules) or solve Rubik’s cubes much more reliably, or extracting calories from fuel</li><li>Tasks where humans are already be reaping most of the value—for instance, perhaps most of the value of forks is in having a handle with prongs attached to the end, and while humans continue to design slightly better ones, and machines might be able to add marginal value to that project more than twice as fast as the human designers, they cannot perform twice as well in terms of the value of each fork, because forks are already 95% as good as they can be.&nbsp;</li><li>Better performance is quickly intractable. For instance, we know that for tasks in particular complexity classes, there are computational limits to how well one can perform across the board. Or for chaotic systems, there can be limits to predictability. (That is, tasks might lack headroom not because they are simple, but because they are complex. E.g. AI probably can’t predict the weather much further out than humans.)</li></ol>

  <p>Categories of task where a lot of headroom seems likely:</p>

  <ol start="4"><li>Competitive tasks where the value of a certain level of performance depends on whether one is better or worse than one’s opponent, so that the marginal value of more performance doesn’t hit diminishing returns, as long as your opponent keeps competing and taking back what you just won. Though in one way this is like having little headroom: there’s no more value to be had—the game is zero sum. And while there might often be a lot of value to be gained by doing a bit better on the margin, still if all sides can invest, then nobody will end up better off than they were. So whether this seems more like high or low headroom depends on what we are asking exactly. Here we are asking if AI systems can do much better than humans: in a zero sum contest like this, they likely can in the sense that they can beat humans, but not in the sense of reaping anything more from the situation than the humans ever got.</li><li>Tasks where it is twice as good to do the same task twice as fast, and where speed is bottlenecked on thinking time.</li><li>Tasks where there is reason to think that optimal performance is radically better than we have seen. For instance, perhaps we can estimate how high Chess Elo rankings must go before reaching perfection by reasoning theoretically about the game, and perhaps it is very high (I don’t know).</li><li>Tasks where humans appear to use very inefficient methods. For instance, it was perhaps predictable before calculators that they would be able to do mathematics much faster than humans, because humans can only keep a small number of digits in their heads, which doesn’t seem like an intrinsically hard problem. Similarly, I hear humans often use mental machinery designed for one mental activity for fairly different ones, through analogy.<span id="easy-footnote-8-3345" class="easy-footnote-margin-adjust"></span><span class="easy-footnote"><a href="#easy-footnote-bottom-8-3345" title="Probably I have this impression from reading Steven Pinker at some point."><sup>8</sup></a></span> For instance, when I think about macroeconomics, I seem to be basically using my intuitions for dealing with water. When I do mathematics in general, I think I’m probably using my mental capacities for imagining physical objects.</li></ol>

  <p><strong><em>What it might look like if this gap matters: </em></strong><em>many challenges in today’s world remain challenging for AI. Human behavior is not readily predictable or manipulable very far beyond what we have explored, only slightly more complicated schemes are feasible before the world’s uncertainties overwhelm planning; much better ads are soon met by much better immune responses; much better commercial decision-making ekes out some additional value across the board but most products were already fulfilling a lot of their potential; incredible virtual prosecutors meet incredible virtual defense attorneys and everything is as it was; there are a few rounds of attack-and-defense in various corporate strategies before a new equilibrium with broad recognition of those possibilities; conflicts and ‘social issues’ remain mostly intractable. There is a brief golden age of science before the newly low-hanging fruit are again plucked and it is only lightning fast in areas where thinking was the main bottleneck, e.g. not in medicine.</em></p>

  <h4>Intelligence may not be an overwhelming advantage</h4>

  <p>Intelligence is helpful for accruing power and resources, all things equal, but many other things are helpful too. For instance money, social standing, allies, evident trustworthiness, not being discriminated against (this was slightly discussed in section ‘Human success isn’t from individual intelligence’). AI systems are not guaranteed to have those in abundance. The argument assumes that any difference in intelligence in particular will eventually win out over any differences in other initial resources. I don’t know of reason to think that. </p>

  <p>Empirical evidence does not seem to support the idea that cognitive ability is a large factor in success. Situations where one entity is much smarter or more broadly mentally competent than other entities regularly occur without the smarter one taking control over the other:</p>

  <ol><li>Species exist with all levels of intelligence. Elephants have not in any sense won over gnats; they do not rule gnats; they do not have obviously more control than gnats over the environment. </li><li>Competence does not seem to aggressively overwhelm other advantages in humans: <ol><li>Looking at the world, intuitively the big discrepancies in power are not seemingly about intelligence.</li><li>IQ 130 humans <a href="https://www.newscientist.com/article/dn11711-smarter-people-are-no-better-off/">apparently</a> earn very roughly $6000-$18,500 per year more than average IQ humans.</li><li>Elected representatives are apparently smarter on average, but it is a slightly shifted curve, <a href="http://perseus.iies.su.se/~tpers/papers/Draft170103.pdf">not a radically difference</a>.</li><li>MENSA isn’t a major force in the world.</li><li>Many places where people see huge success through being cognitively able are ones where they show off their intelligence to impress people, rather than actually using it for decision-making. For instance, writers, actors, song-writers, comedians, all sometimes become very successful through cognitive skills. Whereas scientists, engineers and authors of software use cognitive skills to make choices about the world, and less often become extremely rich and famous, say. If intelligence were that useful for strategic action, it seems like using it for that would be at least as powerful as showing it off. But maybe this is just an accident of which fields have winner-takes-all type dynamics.</li><li>If we look at people who evidently have good cognitive abilities given their intellectual output, their personal lives are not obviously drastically more successful, anecdotally.</li><li>One might counter-counter-argue that humans are very similar to one another in capability, so even if intelligence matters much more than other traits, you won’t see that by looking at  the near-identical humans. This does not seem to be true. Often at least, the difference in performance between mediocre human performance and top level human performance is <a href="https://aiimpacts.org/category/speed-of-ai-transition/range-of-human-performance/">large</a>, relative to the space below, iirc. For instance, in chess, the Elo difference between the best and worst players is about 2000, whereas the difference between the amateur play and random play is <a href="https://chess.stackexchange.com/questions/6508/what-would-be-the-elo-of-a-computer-program-that-plays-at-random">maybe 400-2800 (if you accept Chess StackExchange guesses as a reasonable proxy for the truth here)</a>. And <a href="https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-chess/">in terms of AI progress</a>, amateur human play was reached in the 50s, roughly when research began, and world champion level play was reached in 1997. </li></ol></li></ol>

  <p>And theoretically I don’t know why one would expect greater intelligence to win out over other advantages over time.  There are actually two questionable theories here: 1) Charlotte having more overall control than David at time 0 means that Charlotte will tend to have an even greater share of control at time 1. And, 2) Charlotte having more <em>intelligence</em> than David at time 0 means that Charlotte will have a greater share of control at time 1 even if Bob has more overall control (i.e. more of other resources) at time 1.</p>

  <p><strong><em>What it might look like if this gap matters:</em></strong><em> there are many AI systems around, and they strive for various things. They don’t hold property, or vote, or get a weight in almost anyone’s decisions, or get paid, and are generally treated with suspicion. These things on net keep them from gaining very much power. They are very persuasive speakers however and we can’t stop them from communicating, so there is a constant risk of people willingly handing them power, in response to their moving claims that they are an oppressed minority who suffer. The main thing stopping them from winning is that their position as psychopaths bent on taking power for incredibly pointless ends is widely understood.</em></p>

  <h4>Unclear that many goals realistically incentivise taking over the universe</h4>

  <p>I have some goals. For instance, I want some good romance. My guess is that trying to take over the universe isn’t the best way to achieve this goal. The same goes for a lot of my goals, it seems to me. Possibly I’m in error, but I spend a lot of time pursuing goals, and very little of it trying to take over the universe. Whether a particular goal is best forwarded by trying to take over the universe as a substep seems like a quantitative empirical question, to which the answer is virtually always ‘not remotely’. Don’t get me wrong: all of these goals involve some interest in taking over the universe. All things equal, if I could take over the universe for free, I do think it would help in my romantic pursuits. But taking over the universe is not free. It’s actually super duper duper expensive and hard. So for most goals arising, it doesn’t bear considering. The idea of taking over the universe as a substep is entirely laughable for almost any human goal.</p>

  <p>So why do we think that AI goals are different? I think the thought is that it’s radically easier for AI systems to take over the world, because all they have to do is to annihilate humanity, and they are way better positioned to do that than I am, and also better positioned to survive the death of human civilization than I am. I agree that it is likely easier, but how much easier? So much easier to take it from ‘laughably unhelpful’ to ‘obviously always the best move’? This is another quantitative empirical question.</p>

  <p><strong><em>What it might look like if this gap matters: </em></strong><em>Superintelligent AI systems pursue their goals. Often they achieve them fairly well. This is somewhat contrary to ideal human thriving, but not lethal. For instance, some AI systems are trying to maximize Amazon’s market share, within broad legality. Everyone buys truly incredible amounts of stuff from Amazon, and people often wonder if it is too much stuff. At no point does attempting to murder all humans seem like the best strategy for this. </em></p>

  <h4>Quantity of new cognitive labor is an empirical question, not addressed</h4>

  <p>Whether some set of AI systems can take over the world with their new intelligence probably depends how much total cognitive labor they represent. For instance, if they are in total slightly more capable than von Neumann, they probably can’t take over the world. If they are together as capable (in some sense) as a million 21st Century human civilizations, then they probably can (at least in the 21st Century).</p>

  <p>It also matters how much of that is goal-directed at all, and highly intelligent, and how much of that is directed at achieving the AI systems’ own goals rather than those we intended them for, and how much of that is directed at taking over the world. </p>

  <p>If we continued to build hardware, presumably at some point AI systems would account for most of the cognitive labor in the world. But if there is first an extended period of more minimal advanced AI presence, that would probably prevent an immediate death outcome, and improve humanity’s prospects for controlling a slow-moving AI power grab. </p>

  <p><strong><em>What it might look like if this gap matters: </em></strong><em>when advanced AI is developed, there is a lot of new cognitive labor in the world, but it is a minuscule fraction of all of the cognitive labor in the world. A large part of it is not goal-directed at all, and of that, most of the new AI thought is applied to tasks it was intended for. Thus what part of it is spent on scheming to grab power for AI systems is too small to grab much power quickly. The amount of AI cognitive labor grows fast over time, and in several decades it is most of the cognitive labor, but humanity has had extensive experience dealing with its power grabbing.</em></p>

  <h4>Speed of intelligence growth is ambiguous</h4>

  <p>The idea that a superhuman AI would be able to rapidly destroy the world seems <em>prima facie</em> unlikely, since no other entity has ever done that. Two common broad arguments for it:</p>

  <ol><li>There will be a feedback loop in which intelligent AI makes more intelligent AI repeatedly until AI is very intelligent.</li><li>Very small differences in brains seem to correspond to very large differences in performance, based on observing humans and other apes. Thus any movement past human-level will take us to unimaginably superhuman level.</li></ol>

  <p>These both seem questionable.</p>

  <ol><li>Feedback loops can happen at very different rates. Identifying a feedback loop empirically does not signify an explosion of whatever you are looking at. For instance, technology is already helping improve technology. To get to a confident conclusion of doom, you need evidence that the feedback loop is fast.</li><li>It does not seem clear that small improvements in brains lead to large changes in intelligence in general, or will do on the relevant margin. Small differences between humans and other primates might include those helpful for communication (see Section &#8216;Human success isn&#8217;t from individual intelligence&#8217;), which do not seem relevant here. If there were a particularly powerful cognitive development between chimps and humans, it is unclear that AI researchers find that same insight at the same point in the process (rather than at some other time).&nbsp;</li></ol>

  <p>A large number of other arguments have been posed for expecting very fast growth in intelligence at around human level. I previously made <a href="https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/">a list of them with counterarguments</a>, though none seemed very compelling. Overall, I don’t know of strong reason to expect very fast growth in AI capabilities at around human-level AI performance, though I hear such arguments might exist. </p>

  <p><strong><em>What it would look like if this gap mattered: </em></strong><em>AI systems would at some point perform at around human level at various tasks, and would contribute to AI research, along with everything else. This would contribute to progress to an extent familiar from other technological progress feedback, and would not e.g. lead to a superintelligent AI system in minutes.</em></p>

  <h4>Key concepts are vague</h4>

  <p>Concepts such as ‘control’, ‘power’, and ‘alignment with human values’ all seem vague. ‘Control’ is not zero sum (as seemingly assumed) and is somewhat hard to pin down, I claim. What an ‘aligned’ entity is exactly seems to be contentious in the AI safety community, but I don’t know the details. My guess is that upon further probing, these conceptual issues are resolvable in a way that doesn’t endanger the argument, but I don’t know. I’m not going to go into this here.</p>

  <p><strong><em>What it might look like if this gap matters: </em></strong><em>upon thinking more, we realize that our concerns were confused. Things go fine with AI in ways that seem obvious in retrospect. This might look like it did for people concerned about the ‘population bomb’ or as it did for me in some of my youthful concerns about sustainability: there was a compelling abstract argument for a problem, and the reality didn’t fit the abstractions well enough to play out as predicted.</em></p>

  <h3><strong>D. Contra the whole argument</strong></h3>

  <h4>The argument overall proves too much about corporations</h4>

  <p>Here is the argument again, but modified to be about corporations. A couple of pieces don’t carry over, but they don’t seem integral.</p>

  <p><strong>I. Any given corporation is likely to be ‘goal-directed’</strong></p>

  <p>Reasons to expect this:</p>

  <ol start="4"><li>Goal-directed behavior is likely to be valuable in corporations, e.g. economically</li><li><s>Goal-directed entities may tend to arise from machine learning training processes not intending to create them (at least via the methods that are likely to be used).</s></li><li>‘Coherence arguments’ may imply that systems with some goal-directedness will become more strongly goal-directed over time.</li></ol>

  <p><strong>II. If goal-directed superhuman corporations are built, their desired outcomes will probably be about as bad as an empty universe by human lights</strong></p>

  <p>Reasons to expect this:</p>

  <ol start="4"><li>Finding useful goals that aren’t extinction-level bad appears to be hard: we don’t have a way to usefully point at human goals, and divergences from human goals seem likely to produce goals that are in intense conflict with human goals, due to a) most goals producing convergent incentives for controlling everything, and b) value being ‘fragile’, such that an entity with ‘similar’ values will generally create a future of virtually no value.&nbsp;</li><li>Finding goals that are extinction-level bad and temporarily useful appears to be easy: for example, corporations with the sole objective ‘maximize company revenue’ might profit for a time before gathering the influence and wherewithal to pursue the goal in ways that blatantly harm society.</li><li>Even if humanity found acceptable goals, giving a corporation any specific goals appears to be hard. We don’t know of any procedure to do it<s>, and we have theoretical reasons to expect that AI systems produced through machine learning training will generally end up with goals other than those that they were trained according to</s>. Randomly aberrant goals resulting are probably extinction-level bad, for reasons described in II.1 above.<br /></li></ol>

  <p><strong>III. If most goal-directed corporations have bad goals, the future will very likely be bad</strong></p>

  <p>That is, a set of ill-motivated goal-directed corporations, of a scale likely to occur, would be capable of taking control of the future from humans. This is supported by at least one of the following being true:</p>

  <ol><li><strong>A corporation would destroy humanity rapidly</strong>. This may be via ultra-powerful capabilities at e.g. technology design and strategic scheming, or through gaining such powers in an ‘intelligence explosion‘ (self-improvement cycle). Either of those things may happen either through exceptional heights of intelligence being reached or through highly destructive ideas being available to minds only mildly beyond our own.</li><li><strong>Superhuman AI would gradually come to control the future via accruing power and resources.</strong> Power and resources would be more available to the corporation than to humans on average, because of the corporation having far greater intelligence.</li></ol>

  <p>This argument does point at real issues with corporations, but we do not generally consider such issues existentially deadly. </p>

  <p>One might argue that there are defeating reasons that corporations do not destroy the world: they are made of humans so can be somewhat reined in; they are not smart enough; they are not coherent enough. But in that case, the original argument needs to make reference to these things, so that they apply to one and not the other.</p>

  <p><strong><em>What it might look like if this counterargument matters: </em></strong><em>something like the current world. There are large and powerful systems doing things vastly beyond the ability of individual humans, and acting in a definitively goal-directed way. We have a vague understanding of their goals, and do not assume that they are coherent. Their goals are clearly not aligned with human goals, but they have enough overlap that many people are broadly in favor of their existence. They seek power. This all causes some problems, but problems within the power of humans and other organized human groups to keep under control, for some definition of ‘under control’.</em></p>

  <h2><strong>Conclusion</strong></h2>

  <p>I think there are quite a few gaps in the argument, as I understand it. My current guess (prior to reviewing other arguments and integrating things carefully) is that enough uncertainties might resolve in the dangerous directions that existential risk from AI is a reasonable concern. I don’t at present though see how one would come to think it was overwhelmingly likely.</p>

  <p></p>
</figure>

  </description>
  <link>/2022/10/14/ai_counterargs.html</link>
  <guid isPermaLink="true">http://localhost:4000/2022/10/14/ai_counterargs.html</guid>
  <pubDate>Fri, 14 Oct 2022 05:43:00 -0700</pubDate>
 </item>
 
 <item>
  <title>A game of mattering</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>When I have an overwhelming number of things to do, and insufficient native urge to do them, I often arrange them into a kind of game for myself. The nature and appeal of this game has been relatively stable for about a year, after many years of evolution, so this seems like a reasonable time to share it. I also play it when I just want to structure my day and am in the mood for it. I currently play something like two or three times a week.</p>

<h1 id="the-game">The game</h1>

<p>The basic idea is to lay out the tasks in time a bit like obstacles in a platformer or steps in Dance Dance Revolution, then race through the obstacle course grabbing them under consistently high-but-doable time pressure.</p>

<p>Here’s how to play:</p>
<ol>
  <li>Draw a grid with as many rows as there are remaining hours in your hoped for productive day, and ~3 columns. Each box stands for a particular ~20 minute period (I sometimes play with 15m or 30m periods.)</li>
  <li>Lay out the gameboard: break the stuff you want to do into appropriate units, henceforth ‘items’. An item should fit comfortably in the length of a box, and it should be easy enough to verify completion. (This can be achieved through house rules such as ‘do x a tiny bit = do it until I have a sense that an appropriate tiny bit has been done’ as long as you are happy applying them). Space items out a decent amount so that the whole course is clearly feasible. Include everything you want to do in the day, including nice or relaxing things, or break activities. Drinks, snacks, tiny bouts of exercise, looking at news sites for 5 minutes, etc. Design the track thoughtfully, with hard bouts followed by relief before the next hard bout.</li>
  <li>To play, start in the first box, then move through the boxes according to the time of day. The goal in playing is to collect as many items as you can, as you are forced along the track by the passage of time. You can collect an item by doing the task in or before you get to the box it is in. If it isn’t done by the end of the box, it gets left behind. However if you clear any box entirely, you get to move one item anywhere on the gameboard. So you can rescue something from the past, or rearrange the future to make it more feasible, or if everything is perfect, you can add an entirely new item somewhere.<!--ex--></li>
</ol>

<p>I used to play this with tiny post-it stickers, which I would gather in a large moving pile, acting as a counter:</p>

<p style="text-align:center;">
<img src="https://hosting.photobucket.com/images/i/katjasgrace/PXL_20210915_214726176.jpg" alt="example of game with stickers" width="300" /> <img src="https://hosting.photobucket.com/images/i/katjasgrace/PXL_20210710_183723811.jpg" alt="example of game with stickers" width="300" />
</p>

<p>Now I just draw the whole thing. Crossed out = collected; [] = rescued from the past, now implicitly in the final box; dot in the lower right = box cleared; dot next to item = task done but item stuck in the past (can be collected immediately if rescued).</p>

<p style="text-align:center;">
<img src="https://hosting.photobucket.com/images/i/katjasgrace/IMG_0898.jpeg" alt="IMG_0898" width="500" />
</p>

<h1 id="why-is-this-good">Why is this good?</h1>

<p>I think a basic problem with working on a big pile of things in a big expanse of time is that if you work or not during any particular minute, it feels like it makes nearly no difference to the expectation of success. I’m not quite sure why this is—in fact if I don’t work this minute, I’m going to get one minute less work done. But it feels like if I don’t work this minute, I only need to work a smidgen faster on average to get any particular amount of work done, so what does it matter if I work now or later? And if i had some particular goal (e.g. finishing writing some massive text today), it’s unlikely that my other efforts will get me exactly to the line where this minute pushed me over—probably I will either succeed with hours to spare (haha) or fail hours from my goals.</p>

<p>I picture what’s going on as vaguely something like this—there is often some amount of work that is going to make your success likely, and if you know that you are on a locally steep part of the curve, it is more motivating than if you are either far away from the steep part or don’t know where you are:</p>

<p style="text-align:center;">
<img src="https://hosting.photobucket.com/images/i/katjasgrace/IMG_0951.HEIC" alt="IMG_0898" width="500" />
</p>

<p>Yet on the other hand, the appeal of various non-work activities this specific minute might be the most distinct and tangible things in the world. So when there is a lot to be done in a long time, not working often looks more exciting than working, even if a more rational accounting would disagree.</p>

<p>Having a single specific thing to do within minutes is much more compelling: the task and the time are lined up so that my action right now matters. Slacking this minute is the difference between success and failure.</p>

<p>It feels very different to have one email to deal with in three minutes and to have a thousand to deal with in next fifty hours.
<!-- In the latter case, while in some sense the time pressure is the same, I probably start by getting myself a drink and pottering around staring out of the window or something. --></p>

<p>One might naively respond to this issue by breaking up one’s tasks into tiny chunks, then laying them out in a day of tiny time boxes, then aiming for each to happen by the end of its allotment. But this will be terrible. A few boxes in, either you’ll be ahead or behind. And either way, your immediate actions have drifted away from feeling like they matter. If you are ahead, the pressure is off: you’ll probably succeed at the next increment whether or not you work hard now. If behind, you are definitely going to fail at doing the next box on time, and probably some others, and your present work is for an increased chance of catching up at some vague future box, much like before you had these boxes. (Plus your activities are no longer in line with what your plan was, which for me makes it tempting to scrap the whole thing and do something else.)
<!-- and can substitute with all of the other intervening time. So again, working less hard this minute means you have to work a tiny bit harder in the next five boxes say, to catch up then. So what you do in this minute doesn't viscerally change the odds much. --></p>

<p>A big innovation of this game is to instead ensure that you keep meeting tasks one at a time where each one matters in its moment, as in a game like Beat Saber or Dance Dance Revolution. The game achieves this by adjusting the slack to keep the next ten minutes’ action near the actually-mattering-to-success region all day. If you get behind you have to give up on items and move forward, so you aren’t left struggling for a low probability of catching up. If you get ahead, you add more items and thus tighten the slack.</p>

<p>A thing I like about this is that it actually makes the activity more genuinely fun and compelling, and doesn’t involve trying to trick or uncomfortably binding oneself. It is superficially a lot like a ‘productivity hack’, but I associate these with somehow manipulating or forcing yourself to do something that you at some level have real reason to dislike. I expect such tricks to fail, and I don’t think I want them to succeed.</p>

<p>This seems different: I think humans are just genuinely better at being in an enjoyable flow state when their activities have certain structures that are genuinely compatible with a variety of tasks. Beat saber wouldn’t be fun if all the boxes were just sitting in a giant pile and you had to beat your way through as many as you could over an hour. But with the boxes approaching one at a time, at a manageable rate, where what you do in each moment matters, it really is fun (for many people, I hear—I actually don’t love it, but I do appreciate this particular aspect). The same thing that makes Beat Saber more fun than Saber-a-bunch-of-boxes-on-your-own-schedule can genuinely also be applied to giant piles of tasks.</p>

<p>The fact that this game has lasted a year in my life and I come back to it with verve points to it not being an enemy to any major part of myself.</p>

<p>Another promising way of seeing this game is that this structure lets you see more clearly the true importance of each spent minute, when you were by default in error. Whereas for instance playing Civ IV for five minutes every time you do work (another sometimes way-of-being of mine) is less like causing yourself to perceive reality truly and more like trying to build an alternate incentive structure out of your mistaken perception, that adds up to rational behavior in the real world.</p>

<p>If anyone else tries this, I’m curious to hear how it goes. My above explanation of its merit suggests it might be of broad value. But I also know that perhaps nobody in the world likes organizing things into little boxes as much as I do, so that could also be the main thing going on.</p>

  </description>
  <link>/2022/09/22/2022-game.html</link>
  <guid isPermaLink="true">http://localhost:4000/2022/09/22/2022-game.html</guid>
  <pubDate>Thu, 22 Sep 2022 19:16:00 -0700</pubDate>
 </item>
 
 <item>
  <title>Podcasts on surveys, slower AI, AI arguments</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>I recently talked to Michael Trazzi for his podcast, The Inside View. It just came out, so if that’s a conversation you want to sit in on, do so <a href="https://www.youtube.com/watch?v=rSw3UVDZge0">here</a> [ETA: or read it <a href="https://theinsideview.ai/katja">here</a>].</p>

<p>The main topics were the <a href="https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/">survey of ML folk</a> I recently ran, and my thoughts on moving more slowly on potentially world-threatening AI research (which is to say, AI research in general, <a href="https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/">according to</a> the median surveyed ML researcher…). I also bet him a thousand dollars to his hundred that AI would not make blogging way more efficient in two years, if I recall. (I forget the exact terms, and there’s no way I’m listening to myself talk for that long to find out. If anyone else learns, I’m curious what I agreed to.)</p>

<p>For completeness of podcast reporting: I forgot to mention that <a href="https://axrp.net/episode/2021/07/23/episode-10-ais-future-and-dangers-katja-grace.html">I also talked to Daniel Filan on AXRP</a>, like a year ago. In other old news, I am opposed to the vibe of time-sensitivity often implicit in the public conversation.</p>

  </description>
  <link>/2022/09/17/im-on-podcasts.html</link>
  <guid isPermaLink="true">http://localhost:4000/2022/09/17/im-on-podcasts.html</guid>
  <pubDate>Sat, 17 Sep 2022 23:16:00 -0700</pubDate>
 </item>
 
 <item>
  <title>What do ML researchers think about AI in 2022?</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p><em>Crossposted from <a href="https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/">AI Impacts</a></em></p>

<p>AI Impacts just finished collecting data from a new survey of ML researchers, as similar to the <a href="https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/" data-type="post" data-id="753">2016 one</a> as practical, aside from a couple of new questions that seemed too interesting not to add.<!--ex--></p>

<p><a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/" data-type="post" data-id="3246">This page</a> reports on it preliminarily, and we’ll be adding more details there. But so far, some things that might interest you:</p>

<ul><li><strong>37 years until a 50% chance of <a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/#Definitions">HLMI</a> </strong>according to a complicated aggregate forecast (and biasedly not including data from questions about the conceptually similar Full Automation of Labor, which in 2016 prompted strikingly later estimates)<strong>.</strong> This 2059 aggregate HLMI timeline has become about eight years shorter in the six years since 2016, when the aggregate prediction was 2061, or 45 years out. Note that all of these estimates are conditional on &#8220;human scientific activity continu[ing] without major negative disruption.&#8221;</li><li><strong>P(extremely bad outcome)=5%</strong> The median respondent believes the probability that the long-run effect of advanced AI on humanity will be &#8220;extremely bad (e.g., human extinction)&#8221; is 5%. This is the same as it was in 2016 (though Zhang et al 2022 found 2% in a similar but non-identical question). Many respondents put the chance substantially higher: 48% of respondents gave at least 10% chance of an extremely bad outcome. Though another 25% put it at 0%.</li><li><strong>Explicit P(doom)=5-10% </strong>The levels of badness involved in that last question seemed ambiguous in retrospect, so I added two new questions about human extinction explicitly. The median respondent&#8217;s probability of x-risk from humans failing to control AI<span id="easy-footnote-1-3250" class="easy-footnote-margin-adjust"></span><span class="easy-footnote"><a href="#easy-footnote-bottom-1-3250" title="Or, &amp;#8216;human inability to control future advanced AI systems causing human extinction or similarly permanent and severe disempowerment of the human species&amp;#8217;"><sup>1</sup></a></span> was 10%, weirdly more than median chance of human extinction from AI in general<span id="easy-footnote-2-3250" class="easy-footnote-margin-adjust"></span><span class="easy-footnote"><a href="#easy-footnote-bottom-2-3250" title="That is, &amp;#8216;future AI advances causing human extinction or similarly permanent and severe disempowerment of the human species&amp;#8217;"><sup>2</sup></a></span>, at 5%. This might just be because different people got these questions and the median is quite near the divide between 5% and 10%. The most interesting thing here is probably that these are both very high—it seems the &#8216;extremely bad outcome&#8217; numbers in the old question were not just catastrophizing merely disastrous AI outcomes. </li><li><strong>Support for AI safety research is up</strong>: 69% of respondents believe society should prioritize AI safety research &#8220;more&#8221; or &#8220;much more&#8221; than it is currently prioritized, up from 49% in 2016. </li><li><strong>The median respondent thinks there is an &#8220;about even chance&#8221; that an argument given for an intelligence explosion is broadly correct.</strong> The median respondent also believes machine intelligence will probably (60%) be &#8220;vastly better than humans at all professions&#8221; within 30 years of HLMI, and that the rate of global technological improvement will probably (80%) dramatically increase (e.g., by a factor of ten) as a result of machine intelligence within 30 years of HLMI.</li><li><strong>Years/probabilities framing effect persists: </strong>if you ask people for probabilities of things occurring in a fixed number of years, you get later estimates than if you ask for the number of years until a fixed probability will obtain. This looked very robust in 2016, and shows up again in the 2022 HLMI data. Looking at just the people we asked for years, the aggregate forecast is 29 years, whereas it is 46 years for those asked for probabilities. (We haven&#8217;t checked in other data or for the bigger framing effect yet.)</li><li><strong>Predictions vary a lot</strong>. Pictured below: the attempted reconstructions of people&#8217;s probabilities of HLMI over time, which feed into the aggregate number above. There are few times and probabilities that someone doesn&#8217;t basically endorse the combination of.</li><li><strong>You can download the data</strong> <a href="https://docs.google.com/spreadsheets/d/1u_qcG6erXkH4EJgygl2fpkpJENAv6-kFWJejsw1oA1Q/edit?usp=sharing">here</a> (slightly cleaned and anonymized) and do your own analysis. (If you do, I encourage you to share it!)</li></ul>

<div class="center">
  <p><img src="http://aiimpacts.org/wp-content/uploads/2022/08/Screen-Shot-2022-08-04-at-02.55.28.jpg" alt="Individual inferred gamma distributions" /></p>

  <p>Individual inferred gamma distributions</p>
</div>

<p>The survey had a lot of questions (randomized between participants to make it a reasonable length for any given person), so this blog post doesn’t cover much of it. A bit more is on <a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/" data-type="post" data-id="3246">the page</a> and more will be added. </p>

<p>Thanks to many people for help and support with this project! (Many but probably not all listed on the survey page.)</p>

<hr class="wp-block-separator has-text-color has-background has-cyan-bluish-gray-background-color has-cyan-bluish-gray-color is-style-wide" />

<p><em>Cover image: Probably a bootstrap confidence interval around an aggregate of the above forest of inferred gamma distributions, but honestly everyone who can be sure about that sort of thing went to bed a while ago. So, one for a future update. I have more confidently held views on whether one should let uncertainty be the enemy of putting things up.</em></p>

<hr class="wp-block-separator has-text-color has-background has-cyan-bluish-gray-background-color has-cyan-bluish-gray-color is-style-wide" />

<ol class="easy-footnotes-wrapper"><li class="easy-footnote-single"><span id="easy-footnote-bottom-1-3250" class="easy-footnote-margin-adjust"></span>Or, &#8216;human inability to control future advanced AI systems causing human extinction or similarly permanent and severe disempowerment of the human species&#8217;<a class="easy-footnote-to-top" href="#easy-footnote-1-3250"></a></li><li class="easy-footnote-single"><span id="easy-footnote-bottom-2-3250" class="easy-footnote-margin-adjust"></span>That is, &#8216;future AI advances causing human extinction or similarly permanent and severe disempowerment of the human species&#8217;<a class="easy-footnote-to-top" href="#easy-footnote-2-3250"></a></li></ol>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;&lt;div id="custom_html-19" class="widget_text mh-widget mh-posts-2 widget_custom_html"&gt;&lt;div class="textwidget custom-html-widget"&gt;&lt;div&gt;
</code></pre></div></div>

  </description>
  <link>/2022/08/04/ai_2022_survey.html</link>
  <guid isPermaLink="true">http://localhost:4000/2022/08/04/ai_2022_survey.html</guid>
  <pubDate>Thu, 04 Aug 2022 10:30:00 -0700</pubDate>
 </item>
 
 <item>
  <title>Why do people avoid vaccination?</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>I’ve been fairly confused by the popularity in the US of remaining unvaccinated, in the face of seemingly a non-negligible, relatively immediate personal chance of death or intense illness. And due to the bubbliness of society, I don’t actually seem to know unvaccinated people to ask about it. So in the recent <a href="https://worldspiritsockpuppet.com/2022/02/04/long-covid-survey-2.html">covid survey</a> I ran, I asked people who hadn’t had covid (and thus for whom I didn’t have more pressing questions) whether they were vaccinated, and if not why not. (Note though that these people are 20-40 years old, so not at huge risk of death.)<!--ex--></p>

<p>Their responses:</p>

<blockquote>
  <ol>
    <li>I don’t think I need it, I don’t think covid is a big deal, I don’t think the vaccine works and the more the government/media pushes it the more I don’t want to ever get it.  It should be a private decision between someone and their doctor, not Joe Biden and CNN saying comply or be kicked out of society.
<br />
<br /></li>
    <li>I still dont trust the information and safety claims made by the crooked FDA and CDC. Needs more research and study.
<br />
<br /></li>
    <li>I had a scary vaccine reaction previously.
<br />
<br /></li>
    <li>I am only 32 years old and in decent health so I think I would be fine if I caught COVID. It has almost been two years since the pandemic started and I haven’t gotten sick besides some minor colds. I would rather rely on natural immunity instead of the vaccine.
<br />
<br /></li>
    <li>dont want one
<br />
<br /></li>
    <li>Other health issues where my Doctor wants me to wait to get vaccinated.
<br />
<br /></li>
    <li>I think it is poison at worst and ineffective at best. Also the way the pushed it came off like a homeless man trying to lure me into an ally. The vaccine issue has made me lose 100% trust and faith in media and government i do not believe or trust anything from them anymore
<br />
<br /></li>
    <li>I have anxiety problems and other health issues.
<br />
<br /></li>
    <li>I‚Äôm actually scheduled to get a vaccination, I‚Äôm simply waiting for my appointment date.
<br />
<br /></li>
    <li>The places where the vaccination is available are always crowded and I am somewhat afraid of being outdoors or indoors in large crowds for too long these days.
<br />
<br /></li>
    <li>I am extremely skeptical of this “vaccine”, which has been proven ineffective (vaccinated people keep getting it, and what’s with all of the boosters?!) and has caused too many deaths and/or serious health issues/emergencies. I’ll take my chances on Covid any day over a “vaccine” that magically appeared from 8+ pharmaceutical companies in such an astonishingly short time. That is unheard of, and there is no way to know what is really in it, and what kind of problems those who are ignorant and fearful enough to get it will face later on down the road (that is, if they don’t die from the “vaccinations” themselves. The fact that the government has bribed, coerced, and is trying to FORCE everyone to get this “vaccine” is unprecedented, and quite frankly terrifying and sinister to me. I will NEVER allow a forced injection of god knows what into my own body, as that is wrong in every way there is. Vaccines allegedly keep those who get them safe from contracting whatever it is the vaccine is made for, so why would it be anyone’s business whether or not someone else is vaccinated? The Let’s Go Brandon administration and the dumbocrats that were fearmongering before the administration was even in place have done this by design. Spread fear, mistrust, and divisiveness, and lock people down to isolate them and make them suspicious and fearful of one another. Divide and conquer, because the fascist wannabe communists know that it is the only way to indulge their scummy self-serving agendas. United we stand.
<br />
<br /></li>
    <li>For health reasons.
<br />
<br /></li>
    <li>I’m not at high risk, so I don’t need it immediately. Waiting to see how effective it is.
<br />
<br /></li>
    <li>I don’t think it’s efficient…the prospect of being jabbed by endless boosters doesn’t appeal to me
<br />
<br /></li>
    <li>I’ve never had covid.
<br />
<br /></li>
    <li>I don’t trust the vaccine and have not have gotten covid since the pandemic started. Plus all of the misinformation surrounding covid, from the news media to social media sites. I just don’t know what to believe anymore but I feel that not getting vaccinated is the best option for me.
<br />
<br /></li>
    <li>I am still skeptical of the vaccine since people are still catching covid and its variants
<br />
<br /></li>
    <li>It doesn’t stop the transmission, or stop you from getting covid.  I am at low risk anyways.
<br />
<br /></li>
    <li>I am at very low risk of infection as I do not work and wouldn’t accept any non-remote work. I also do not have any friends or family in the state where I moved to last year and I do not drink so I don’t go out to bars or events, or even restaurants.
<br />
<br /></li>
    <li>I simply have no trust in the big pharmacy and little more in the government. Throw in the fact that the vaccine was created too quickly and while it claims to have lowered infection rates and the severity of COVID-19, I’ve known several people who have been infected and they were fully vaccinated. In addition, I’ve seen many news articles reporting the same. Why should I receive a vaccine that provides such weak protection?
<br />
<br /></li>
    <li>Transportation and access. If a pill form or some self-administering option were available, I would.
<br />
<br /></li>
    <li>I don’t trust it.
<br />
<br /></li>
    <li>I have chosen not to take the vaccine
<br />
<br /></li>
    <li>Because I think its the Governments agenda to start killing people. I don’t trust Big Pharma or the Government. Also, people still get covid with the vaccine, seems like a waste of time.
<br />
<br /></li>
    <li>I live in Ohio, kinda wanna die. Also, it doesnt even dent the new variants; you still ge tit.
<br />
<br /></li>
    <li>do not trust them
<br />
<br /></li>
    <li>Because I believe that the vaccine is part of a either depopulation plan, or a tracking device. Either way there is no logical reason to get it.
<br />
<br /></li>
    <li>I am hesitant because of the possible side effects long-term because not much is known about it.
<br />
<br /></li>
    <li>I don‚Äôt see the benefit since I am not really at risk from covid.
<br />
<br /></li>
    <li>It’s against my religion.
<br />
<br /></li>
    <li>I don’t need a covid vaccination.
<br />
<br /></li>
    <li>I’m concerned about possible serious side effects from the vaccine that may show up some time down the road.
<br />
<br /></li>
    <li>Normally, I would consider it, but the talk of forced mandates has really soured me to it. I dont believe in taking away people’s health freedoms like that. I think it’s an insane intursion on civil liberties, and I fear if we all just accept it, we will become like Australia
<br />
<br /></li>
    <li>I only leave the house once or twice a week so it never really felt necessary to me.</li>
  </ol>
</blockquote>

<p>The biggest themes mentioned here seem to be distrust or hostility regarding the government and media and their various pro-vaccination pressurings, and the impression that the vaccine doesn’t work. (I wonder how much that one is relative to an expectation of ‘very occasional breakthrough cases’ that one might also trace to someone’s questionable communication choices.) An unspoken theme is arguably being in possession of relatively little information (like, what’s going on with the person who is still waiting to find out if the vaccines are effective? And do people just not know that the vaccine reduces the downside of covid, even if it is still possible to get it? Or do all of these people know things I don’t?).</p>

<p>I wonder if much good could be done in this kind of situation with some kind of clearly independent and apolitical personal policy research institution, who prioritized being trustworthy and accurate. Like, I don’t trust the government or media especially either on this, but I and people I do trust have a decent amount of research capacity and familiarity with things like academic papers, so we arguably have an unusually good shot at figuring out the situation without having to make reference to the government or media much (assuming there aren’t more intense conspiracies afoot, in which case we will end up dead or tracking-deviced, I suppose). If I wasn’t in such a position, I feel like I’d pay a decent amount for someone to think about such things for me. I guess this is related to the service that doctors are meant to provide, though the thing I imagine would look very different to real doctors in practice.</p>

  </description>
  <link>/2022/02/10/why-not-vaccinate.html</link>
  <guid isPermaLink="true">http://localhost:4000/2022/02/10/why-not-vaccinate.html</guid>
  <pubDate>Thu, 10 Feb 2022 11:17:00 -0800</pubDate>
 </item>
 
 <item>
  <title>Punishing the good</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>Should you punish people for wronging others, or for making the wrong call about wronging others?<!--ex--></p>

<p>For example:</p>
<ol>
  <li>A newspaper sends me annoying emails all the time, but suppose that empirically if they didn’t behave like this, they would get markedly fewer subscribers, and may not survive. And suppose their survival is in fact worth a little annoyance for a lot of people, we all agree. Such that if I was in their position, I agree that I would send out the annoying emails. Should I resent them and unsubscribe from their paper for their antisocial behavior, or praise them and be friendly because overall I think they made the right call?</li>
  <li>Suppose Bob eats beef, which he thinks makes him feel somewhat better and so be better able to carry out his job as a diplomat negotiating issues in which tens of thousands of lives are at stake. He also thinks it is pretty bad for the cows, but worth it on net. Suppose he’s right about all of this. Five hundred years later, carnivory is illegal and hated, and historians report that Bob, while in other regards a hero, did eat beef. Should the people of 2521 think of Bob as an ambiguous figure, worthy of both pride and contempt? or should they treat him as purely a hero, who made the best choice in his circumstances?</li>
</ol>

<p>I have one intuition that says, ‘how can you punish someone for doing the very best thing they could have done? What did you want them to do? And are you going to not punish the alternative person, who made a worse choice for the world, but didn’t harm someone in the process? Are you just going to punish everyone different amounts?’</p>

<p>But an argument for the other side—for punishing people for doing the right thing—is that it is needed to get the incentives straight. If Alice does $100 of harm to Bruce to provide $1000 of help to Carrie, then let’s suppose that that’s good (ignoring the potential violation of property rights, which seems like it shouldn’t be ignored ultimately). But if we let such things pass, then Alice might also do this when she guesses that is only worth $60 to Carrie, if she cares about Carrie more than Bruce. Whereas if we always punish Alice just as much as she harmed Bruce, then she will take the action exactly when she would think it worth it if it was her own welfare at stake, rather than Bruce’s. (This is just the general argument for internalizing externalities - having people pay for the costs they impose on others.)</p>

<p>This resolution is weirder to the extent that the punishment is in the form of social disgrace and the like. It’s one thing to charge Bob money for his harms to cows, and another to go around saying ‘Bob made the best altruistic decisions he could, and I would do the same in his place. Also I do think he’s contemptible.’</p>

<p>It also leaves Bob in a weird position, in which he feels fine about his decision to eat beef, but also considers himself a bit of a reprehensible baddie. Should this bother him? Should he try to reform?</p>

<p>I’m still inclined toward punishing such people, or alternately to think that the issue should be treated with more nuance than I have done, e.g. distinguishing punishments from others’ opinions of you, and more straightforward punishments.</p>

  </description>
  <link>/2021/07/20/punishing-whats-right.html</link>
  <guid isPermaLink="true">http://localhost:4000/2021/07/20/punishing-whats-right.html</guid>
  <pubDate>Tue, 20 Jul 2021 16:18:00 -0700</pubDate>
 </item>
 
 <item>
  <title>Typology of blog posts that don't always add anything clear and insightful</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>I used to think a good blog post should basically be a description of a novel insight.<!--ex--></p>

<p>To break it down more, on this view:</p>

<ol>
  <li>A blog post should have a <strong>propositional</strong> <strong>claim</strong> (e.g. ‘the biggest externalities are from noise pollution’, or ‘noise pollution is a concept’ vs. expression of someone’s feelings produced by externalities, or a series of reflections on externalities). A ‘propositional claim’ here can be described straightforwardly in words, and usually conveys information (i.e. they say the world is one way instead of another way).</li>
  <li>It should be a <strong>general</strong> claim—i.e. applicable to many times and places and counterfactuals (e.g. ‘here is how tragedies of the commons work: …’ vs. ‘here is a thing that happened to me yesterday: …’)</li>
  <li>It should be a <strong>novel</strong> claim(e.g. a new reason to doubt one of the explanations put forward for the demographic transition)</li>
  <li>The claim should be <strong>described</strong>, which is to imply that the content should be:
    <ol>
      <li><em>Verbal</em> (or otherwise symbolic, e.g. a table of numbers surrounded by text would count)</li>
      <li><em>Explicit</em> (saying the things it means, rather than alluding to them)</li>
      <li>Mostly <em>concerned with conveying the relevant propositions</em> (vs. for instance mostly concerned with affecting the reader’s mood or beliefs directly)</li>
    </ol>
  </li>
</ol>

<p>I probably would have agreed that the odd vignette was also a good blog post, but ideally it should be contained in some explicit discussion of what was to be learned from it. I probably wouldn’t have held my more recent <a href="https://worldspiritsockpuppet.com/worldlypositions.html">Worldly Positions</a> blog<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> in high esteem.</p>

<p>I now think that departures from all of these things are often good. So in the spirit of novel descriptions of explicit and general claims, I have made a typology of different combinations of these axes.</p>

<p>Before getting to it, I’ll explain some part of the value of each category that I think I overlooked, for anyone similar to my twenty year old self.</p>

<h2 id="worthy-non-propositional-claim-content">Worthy non-propositional-claim content</h2>

<p>Minds have many characteristics other than propositional beliefs. For instance, they can have feelings and attitudes and intuitions and grokkings and senses. They can meditate and chop onions quickly and look on the bright side and tend to think in terms of systems. They can also have different versions of ‘beliefs’ that don’t necessarily correspond to differences in what propositions they would assent to. For instance, they can say ‘it’s good to exercise’, or they can viscerally anticipate a better future when they choose to exercise. And even among straightforward beliefs held by minds, there are many that aren’t easily expressed in words. For instance, I have an impression of what summer evenings in the garden of a lively country restaurant were like, but to convey that sense to you is an art, and probably involves saying different propositional things in the hope that your mind will fill in the same whatever-else in the gaps. So this belief doesn’t seem to live in my mind in a simple propositional form, nor easily make its way into one.</p>

<p>All of this suggests that the set of things that you might want to communicate to a mind is large and contains much that is not naturally propositional.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup></p>

<p>Minds can also take many inputs other than propositional claims. For instance, instructions and remindings and stories and music and suggestions implicit in propositional claims and body language and images. So if you want to make available a different way of being to a mind—for instance you want it to find salient the instability of the global system—then it’s not obvious that propositional claims are the best way.</p>

<p>Given that minds can take many non-propositional inputs, and take many non-propositional states, you should just expect that there are a lot of things to be said that aren’t naturally propositional, in form or content. You should expect messages where the payload is intended to influence a mind’s non-propositional states, and ones where the mode of communication is not propositional.</p>

<h3 id="in-communicating-propositional-claims">…in communicating propositional claims</h3>

<p>There are different versions of ‘understanding’ a proposition. I like to distinguish ‘knowing’ or ‘understanding’ a thing — which is to say, seeing it fit into your abstract model of the world, being inclined to assent to it — and ‘realizing’ it — intuitively experiencing its truth in the world that you live in. Joe Carlsmith <a href="https://handsandcities.com/2021/01/31/believing-in-things-you-cannot-see/">explores</a> this distinction at more length, and gives an example I like:</p>

<blockquote>
  <p>If asked, one would agree that the people one sees on a day to day basis — on the subway, at parties, at work — all have richly detailed and complex inner lives, struggles, histories, perspectives; but this fact isn’t always present and vivid in one’s lived world; and when it becomes so, it can make an important difference to one’s ethical orientation, even if the propositions one assents to have not obviously changed.</p>
</blockquote>

<p>I repeatedly have the experience of ‘already knowing’ some obvious thing that people always say for ages before ‘realizing’ it. For instance, ‘the map is not the territory’. (“Of course the map isn’t the territory. Why would it be? That would be some stupid mistake, thinking that the map was the territory. Like, what would your model of the situation even be like? That the place you live is also your own mind?”) Then at some point it actually hits me that stuff that seems to be in the world ISN’T IN THE WORLD; WHAT SEEMS LIKE THE WORLD IS MY OWN MIND’S IMAGE OF THE WORLD. For instance, long after seeming to know that ‘the map isn’t the territory’ I was astonished to realize that those things that are just boring in their basic essence, like sports statistics and home care magazines, things that seem to be fundamentally drab, are not like that at all. They gleam with just as much allure as the things I am most compelled by, from many vantage points out there—just not mine. And in such a case I say to myself, ‘Oh wow, I just realized something…huh, I guess it is that the map is not the territory…but I knew that?’. Probably reading this, you are still thinking, ‘um yes, you weren’t aware that boringness is person-dependent?’ And I <em>was</em> aware of that. I ‘knew’ it. And I even knew it in some intuitively available ways—for instance, just because I find <em>Married at First Sight</em> interesting, I did not expect my boyfriend to find it so. In particular, in approaching my boyfriend with the news that I have been watching a bunch of <em>Married at First Sight</em>, I viscerally did not expect ‘boyfriend sympathizes with appeal of objectively excellent show’ type observations (in fact he liked it, and I was in fact surprised). But still the boringness of other subjects is depicted to me as part of them, like being red is depicted as in the world (whereas ‘liable to reduce my hunger’ say, is I think more accurately represented by my mind as a feature of myself). And ‘realizing’ that that isn’t right changes how the world that I spend my concrete days in seems.</p>

<p>(I know I have hardly explained or defended this claim that ‘realizing’ is a thing, and important, but I’m not going to do that properly here.)</p>

<p>All of these ‘realizations’ seem to be non-propositional. You already had some proposition, and then you get something else. I think of ‘realizing’ a proposition as acquiring a related non-proposition. To realize the proposition ‘other people have inner lives’ is to take in some non-proposition. Perhaps a spacious sense of those other minds being right there around you. If you are communicating a proposition, to have it actually realized, you want to get its non-proposition partner into the recipient’s mind also. This isn’t really right, because each proposition probably has a multitude of intuitive realizations of it, and each intuitive sense of the world could be part of appreciating a multitude of different propositions. But at any rate, communicating a proposition well, so that the other person can really make use of it, often seems to involve conveying a lot of its non-propositional brethren.</p>

<h2 id="worthy-non-descriptive-communication">Worthy non-descriptive communication</h2>

<p>Closely related to non-propositional content is non-descriptive communication, which I shall call ‘evocative’ communication.</p>

<p>I’m thinking of a few different axes as being related to descriptiveness of communication:</p>

<ul>
  <li>Verbalness (consisting of words, e.g. “donkeys are nice” vs. a video of a nice donkey)</li>
  <li>Explicitness (saying in words the thing you mean, rather than demonstrating it or suggesting it or subtly causing it to creep into the background of the picture you are painting without naming it. E.g. “I want us to follow this protocol” vs. “Most reasonable people are following this protocol now”)</li>
  <li>Neutrality (not setting out to affect the readers’ emotions except via content itself)</li>
</ul>

<p>I think of the most vanilla communication as being explicit, verbal and neutral. And this seems pretty good for conveying propositional content. But I suspect that non-propositional content is often conveyed better through evocative communication.</p>

<p>(Or perhaps it is more like: communicating propositional claims explicitly with language is uniquely easy, because explicit language is basically a system we set up for communicating, and propositions are a kind of message that is uniquely well suited to it. But once we leave the set of things that are well communicated in this way, and given that there are lots of other ways to communicate things, non-descriptive forms of communication are much more likely to be helpful than they were.)</p>

<p>Relatedly, I think non-descriptive communication can be helpful in making the ‘realizing’ versions of propositional claims available to minds. That is, in really showing them to us. So in that way, evocative communication seems also potentially valuable for communicating propositional content well.</p>

<h2 id="worthy-communication-of-non-propositional-things-descriptively">Worthy communication of non-propositional things descriptively</h2>

<p>Going the opposite way—trying to communicate ineffable things in words—also seems valuable, because a) groping nearby propositionally does contribute to understanding, and b) only understanding things in ineffable ways leaves them unavailable to our reasoning faculties in important ways.</p>

<h2 id="worthy-non-generality">Worthy non-generality</h2>

<p>I thought that if things were not general, then they were particularly unimportant to talk about. All things equal, isn’t it way better to understand a broad class of things better than a single thing?</p>

<p>Some ways this is misleading:</p>

<ul>
  <li>Understanding specific things is often basically a prerequisite for understanding general things. For instance, devising a general theory of circumstances under which arms races develop will be harder without specific information about the behavior of specific nations historically, to inspire or constrain your theorizing</li>
  <li>Understanding specific things one after another will often automatically lead to your having an intuitive general model, through some kind of brain magic, even in cases where you would have had a hard time making an explicit model. For instance, after you have seen a thousand small disputes run their course, you might have a pretty good guess about how the current dispute will go, even if you couldn’t begin to describe a theory of argumentation for the relevant community.</li>
  <li>Specific things are often broadly relevant to the specific world that you live in. For instance, exactly what happened in a particular past war might determine what current obligations should be and what sentiments are warranted, and who is owed, and what particular current parties might be expected to want or take for granted. Which is perhaps only of much interest in a narrow range of circumstances, but if they are the circumstances in which we will live for decades, it might be consistently material.</li>
</ul>

<h2 id="worthy-non-originality-of-content">Worthy non-originality of content</h2>

<p>On my naive model, you don’t want to repeat something that someone else said, because there is implicitly no value in the repetition—the thing has already been said, so re-saying adds nothing and seems to imply that you are either ignorant or hoping to dupe ignorant others into giving you undeserved credit.</p>

<p>But on a model where many claims are easy enough to accept, but hard to realize, things look very different. The first time someone writes down an idea, the chances of it really getting through to anyone with much of its full power are low. The typical reader needs to meet the idea repeatedly, from different angles, to start to realize it.</p>

<p>In a world like that, a lot of value comes from rehashing older ideas. Also in that world, rehashing isn’t the easy cashing in of someone else’s work. Writing something in a way that might really reach some people who haven’t yet been reached is its own art.</p>

<h2 id="worthy-non-originality-of-communication">Worthy non-originality of communication</h2>

<p>I think I also kind of imagined that once an idea had been put into the ‘public arena’ then the job was done. But another way in which unoriginality is incredibly valuable is that each person can only see such a  minuscule fraction of what has ever been written or created, and they can’t even see what they can’t see, that locating particularly apt bits and sharing them with the right audience can be as valuable as writing the thing in the first place. This is curating and signal boosting. For these, you don’t even need to write anything original. But again, doing them well is not trivial. Knowing which of the cornucopia of content should be shown to someone is a hard intellectual task.</p>

<h2 id="typology">Typology</h2>

<p>Here is my tentative four-dimensional typology of kinds of blog posts. Any blog post maps to a path from some kind of content on the left, through some kind of communication to publication on the right. Content varies on two axes: generality and propositionalness. Communication varies in evocativeness. And blog posts themselves vary in how early in this pipeline the author adds value. For instance, among posts with a general propositional idea as their content, communicated in a non-propositional way, there are ones where the author came up with the idea, ones where the author took someone else’s idea and wrote something evocative about it, and ones that are repostings of either of the above. Thus, somewhat confusingly, there are 16 (pathways from left to right) x 3 (steps per pathway) = 46 total blog post types represented here, not the 36 you might expect from the number of squares.</p>

<p>I include a random assortment of examples, some obscure, zooming probably required (apologies).</p>

<p><a href="https://katjagrace.files.wordpress.com/2021/07/katja-2-e1626132069362.png"><img src="https://katjagrace.files.wordpress.com/2021/07/katja-2-e1626132069362.png" alt="Blog post typology" /></a></p>

<h2 id="main-updates">Main updates</h2>

<ol>
  <li>Lots of worthy things are hard to describe in words</li>
  <li>‘Realizing’ is a thing, and valuable, and different to understanding</li>
  <li>Details can be good</li>
  <li>Having ideas is not obviously the main place one can add value</li>
</ol>

<h2 id="takeaways">Takeaways</h2>

<ol>
  <li>It’s good to write all manner of different kinds of blog posts</li>
  <li>It’s good to just take other people’s ideas and write blog posts about them, especially of different kinds than the original blog posts</li>
  <li>It’s good to just take one’s own ideas and write second or third blog posts saying exactly the same thing in different ways</li>
</ol>

<h2 id="other-thoughts">Other thoughts</h2>

<p>These different sorts of blog posts aren’t always valuable, of course. They have to be done well. Compellingly writing about something that isn’t worthy of people’s attention, or curating the wrong things can be as bad as the good versions of these things are good.</p>

<p>Epistemic status: overall I expect to find that this post is badly wrong in at least one way in short order, but to be sufficiently interested in other things that I don’t get around to fixing it. Another good thing about rehashing others ideas is that you can make subtle edits where they are wrong.</p>

<!-- Footnotes themselves at the bottom. -->
<h2 id="notes">Notes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">

      <p>Older posts <a href="https://worldlypositions.tumblr.com/">here</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">

      <p>I don’t want to make strong claims about exactly what counts as propositional—maybe these things are propositional in some complicated way—but hopefully I’m pointing at an axis of straightforward propositionalness versus something else, regardless. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </description>
  <link>/2021/07/12/typology-of-blog-posts.html</link>
  <guid isPermaLink="true">http://localhost:4000/2021/07/12/typology-of-blog-posts.html</guid>
  <pubDate>Mon, 12 Jul 2021 18:28:00 -0700</pubDate>
 </item>
 
 <item>
  <title>The ecology of conviction</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>Supposing that <a href="https://worldspiritsockpuppet.com/2020/11/30/sincerity-trends.html">sincerity has declined</a>, why?</p>

<p>It feels natural to me that sincere enthusiasms should be rare relative to criticism and half-heartedness. But I would have thought this was born of fairly basic features of the situation, and so wouldn’t change over time.</p>

<p>It seems clearly easier and less socially risky to be critical of things, or non-committal, than to stand for a positive vision. It is easier to produce a valid criticism than an idea immune to valid criticism (and easier again to say, ‘this is very simplistic - the situation is subtle’). And if an idea is criticized, the critic gets to seem sophisticated, while the holder of the idea gets to seem naïve. A criticism is smaller than a positive vision, so a critic is usually not staking their reputation on their criticism as much, or claiming that it is good, in the way that the enthusiast is.<!--ex--></p>

<p>But there are also rewards for positive visions and for sincere enthusiasm that aren’t had by critics and routine doubters. So for things to change over time, you really just need the scale of these incentives to change, whether in a basic way or because the situation is changing.</p>

<p>One way this could have happened is that the internet (or even earlier change in the information economy) somehow changed the ecology of enthusiasts and doubters, pushing the incentives away from enthusiasm. e.g. The ease, convenience and anonymity of criticizing and doubting on the internet puts a given positive vision in contact with many more critics, making it basically impossible for an idea to emerge not substantially marred by doubt and teeming with uncertainties and summarizable as ‘maybe X, but I don’t know, it’s complicated’. This makes presenting positive visions less appealing, reducing the population of positive vision havers, and making them either less confident or more the kinds of people whose confidence isn’t affected by the volume of doubt other people might have about what they are saying. Which all make them even easier targets for criticism, and make confident enthusiasm for an idea increasingly correlated with being some kind of arrogant fool. Which decreases the basic respect offered by society for someone seeming to have a positive vision.</p>

<p>This is a very speculative story, but something like these kinds of dynamics seems plausible.</p>

<p>These thoughts were inspired by a conversation I had with Nick Beckstead.</p>

  </description>
  <link>/2021/02/13/ecology-of-conviction.html</link>
  <guid isPermaLink="true">http://localhost:4000/2021/02/13/ecology-of-conviction.html</guid>
  <pubDate>Sat, 13 Feb 2021 09:30:00 -0800</pubDate>
 </item>
 
 <item>
  <title>In balance and flux</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>Someone more familiar with ecology recently noted to me that it used to be a popular view that nature was ‘in balance’ and had some equilibrium state, that it should be returned to. Whereas the new understanding is that there was never an equilibrium state. Natural systems are always changing. Another friend who works in natural management also recently told me that their role in the past might have been trying to restore things to their ‘natural state’, but now the goal was to prepare yourself for what your ecology was becoming. A brief Googling returns this National Geographic article by Tik Root, <a href="https://www.nationalgeographic.com/environment/global-warming/balance-of-nature-explained/"><em>The ‘balance of nature’ is an enduring concept. But it’s wrong.</em></a> along the same lines. In fairness, they seem to be arguing against both the idea that nature is in a balance so intense that you can easily disrupt it, and the idea that nature is in a balance so sturdy that it will correct anything you do to it, which sounds plausible. But they don’t say that ecosystems are probably in some kind of intermediately sturdy balance, in many dimensions at least. They say that nature is ‘in flux’ and that the notion of balance is a misconception.</p>

<p>It seems to me though that there is very often equilibrium in some dimensions, even in a system that is in motion in other dimensions, and that that balance can be very important to maintain.</p>

<p>Some examples:</p>
<ul>
  <li>bicycle</li>
  <li>society with citizens with a variety of demeanors, undergoing broad social change</li>
  <li>human growing older, moving to Germany, and getting pregnant, while maintaining a narrow range of temperatures and blood concentrations of different chemicals<!--ex--></li>
</ul>

<p>So the observation that a system is in flux seems fairly irrelevant to whether it is in equilibrium.</p>

<p>Any system designed to go somewhere relies on some of its parameters remaining within narrow windows. Nature isn’t designed to go somewhere, so the issue of what ‘should’ happen with it is non-obvious. But the fact that ecosystems always gradually change along some dimensions (e.g. grassland becoming forest) doesn’t seem to imply that there are not still balance in other dimensions, where they don’t change so much, and where changing is more liable to lead to very different and arguably less good states.</p>

<p>For instance, as a grassland gradually reforests, it might continue to have a large number of plant eating bugs, and bug-eating birds, such that the plant eating bugs would destroy the plants entirely if there were ever too many of them, but as there become more of them, the birds also flourish, and then eat them. As the forest grows, the tree-eating bugs become more common relative to the grass-eating bugs, but the rough equilibrium of plants, bugs, and birds remains. If the modern world was disrupting the reproduction of the birds, so that they were diminishing even while the bugs to eat were plentiful, threatening a bug-explosion-collapse in which the trees and grass would be destroyed by the brief insect plague, I think it would be reasonable to say that the modern world was disrupting the equilibrium, or putting nature out of balance.</p>

<p>The fact that your bike has been moving forward for miles doesn’t mean that leaning a foot to the left suddenly is meaningless, in systems terms.</p>

  </description>
  <link>/2021/02/11/nature-balance.html</link>
  <guid isPermaLink="true">http://localhost:4000/2021/02/11/nature-balance.html</guid>
  <pubDate>Thu, 11 Feb 2021 06:27:00 -0800</pubDate>
 </item>
 
 <item>
  <title>What is going on in the world?</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>Here’s a list of alternative high level narratives about what is importantly going on in the world—the central plot, as it were—for the purpose of thinking about what role in a plot to take:<!--ex--></p>

<ul>
  <li>The US is falling apart rapidly (on the scale of years), as evident in US politics departing from sanity and honor, sharp polarization, violent civil unrest, hopeless pandemic responses, ensuing economic catastrophe, one in a thousand Americans dying by infectious disease in 2020, and the abiding popularity of Trump in spite of it all.</li>
  <li>Western civilization is declining on the scale of half a century, as evidenced by its inability to build things it used to be able to build, and the ceasing of apparent economic acceleration toward a singularity.</li>
  <li>AI agents will control the future, and which ones we create is the only thing about our time that will matter in the long run. Major subplots:
    <ul>
      <li>‘Aligned’ AI is necessary for a non-doom outcome, and hard.</li>
      <li>Arms races worsen things a lot.</li>
      <li>The order of technologies matters a lot / who gets things first matters a lot, and many groups will develop or do things as a matter of local incentives, with no regard for the larger consequences.</li>
      <li>Seeing more clearly what’s going on ahead of time helps all efforts, especially in the very unclear and speculative circumstances (e.g. this has a decent chance of replacing subplots here with truer ones, moving large sections of AI-risk effort to better endeavors).</li>
      <li>The main task is finding levers that can be pulled at all.</li>
      <li>Bringing in people with energy to pull levers is where it’s at.</li>
    </ul>
  </li>
  <li>Institutions could be way better across the board, and these are key to large numbers of people positively interacting, which is critical to the bounty of our times. Improvement could make a big difference to swathes of endeavors, and well-picked improvements would make a difference to endeavors that matter.</li>
  <li>Most people are suffering or drastically undershooting their potential, for tractable reasons.</li>
  <li>Most human effort is being wasted on endeavors with no abiding value.</li>
  <li>If we take anthropic reasoning and our observations about space seriously, we appear very likely to be in a ‘Great Filter’, which appears likely to kill us (and unlikely to be AI).</li>
  <li>Everyone is going to die, the way things stand.</li>
  <li>Most of the resources ever available are in space, not subject to property rights, and in danger of being ultimately had by the most effective stuff-grabbers. This could begin fairly soon in historical terms.</li>
  <li>Nothing we do matters for any of several reasons (moral non-realism, infinite ethics, living in a simulation, being a Boltzmann brain, ..?)</li>
  <li>There are vast quantum worlds that we are not considering in any of our dealings.</li>
  <li>There is a strong chance that we live in a simulation, making the relevance of each of our actions different from that which we assume.</li>
  <li>There is reason to think that acausal trade should be a major factor in what we do, long term, and we are not focusing on it much and ill prepared.</li>
  <li>Expected utility theory is the basis of our best understanding of how best to behave, and there is reason to think that it does not represent what we want. Namely, Pascal’s mugging, or the option of destroying the world with all but one in a trillion chance for a proportionately greater utopia, <a href="https://en.wikipedia.org/wiki/St._Petersburg_paradox#The_paradox">etc</a>.</li>
  <li>Consciousness is a substantial component of what we care about, and we not only don’t understand it, but are frequently convinced that it is impossible to understand satisfactorily. At the same time, we are on the verge of creating things that are very likely conscious, and so being able to affect the set of conscious experiences in the world tremendously. Very little attention is being given to doing this well.</li>
  <li>We have weapons that could destroy civilization immediately, which are under the control of various not-perfectly-reliable people. We don’t have a strong guarantee of this not going badly.</li>
  <li>Biotechnology is advancing rapidly, and threatens to put extremely dangerous tools in the hands of personal labs, possibly bringing about a ‘<a href="https://www.globalpolicyjournal.com/articles/global-public-goods-and-bads/vulnerable-world-hypothesis#:~:text=This%20paper%20introduces%20the%20concept,semi%E2%80%90anarchic%20default%20condition'.">vulnerable world</a>’ scenario.</li>
  <li>Technology keeps advancing, and we may be in a vulnerable world scenario.</li>
  <li>The world is utterly full of un-internalized externalities and they are wrecking everything.</li>
  <li>There are lots of things to do in the world, we can only do a minuscule fraction, and we are hardly systematically evaluating them at all. Meanwhile massive well-intentioned efforts are going into doing things that are probably much less good than they could be.</li>
  <li>AI is powerful force for good, and if it doesn’t pose an existential risk, the earlier we make progress on it, the faster we can move to a world of unprecedented awesomeness, health and prosperity.</li>
  <li>There are risks to the future of humanity (‘existential risks’), and vastly more is at stake in these than in anything else going on (if we also include catastrophic trajectory changes). Meanwhile the world’s thinking and responsiveness to these risks is incredibly minor and they are taken unseriously.</li>
  <li>The world is controlled by governments, and really awesome governance seems to be scarce and terrible governance common. Yet we probably have a lot of academic theorizing on governance institutions, and a single excellent government based on scalable principles might have influence beyond its own state.</li>
  <li>The world is hiding, immobilized and wasted by a raging pandemic.</li>
</ul>

<p>It’s a draft. What should I add? (If, in life, you’ve chosen among ways to improve the world, is there a simple story within which your choices make particular sense?)</p>

  </description>
  <link>/2021/01/17/whats-going-on.html</link>
  <guid isPermaLink="true">http://localhost:4000/2021/01/17/whats-going-on.html</guid>
  <pubDate>Sun, 17 Jan 2021 01:28:39 -0800</pubDate>
 </item>
 
 <item>
  <title>Condition-directedness</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>In chess, you can’t play by picking a desired end of the game and backward chaining to the first move, because there are vastly more possible chains of moves than your brain can deal with, and the good ones are few. Instead, chess players steer by heuristic senses of the worth of situations. I assume they still back-chain a few moves (‘if I go there, she’ll have to move her rook, freeing my queen’) but just leading from a heuristically worse to a heuristically better situation a short hop away.<!--ex--></p>

<p>In life, it is often taken for granted that one should pursue goals, not just very locally, but over scales of decades. The alternative is taken to be being unambitious and directionless.</p>

<p>But there should also be an alternative that is equivalent to the chess one: heuristically improving the situation, without setting your eye on a particular pathway to a particular end-state.</p>

<p>Which seems like actually what people do a lot of the time. For instance, making your living room nice without a particular plan for it, or reading to be ‘well read’, or exercising to be ‘fit’ (at least insofar as having a nice living space and being fit and well-read are taken as generally promising situations rather than stepping stones immediately prior to some envisaged meeting, say). Even at a much higher level, spending a whole working life upholding the law or reporting on events or teaching the young because these put society in a better situation overall, not because they will lead to some very specific outcome.</p>

<p>In spite of its commonness, I’m not sure that I have heard of this type of action labeled as distinct from goal-directedness and undirectedness. I’ll call it condition-directedness for now. When people are asked for their five year plans, they become uncomfortable if they don’t have one, rather than proudly stating that they don’t currently subscribe to goal-oriented strategy at that scale. Maybe it’s just that I hang out in this strange Effective Altruist community, where all things are meant to be judged by their final measure on the goal, which perhaps encourages evaluating them explicitly with reference to an envisaged path to the goal, especially if it is otherwise hard to distinguish the valuable actions from doing whatever you feel like.</p>

<p>It seems like one could be condition-directed and yet very ambitious and not directionless. (Though your ambition would be non-specific, and your direction would be local, and maybe they are the worse for these things?) For instance, you might work tirelessly on whatever seems like it will improve the thriving of a community that you are part of, and always know in which direction you are pushing, and have no idea what you will be doing in five years.</p>

<p>Whether condition-directedness is a good kind of strategy would seem to depend on the game you are playing, and your resources for measuring and reasoning about it. In chess, condition-directedness seems necessary. Somehow longer term plans do seem more feasible in life than in chess though, so it is possible that they are always better in life, at the scales in question. I doubt this, especially given the observation that people often seem to be condition-directed, at least at some scales and in some parts of life.</p>

<p>(These thoughts currently seem confused to me - for instance, what is up with scales? How is my knowing that I do want to take the king relevant?)</p>

<p><em>Inspired by a conversation with John Salvatier.</em></p>

  </description>
  <link>/2021/01/08/condition-directedness.html</link>
  <guid isPermaLink="true">http://localhost:4000/2021/01/08/condition-directedness.html</guid>
  <pubDate>Fri, 08 Jan 2021 01:21:42 -0800</pubDate>
 </item>
 
 <item>
  <title>Opposite attractions</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>Is the opposite of what you love also what you love?</p>

<p>I think there’s a general pattern where if you value A you tend to increase the amount of it in your life, and you end feeling very positively about various opposites of A—things that are very unlike A, or partially prevent A, or undo some of A’s consequences—as well. At least some of the time, or for some parts of you, or in some aspects, or when your situation changes a bit. Especially if you contain multitudes.</p>

<p>Examples:<!--ex--></p>
<ul>
  <li>Alice values openness, so tends to be very open: she tells anyone who asks (and many people who don’t) what’s going on in her life, and writes about it abundantly on the internet. But when she is embarrassed about something, she feels oppressed by everyone being able to see her so easily. So then she hides in her room, works at night when nobody is awake to think of her, and writes nothing online. Because for her, interacting with someone basically equates to showing them everything, her love of openness comes with a secondary love of being totally alone in her room.</li>
  <li>Bob values connecting with people, and it seems hard in the modern world, but he practices heartfelt listening and looking people in the eye, and mentally jumping into their perspectives. He often has meaningful conversations in the grocery line, which he enjoys and is proud of. He goes to Burning Man and finds thousands of people desperate to connect with him, so that his normal behavior is quickly leading to an onslaught of connecting that is more than he wants. He finds himself savoring the impediments to connection—the end of an eye-gazing activity, the chance to duck out of a conversation, the walls of his tent—in a way that nobody else at Burning Man is.</li>
  <li>An extreme commitment to honesty and openness with your partner <a href="https://meteuphoric.com/2010/11/10/know-thyself-vs-know-one-another/">might</a> leads to a secondary inclination away from honesty and openness with yourself.</li>
  <li>A person who loves travel also loves being at home again afterward, with a pointed passion absent from a person who is a perpetual homebody.</li>
  <li>A person who loves jumping in ice water is more likely to also love saunas than someone who doesn’t.</li>
  <li>A person who loves snow is more likely to love roaring fires.</li>
  <li>A person who loves walking has really enjoyed lying down at the end of the day.</li>
  <li>A person who surrounds themselves with systems loves total abandonment of them during holiday more than he who only had an appointment calendar and an alarm clock to begin with.</li>
  <li>A person with five children because they love children probably wants a babysitter for the evening more than the person who ambivalently had a single child.</li>
  <li>A person who loves hanging out with people who share an interest in the principles of effective altruism is often also especially excited to hang out with people who don’t, on the occasions when they do that.</li>
  <li>A person who directs most of their money to charity is more obsessed with the possibility of buying an expensive dress than their friend who cares less about charity.</li>
  <li>A person who is so drawn to their partner’s company that they can’t stay away from them at home sometimes gets more out of solitary travel than someone more solitariness-focused in general.</li>
  <li>A person craving danger also cares about confidence in safety mechanisms.</li>
  <li>A person who loves the sun wants sunglasses and sunscreen more than a person who stays indoors.</li>
</ul>

<p>This pattern makes sense, because people and things are multifaceted, and effects are uncertain and delayed. So some aspect of you liking some aspect of a thing at some time will often mean you ramp up that kind of thing, producing effects other than the one you liked, plus more of the effect that you liked than intended because of delay. And anyway you are a somewhat different creature by then, and maybe always had parts less amenable to the desired thing anyway. Or more simply, because in systems full of <a href="https://en.wikipedia.org/wiki/Negative_feedback">negative feedbacks</a>, effects tend to produce opposite effects, and you and the world are such systems.</p>

  </description>
  <link>/2020/12/19/opposite-attractions.html</link>
  <guid isPermaLink="true">http://localhost:4000/2020/12/19/opposite-attractions.html</guid>
  <pubDate>Sat, 19 Dec 2020 13:00:00 -0800</pubDate>
 </item>
 
 <item>
  <title>What is it good for? But actually?</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>I didn’t learn about history very well prior to my thirties somehow, but lately I’ve been variously trying to rectify this. Lately I’ve been reading Howard Zinn’s <a href="https://www.amazon.com/dp/B00O2YH8EQ/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1">People’s History of the United States</a>, listening to Steven Pinker’s <a href="https://www.amazon.com/Better-Angels-Our-Nature-Violence/dp/0670022950/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">the Better Angels of Our Nature</a>, watching Ken Burns and Lynn Novick’s <a href="http://www.pbs.org/kenburns/the-vietnam-war/home/">documentary about the Vietnam War</a> and watching <a href="https://www.youtube.com/user/Webzwithaz">Oversimplified</a> history videos on YouTube (which I find too lighthearted for the subject matter, but if you want to squeeze extra history learning in your leisure and dessert time, compromises can be worth it.)</p>

<p>There is a basic feature of all this that I’m perpetually confused about: how has there been so much energy for going to war?<!--ex--></p>

<p>It’s hard to explain my confusion, because in each particular case, there might be plenty of plausible motives given–someone wants ‘power’, or to ‘reunite their country’, or there is some customary enemy, or that enemy might attack them otherwise–but overall, it seems like the kind of thing people should be extremely averse to, such that even if there were plausibly good justifications, they wouldn’t just win out constantly, other justifications for not doing the thing would usually be found. Like, there are great reasons for writing epic treatises on abstract topics, but somehow, most people find that they don’t get around to it. I expect going to some huge effort to travel overseas and die in the mud to be more like that, intuitively.</p>

<p>To be clear, I’m not confused here about people fighting in defense of things they care a lot about—joining the army when their country is under attack, or joining the Allies in WWII. And I’m not confused by people who are forced to fight, by conscription or desperate need of money. It’s just that across these various sources on history, I haven’t seen much comprehensible-to-me explanation of what’s going on in the minds of the people who volunteer to go to war (or take part in smaller dangerous violence) when the stakes aren’t already at the life or death level for them.</p>

<p>I am also not criticizing the people whose motives I am confused by–I’m confident that I’m missing things.</p>

<p>It’s like if I woke up tomorrow to find that half the country was volunteering to cut off their little finger for charity, I’d be pretty surprised. And if upon inquiring, each person had something to say—about how it was a good charity, or how suffering is brave and valiant, or how their Dad did it already, or how they were being emotionally manipulated by someone else who wanted it to happen, or they how wanted to be part of something—each one might not be that unlikely, but I’d still feel overall super confused, at a high level, at there being enough total energy behind this, given that it’s a pretty costly thing to do.</p>

<p>At first glance, the historical people heading off to war don’t feel surprising. But I feel like this is because it is taken for granted as what historical people do. Just as in stories about Christmas, it is taken for granted that Santa Clause will make and distribute billions of toys, because that’s what he does, even though his motives are actually fairly opaque. But historical people presumably had internal lives that would be recognizable to me. What did it look like from the inside, to hear that WWI was starting, and hurry to sign up? Or to volunteer for the French military in time to fight to maintain French control in Vietnam, in the <a href="https://en.wikipedia.org/wiki/First_Indochina_War">First Indochina War</a>, that preceded the Vietnam War?</p>

<p>I’d feel less surprised in a world where deadly conflict was more like cannibalism is in our world. Where yes, technically humans are edible, so if you are hungry enough you can eat them, but it is extremely rare for it to get to that, because nobody wants to be on any side of it, and they have very strong and consistent feelings about that, and if anyone really wanted to eat thousands or millions of people, say to bolster their personal or group power, it would be prohibitively expensive in terms of money or social capital to overcome the universal distaste for this idea.</p>

  </description>
  <link>/2020/12/16/war.html</link>
  <guid isPermaLink="true">http://localhost:4000/2020/12/16/war.html</guid>
  <pubDate>Wed, 16 Dec 2020 00:09:35 -0800</pubDate>
 </item>
 
 <item>
  <title>Unexplored modes of language</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>English can be communicated via 2D symbols that can be drawn on paper using a hand and seen with eyes, or via sounds that can be made with a mouth and heard by ears.</p>

<p>These two forms are the same language because the mouth sounds and drawn symbols correspond at the level of words (and usually as far as sounds and letters, at least substantially). That is, if I write ‘ambition’, there is a specific mouth sound that you would use if converting it to spoken English, whereas if you were converting it to spoken French, there might not be a natural equivalent.<!--ex--></p>

<p>As far as I know, most popular languages are like this: they have a mouth-sound version and a hand-drawn (or hand-typed) version. They often have a braille version, with symbols that can be felt by touch instead of vision. An exception is sign languages (which are generally not just alternate versions of spoken languages), which use 4-D symbols gestured by hands over time, and received by eyes.</p>

<p>I wonder whether there are more modes of languages that it would be good to have. Would we have them, if there were? It’s not clear from <a href="https://en.wikipedia.org/wiki/History_of_sign_language">a brief perusal of Wikipedia</a> that Europe had sophisticated sign languages prior to about five hundred years ago. Communication methods generally have strong network effects—it’s not worth communicating by some method that nobody can understand, just like it’s not worth joining an empty dating site—and new physical modes of English are much more expensive than for instance new messaging platforms, and have nobody to promote them.</p>

<p>Uncommon modes of language that seem potentially good (an uninformed brainstorm):</p>
<ul>
  <li><strong>symbols drawn with hands on receiver’s skin, received by touch</strong>, I’ve heard of blind and deaf people such as Helen Keller using this, but it seems useful for instance when it is loud, or when you don’t want to be overheard or to annoy people nearby, or for covert communication under the table at a larger event, or for when you are wearing a giant face mask.
-<strong>symbols gestured with whole body</strong> like interpretive dance, but with objective interpretation. Good from a distance, when loud, etc. Perhaps conducive to different sorts of expressiveness, like how verbal communication makes singing with lyrics possible, and there is complementarity between the words and the music.</li>
  <li><strong>symbols gestured with whole body, interpreted by computer, received as written text</strong> What if keyboards were like a Kinect dance game? Instead of using your treadmill desk while you type with your hands, you just type with your arms, legs and body in a virtual reality whole-body keyboard space. Mostly good for exercise, non-sedentariness, feeling alive, etc.</li>
  <li><strong>drumming/tapping, received by ears or touch</strong> possibly faster than spoken language, because precise sounds can be very fast. I don’t know. This doesn’t really sound good.</li>
  <li><strong>a sign version of English</strong> this <a href="https://www.verywellhealth.com/signing-exact-english-and-syntax-1046860">exists, but is rare</a>. Good for when it is loud, when you don’t want to be overheard, when you are wearing a giant face mask or are opposed to exhaling too much on the other person, when you are at a distance, etc.</li>
  <li><strong>symbols drawn with hands in one place</strong> e.g. the surface of a phone, or a small number of phone buttons, such that you could enter stuff on your phone by tapping your fingers in place in a comfortable position with the hand you were holding it with, preferably still in your pocket, rather than awkwardly moving them around on the surface while you hold it either with another hand or some non-moving parts of the same hand, and having to look at the screen while you do it. This could be combined with the first one on this list.</li>
  <li><strong>What else?</strong></li>
</ul>

<p>Maybe if there’s a really good one, we could overcome the network effect with an assurance contract. (Or try to, and learn more about why assurance contracts aren’t used more.)</p>

  </description>
  <link>/2020/12/09/unexplored-modes-of-language.html</link>
  <guid isPermaLink="true">http://localhost:4000/2020/12/09/unexplored-modes-of-language.html</guid>
  <pubDate>Wed, 09 Dec 2020 09:48:56 -0800</pubDate>
 </item>
 
 <item>
  <title>Why are delicious biscuits obscure?</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>I saw a picture of these biscuits (or cookies), and they looked very delicious. So much so that I took the uncharacteristic step of actually making them. They were indeed among the most delicious biscuits of which I am aware. And yet I don’t recall hearing of them before. This seems like a telling sign about something. (The capitalist machinery? Culture? Industrial food production constraints? The vagaries of individual enjoyment?)</p>

<p><img src="https://www.dropbox.com/s/91nepe5q84xtbyf/iStock-1088141566.jpg?raw=1" alt="Kolakakor" /><!--ex--></p>

<p>Why doesn’t the market offer these delicious biscuits all over the place? Isn’t this just the kind of rival, excludable, information-available, well-internalized good that markets are on top of?</p>

<p>Some explanations that occur to me:</p>
<ol>
  <li>I am wrong or unusual in my assessment of deliciousness, and for instance most people would find a chocolate chip cookie or an Oreo more delicious.</li>
  <li>They are harder to cook commercially than the ubiquitous biscuits for some reason. e.g. they are most delicious warm.</li>
  <li>They are Swedish, and there are mysterious cultural or linguistic barriers to foods spreading from their original homes. This would also help explain some other observations, to the extent that it counts as an explanation at all.</li>
  <li>Deliciousness is not a central factor in food spread. (Then what is?)</li>
</ol>

<p>If you want to help investigate, you can do so by carrying out the following recipe and reporting on the percentile of deliciousness of the resulting biscuits. (I do not claim that this is a high priority investigation to take part in, unless you are hungry for delicious biscuits or a firsthand encounter with a moderately interesting sociological puzzle.)</p>

<p> </p>

<p>*</p>

<p> </p>

<p><strong>Kolakakor</strong></p>

<p>(Or <em>Kolasnittar</em>. Adapted from <a href="https://www.houseandgarden.co.uk/recipe/caramel-shortbread">House &amp; Garden’s</a> account of a recipe in Magnus Nilsson’s “<a href="https://www.amazon.com/Nordic-Baking-Book-Magnus-Nilsson/dp/0714876844">The Nordic Baking Book</a>”. It’s quite plausible that their versions are better than mine, which has undergone pressure for ease plus some random ingredient substitutions. However I offer mine, since it is the one I can really vouch for.)</p>

<p><em>Takes about fifteen minutes of making, and fifteen further minutes of waiting. Makes enough biscuits for about five people to eat too many biscuits, plus a handful left over. (Other recipe calls it about 40 ‘shortbreads’)</em></p>

<p><strong>Ingredients</strong></p>

<ul>
  <li>200 g melted butter (e.g. microwave it)</li>
  <li>180 g sugar</li>
  <li>50 g golden syrup</li>
  <li>50g honey</li>
  <li>300 g flour, ideally <a href="https://shop.kingarthurbaking.com/items/gluten-free-all-purpose-flour">King Arthur gluten free flour</a>, but wheat flour will also do</li>
  <li>1 teaspoon bicarbonate of soda (baking soda)</li>
  <li>2 teaspoon ground ginger</li>
  <li>2 good pinches of salt</li>
</ul>

<p><strong>Method</strong></p>

<ol>
  <li>Preheat oven: 175°C/345°F</li>
  <li>Put everything in a mixing bowl (if you have kitchen scales, put the mixing bowl on them, set scales to zero, add an ingredient, reset scales to zero, add the next ingredient, etc.)</li>
  <li>Mix.</li>
  <li>Taste [warning: <a href="https://www.cdc.gov/foodsafety/communication/no-raw-dough.html">public health officials say not to do this because eating raw flour is dangerous</a>]. Adjust mixedness, saltiness, etc. It should be very roughly the consistency of peanut butter, i.e. probably less firm than you expect. (Taste more, as desired. Wonder why we cook biscuits at all. Consider rebellion. Consider Chesterton’s fence. Taste one more time.)</li>
  <li>Cover a big tray or a couple of small trays with baking paper.</li>
  <li>Make the dough into about four logs, around an inch in diameter, spaced several inches from one another and the edges of the paper. They can be misshapen; their shapes are temporary.</li>
  <li>Cook for about 15 minutes, or until golden and spread out into 1-4 giant flat seas of biscuit. When you take them out, they will be very soft and probably not appear to be cooked.</li>
  <li>As soon as they slightly cool and firm up enough to pick up, start chopping them into strips about 1.25 inches wide and eating them.</li>
</ol>

<p> </p>

<p>*</p>

<p> </p>

<p>Bonus mystery: they are gluten free, egg free, and can probably easily be dairy free. The contest with common vegan and/or gluten free biscuit seems even more winnable, so why haven’t they even taken over that market?</p>

  </description>
  <link>/2020/12/07/why-are-delicious-biscuits-obscure.html</link>
  <guid isPermaLink="true">http://localhost:4000/2020/12/07/why-are-delicious-biscuits-obscure.html</guid>
  <pubDate>Mon, 07 Dec 2020 08:06:40 -0800</pubDate>
 </item>
 
 <item>
  <title>Cultural accumulation</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>When I think of humans being so smart due to ‘cultural accumulation’, I think of lots of tiny innovations in thought and technology being made by different people, and added to the interpersonal currents of culture that wash into each person’s brain, leaving a twenty year old in 2020 much better intellectually equipped than a 90 year old who spent their whole life thinking in 1200 AD.<!--ex--></p>

<p>This morning I was chatting to my boyfriend about whether a person who went back in time (let’s say a thousand years) would be able to gather more social power than they can now in their own time. Some folk we know were <a href="https://www.lesswrong.com/s/mzgtmmTKKn5MuCzFJ/p/oiuZjPfknKsSc5waC#Discussion_on_the_ease_of_taking_control_of_the_world">discussing</a> the claim that some humans would have a shot at literally take over the world if sent back in time, and we found this implausible.</p>

<p>The most obvious differences between a 2020 person and a 1200 AD person, in 1200 AD, is that they have experience with incredible technological advances that the 1200 AD native doesn’t even know are possible. But a notable thing about a modern person is that they famously <a href="https://www.behance.net/gallery/35437979/Velocipedia">don’t know what a bicycle looks like</a>, so the level of technology they might be able to actually rebuild on short notice  in 1200 AD is probably not at the level of a nutcracker, and they probably already had those in 1200 AD.</p>

<p>How does 2020 have complicated technology, if most people don’t know how it works? One big part is specialization: across the world, quite a few people do know what bicycles look like. And more to the point, presumably some of them know in great detail what bicycle chains look like, and what they are made of, and what happens if you make them out of slightly different materials or in slightly different shapes, and how such things interact with the functioning of the bicycle.</p>

<p>But suppose the 2020 person who is sent back is a bicycle expert, and regularly builds their own at home. Can they introduce bikes to the world <a href="https://en.wikipedia.org/wiki/Bicycle#History">600 years early</a>? My tentative guess is yes, but not very ridable ones, because they don’t have machines for making bike parts, or any idea what those machines are like or the principles behind them. They can probably demonstrate the idea of a bike with wood and cast iron and leather, supposing others are cooperative with various iron casting, wood shaping, leather-making know-how. But can they make a bike that is worth paying for and riding?</p>

<p>I’m not sure, and bikes were selected here for being so simple that an average person might know what their machinery looks like. Which makes them unusually close among technologies to simple chunks of metal. I don’t think a microwave oven engineer can introduce microwave ovens in 1200, or a silicon chip engineer can make much progress on introducing silicon chips. These require other technologies that require other technologies too many layers back.</p>

<p>But what if the whole of 2020 society was transported to 1200? The metal extruding experts and the electricity experts and the factory construction experts and Elon Musk? Could they just jump back to 2020 levels of technology, since they know everything relevant between them? (Assuming they are somehow as well coordinated in this project as they are in 2020, and are not just putting all of their personal efforts into avoiding being burned at the stake or randomly tortured in the streets.)</p>

<p>A big way this might fail is if 2020 society knows everything between them needed to use 2020 artifacts to get more 2020 artifacts, but don’t know how to use 1200 artifacts to get 2020 artifacts.</p>

<p>On that story, the 1200 people might start out knowing methods for making c. 1200 artifacts using c. 1200 artifacts, but they accumulate between them the ideas to get them to c. 1220 artifacts with the c. 1200 artifacts, which they use to actually create those new artifacts. They pass to their children this collection of c. 1220 artifacts and the ideas needed to use those artifacts to get more c. 1220 artifacts. But the new c. 1220 artifacts and methods replaced some of the old c. 1200 artifacts and methods. So the knowledge passed on doesn’t include how to use those obsoleted artifacts to create the new artifacts, or the knowledge about how to make the obsoleted artifacts. And the artifacts passed on don’t include the obsoleted ones. If this happens every generation for a thousand years, the cultural inheritance received by the 2020 generation includes some highly improved artifacts plus the knowledge about how to use them, but not necessarily any record of the path that got there from prehistory, or of the tools that made the tools that made the tools that made these artifacts.</p>

<p>This differs from my first impression of ‘cultural accumulation’ in that:</p>
<ol>
  <li>physical artifacts are central to the process: a lot of the accumulation is happening inside them, rather than in memetic space.</li>
  <li>humanity is not accumulating all of the ideas it has come up with so far, even the important ones. It is accumulating something more like a best set of instructions for the current situation, and throwing a lot out as it goes.</li>
</ol>

<p>Is this is how things are, or is my first impression more true?</p>

  </description>
  <link>/2020/12/05/cultural-accumulation.html</link>
  <guid isPermaLink="true">http://localhost:4000/2020/12/05/cultural-accumulation.html</guid>
  <pubDate>Sat, 05 Dec 2020 21:08:11 -0800</pubDate>
 </item>
 
 <item>
  <title>Misalignment and misuse: whose values are manifest?</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>AI related disasters are often categorized as involving misaligned AI, or misuse, or accident. Where:</p>
<ul>
  <li>misuse means the bad outcomes were wanted by the people involved,</li>
  <li>misalignment means the bad outcomes were wanted by AI (and not by its human creators), and</li>
  <li>accident means that the bad outcomes were not wanted by those in power but happened anyway due to error.</li>
</ul>

<p>In thinking about specific scenarios, these concepts seem less helpful.<!--ex--></p>

<p>I think a likely scenario leading to bad outcomes is that AI can be made which gives a set of people things they want, at the expense of future or distant resources that the relevant people do not care about or do not own.</p>

<p>For example, consider autonomous business strategizing AI systems that are profitable additions to many companies, but in the long run accrue resources and influence and really just want certain businesses to nominally succeed, resulting in a worthless future. Suppose Bob is considering whether to get a business strategizing AI for his business. It will make the difference between his business thriving and struggling, which will change his life. He suspects that within several hundred years, if this sort of thing continues, the AI systems will control everything. Bob probably doesn’t hesitate, in the way that businesses don’t hesitate to use gas vehicles even if the people involved genuinely think that climate change will be a massive catastrophe in hundreds of years.</p>

<p>When the business strategizing AI systems finally plough all of the resources in the universe into a host of thriving 21st Century businesses, was this misuse or misalignment or accident? The strange new values that were satisfied were those of the AI systems, but the entire outcome only happened because people like Bob chose it knowingly (let’s say). Bob liked it more than the long glorious human future where his business was less good. That sounds like misuse. Yet also in a system of many people, letting this decision fall to Bob may well have been an accident on the part of others, such as the technology’s makers or legislators.</p>

<p>Outcomes are the result of the interplay of choices, driven by different values. Thus it isn’t necessarily sensical to think of them as flowing from one entity’s values or another’s. Here, AI technology created a better option for both Bob and some newly-minted misaligned AI values that it also created—‘Bob has a great business, AI gets the future’—and that option was worse for the rest of the world. They chose it together, and the choice needed both Bob to be a misuser and the AI to be misaligned. But this isn’t a weird corner case, this is a natural way for the future to be destroyed in an economy.</p>

<p><em>Thanks to Joe Carlsmith for conversation leading to this post.</em></p>

  </description>
  <link>/2020/11/13/misalignment-and-misuse.html</link>
  <guid isPermaLink="true">http://localhost:4000/2020/11/13/misalignment-and-misuse.html</guid>
  <pubDate>Fri, 13 Nov 2020 00:25:04 -0800</pubDate>
 </item>
 
 <item>
  <title>Tweet markets for impersonal truth tracking?</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>Should social media label statements as false, misleading or contested?</p>

<p>Let’s approach it from the perspective of what would make the world best, rather than e.g. what rights do the social media companies have, as owners of the social media companies.</p>

<p>The basic upside seems to be that pragmatically, people share all kinds of false things on social media, and that leads to badness, and this slows that down.<!--ex--></p>

<p>The basic problem with it is that maybe we can’t distinguish worlds where social media companies label false things as false, and those where they label things they don’t like as false, or things that aren’t endorsed by other ‘official’ entities. So maybe we don’t want such companies to have the job of deciding what is considered true or false, because a) we don’t trust them enough to give them this sacred and highly pressured job forever, or b) we don’t expect everyone to trust them forever, and it would be nice to have better recourse when disagreement appears than ‘but I believe them’.</p>

<p>If there were a way to systematically inhibit or label false content based on its falseness directly, rather than via a person’s judgment, that would be an interesting solution that perhaps everyone reasonable would agree to add. If prediction markets were way more ubiquitous, each contentious propositional Tweet could say under it the market odds for the claim.</p>

<p>Or what if Twitter itself were a prediction market, trading in Twitter visibility? For just-posted Tweets, instead of liking them, you can bet your own cred on them. Then a while later, they are shown again and people can vote on whether they turned out right and you win or lose cred. Then your total cred determines how much visibility your own Tweets get.</p>

<p>It seems like this would solve:</p>
<ul>
  <li>the problem for prediction markets where it is illegal to bet money and hard to be excited about fake money</li>
  <li>the problem for prediction markets where it’s annoying to go somewhere to predict things when you are doing something else, like looking at Twitter</li>
  <li>the problem for Twitter where it is full of fake claims</li>
  <li>the problem for Twitter users where they have to listen to fake claims all the time, and worry about whether all kinds of things are true or not</li>
</ul>

<p>It would be pretty imperfect, since it throws the gavel to future Twitter users, but perhaps they are an improvement on the status quo, or on the status quo without the social media platforms themselves making judgments.</p>

  </description>
  <link>/2020/11/09/tweet-markets.html</link>
  <guid isPermaLink="true">http://localhost:4000/2020/11/09/tweet-markets.html</guid>
  <pubDate>Mon, 09 Nov 2020 10:01:22 -0800</pubDate>
 </item>
 
 <item>
  <title>Automated intelligence is not AI</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>Sometimes we think of ‘artificial intelligence’ as whatever technology ultimately automates human cognitive labor.</p>

<p>I question this equivalence, looking at past automation. In practice human cognitive labor is replaced by things that don’t seem at all cognitive, or like what we otherwise mean by AI.</p>

<p>Some examples:<!--ex--></p>
<ol>
  <li>Early in the existence of bread, it might have been toasted by someone holding it close to a fire and repeatedly observing it and recognizing its level of doneness and adjusting. Now we have machines that hold the bread exactly the right distance away from a predictable heat source for a perfect amount of time. You could say that the shape of the object embodies a lot of intelligence, or that intelligence went into creating this ideal but non-intelligent tool.</li>
  <li>Self-cleaning ovens replace humans cleaning ovens. Humans clean ovens with a lot of thought—looking at and identifying different materials and forming and following plans to remove some of them. Ovens clean themselves by getting very hot.</li>
  <li>Carving a rabbit out of chocolate takes knowledge of a rabbit’s details, along with knowledge of how to move your hands to translate such details into chocolate with a knife. A rabbit mold automates this work, and while this route may still involve intelligence in the melting and pouring of the chocolate, all rabbit knowledge is now implicit in the shape of the tool, though I think nobody would call a rabbit-shaped tin ‘artificial intelligence’.</li>
  <li>Human pouring of orange juice into glasses involves various mental skills. For instance, classifying orange juice and glasses and judging how they relate to one another in space, and moving them while keeping an eye on this. Automatic orange juice pouring involves for instance a button that can only be pressed with a glass when the glass is in a narrow range of locations, which opens an orange juice faucet running into a spot common to all the possible glass-locations.</li>
</ol>

<p>Some of this is that humans use intelligence where they can use some other resource, because it is cheap on the margin where the other resource is expensive. For instance, to get toast, you could just leave a lot of bread at different distances then eat the one that is good. That is bread-expensive and human-intelligence-cheap (once you come up with the plan at least). But humans had lots of intelligence and not much bread. And if later we automate a task like this, before we have computers that can act very similarly to brains, then the alternate procedure will tend to be one that replaces human thought with something that actually is cheap at the time, such as metal.</p>

<p>I think a lot of this is that to deal with a given problem you can either use flexible intelligence in the moment, or you can have an inflexible system that happens to be just what you need. Often you will start out using the flexible intelligence, because being flexible it is useful for lots of things, so you have some sitting around for everything, whereas you don’t have an inflexible system that happens to be just what you need. But if a problem seems to be happening a lot, it can become worth investing the up-front cost of getting the ideal tool, to free up your flexible intelligence again.</p>

  </description>
  <link>/2020/11/01/automating-intelligence.html</link>
  <guid isPermaLink="true">http://localhost:4000/2020/11/01/automating-intelligence.html</guid>
  <pubDate>Sun, 01 Nov 2020 01:58:21 -0700</pubDate>
 </item>
 
 <item>
  <title>Whence the symptoms of social media?</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>A thing I liked about <em>The Social Dilemma</em> was the evocative image of oneself being in an epic contest for one’s attention with a massive and sophisticated data-nourished machine, tended by teams of manipulation experts. The hopelessness of the usual strategies—like spur-of-the-moment deciding to ‘try to use social media less’—in the face of such power seems clear.</p>

<p>But <a href="https://worldspiritsockpuppet.com/2020/10/26/the-social-dilemma-review.html">another</a> question I have is whether this basic story of our situation—that powerful forces are fluently manipulating our behavior—is true.</p>

<p>Some contrary observations from my own life:<!--ex--></p>
<ul>
  <li><strong>The phenomenon of spending way too long doing apparently pointless things on my phone seems to be at least as often caused by things that are not massively honed to manipulate me.</strong> For instance, I recently play a lot of <a href="https://en.wikipedia.org/wiki/Nonogram">nonograms</a>, a kind of visual logic puzzle that was <a href="https://en.wikipedia.org/wiki/Nonogram#History">invented by two people independently in the 80s</a> and which I play in one of many somewhat awkward-to-use phone apps, I assume made by small teams mostly focused on making the app work smoothly. My sense is that if I didn’t have nonograms style games or social media or news to scroll through, then I would still often idly pick up my phone and draw, or read books, or learn Spanish, or memorize geographic facts, or scroll through just anything on offer to scroll through (I also do these kinds of things already). So my guess is that it is my phone’s responsiveness and portability and tendency to do complicated things if you press buttons on it, that makes it a risk for time consumption. Facebook’s efforts to grab my attention probably don’t hurt, but I don’t feel like they are most of the explanation for phone-overuse in my own life.</li>
  <li><strong>Notifications seem clumsy and costly.</strong> They do grab my attention pretty straightforwardly, but this strategy appears to have about the sophistication of going up to someone and tapping them on the shoulder continually, when you have a sufficiently valuable relationship that they can’t just break it off you annoy them too much. In that case it isn’t some genius manipulation technique, it’s just burning through the goodwill the services have gathered by being valuable in other ways. If I get unnecessary notifications, I am often annoyed and try to stop them or destroy the thing causing them.</li>
  <li><strong>I do often scroll through feeds for longer than I might have planned to, but the same goes for non-manipulatively-honed feeds.</strong> For instance when I do a Google Image search for skin infections, or open some random report and forget why I’m looking at it. So I think scrolling down things might be a pretty natural behavior for things that haven’t finished yet, and are interesting at all (but maybe not so interesting that one is, you know, awake..)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup></li>
  <li><strong>A thing that feels attractive about Facebook is that one wants to look at things that other people are looking at.</strong> (Thus for instance reading books and blog posts that just came out over older, better ones.) Social media have this, but presumably not much more than newspapers did before, since a greater fraction of the world was looking at the same newspaper before.</li>
</ul>

<p>In sum, I offer the alternate theory that various technology companies have combined:</p>
<ul>
  <li>pinging people</li>
  <li>about things they are at least somewhat interested in</li>
  <li>that everyone is looking at</li>
  <li>situated in an indefinite scroll</li>
  <li>on a responsive, detailed pocket button-box</li>
</ul>

<p>…and that most of the attention-suck and influence that we see is about those things, not about the hidden algorithmic optimizing forces that Facebook might have.</p>

<hr />

<p><em>(<a href="https://worldspiritsockpuppet.com/2020/10/26/the-social-dilemma-review.html">Part 1 of Social Dilemma review</a>)</em></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>My boyfriend offers alternate theory, that my scrolling instinct comes from Facebook. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </description>
  <link>/2020/10/27/social-dilemma-2.html</link>
  <guid isPermaLink="true">http://localhost:4000/2020/10/27/social-dilemma-2.html</guid>
  <pubDate>Tue, 27 Oct 2020 00:22:12 -0700</pubDate>
 </item>
 
 <item>
  <title>But what kinds of puppets are we?</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>I watched <a href="https://www.thesocialdilemma.com/">The Social Dilemma</a> last night. I took the problem that it warned of to be the following:</p>

<ol>
  <li>Social media and similar online services make their money by <a href="https://worldspiritsockpuppet.com/2020/10/14/the-bads-of-ads.html">selling your attention to advertisers</a></li>
  <li>These companies put vast optimization effort into manipulating you, to extract more attention</li>
  <li>This means your behavior and attention is probably very shaped by these forces (which you can perhaps confirm by noting your own readiness to scroll through stuff on your phone)</li>
</ol>

<p>This seems broadly plausible and bad, but I wonder if it isn’t quite that bad.<!--ex--></p>

<p>I heard the film as suggesting that your behavior and thoughts in general are being twisted by these forces. But lets distinguish between a system where huge resources are going into keeping you scrolling say—at which point an advertiser will pay for their shot at persuading you—and a system where those resources are going into manipulating you directly to do the things that the advertiser would like. In the first case, maybe you look at your phone too much, but there isn’t a clear pressure on your opinions or behavior besides pro phone. In the second case, maybe you end up with whatever opinions and actions someone paid the most for (this all supposing the system works). Let’s call these distorted-looking and distorted-acting.</p>

<p>While watching I interpreted the film suggesting the sort of broad manipulation that would come with distorted-acting, but thinking about it afterwards, isn’t the kind of optimization going on with social media actually distorted-looking? (Followed by whatever optimization the advertisers do to get you to do what they want, which I guess is of a kind with what they have always done, so at least not a new experimental horror.) I actually don’t really know. And maybe it isn’t a bright distinction.</p>

<p>Maybe optimization for you <em>clicking</em> on ads should be a different category (i.e. ‘distorted-clicking’). This seems close to distorted-looking, in that it isn’t directly seeking to manipulate your behavior outside of your phone session, but a big step closer to distorted-acting, since you have been set off toward whatever you have ultimately been targeted to buy.</p>

<p>I was at first thinking that distorted-looking was safer than distorted-acting. But distorted-looking forces probably do also distort your opinions and actions. For instance, as the film suggested, you are likely to look more if you get interested in something that there is a lot of content on, or something that upsets you and traps your attention.</p>

<p>I could imagine distorted-looking actually being worse than distorted-acting: when your opinion can be bought, the change in it is presumably what someone would want. Whereas when your opinion is manipulated as a weird side effect of someone trying to get you to look more, then it could be any random thing, which might be terrible.(Or would there be such weird side effects in both cases anyway?)</p>

  </description>
  <link>/2020/10/26/the-social-dilemma-review.html</link>
  <guid isPermaLink="true">http://localhost:4000/2020/10/26/the-social-dilemma-review.html</guid>
  <pubDate>Mon, 26 Oct 2020 00:52:20 -0700</pubDate>
 </item>
 
 <item>
  <title>Yet another world spirit sock puppet</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>I have almost successfully made and made decent <a href="https://worldspiritsockpuppet.com">this here</a> my new blog, in spite of little pre-existing familiarity with relevant tools beyond things like persistence in the face of adversity and Googling things. I don’t fully understand how it works, but it is a different and freer non-understanding than with Wordpress or Tumblr. This blog is more mine to have mis-built and to go back and fix. It is like not understanding why your cake is still a liquid rather than like not understanding why your printer isn’t recognized by your computer.</p>

<p>My plan is to blog at <a href="https://worldspiritsockpuppet.com">worldspiritsockpuppet.com</a> now, and cross-post to my older blogs the subset of posts that fit there.</p>

<p>The main remaining thing is to add comments. If anyone has views about how those should be, er, tweet at me?</p>

  </description>
  <link>/2020/10/24/new-blog.html</link>
  <guid isPermaLink="true">http://localhost:4000/2020/10/24/new-blog.html</guid>
  <pubDate>Sat, 24 Oct 2020 22:11:19 -0700</pubDate>
 </item>
 
 <item>
  <title>The bads of ads</title>
  <description>
    <p><emph>Crossposted via RSS from worldspiritsockpuppet.com; faithful transmission uncertain</emph></p>
    <br>
    <p>In London at the start of the year, perhaps there was more advertising than there usually is in my life, because I found its presence disgusting and upsetting. Could I not use public transport without having my mind intruded upon continually by trite performative questions?</p>

<p><img src="https://hosting.photobucket.com/images/i/katjasgrace/eendra_underground_ads.jpg" alt="London underground" style="margin:25px 0px" /></p>

<p>Sometimes I fantasize about a future where stealing someone’s attention to suggest for the fourteenth time that they watch your awful-looking play is rightly looked upon as akin to picking their pocket.</p>

<p>Stepping back, advertising is widely found to be a distasteful activity. But I think it is helpful to distinguish the different unpleasant flavors potentially involved (and often not involved—there is good advertising):<!--ex--></p>

<ol>
  <li>
    <p><strong>Mind manipulation</strong>: Advertising is famous for uncooperatively manipulating people’s beliefs and values in whatever way makes them more likely to pay money somehow. For instance, deceptively encouraging the belief that everyone uses a certain product, or trying to spark unwanted wants.</p>

    <p><img src="https://hosting.photobucket.com/images/i/katjasgrace/markham_ad_paint.jpg" alt="Painting an ad" style="margin:25px 0px" /></p>
  </li>
  <li>
    <p><strong>Zero-sumness</strong>: To the extent advertising is aimed at raising the name recognition and thus market share of one product over its similar rivals, it is zero or negative sum: burning effort on both sides and the attention of the customer for no overall value.</p>

    <p><img src="https://hosting.photobucket.com/images/i/katjasgrace/subiyanto_nestle_ad.jpg" height="495" /> <img src="https://hosting.photobucket.com/images/i/katjasgrace/weir_coke_ad.jpg" height="495" /></p>
  </li>
  <li>
    <p><strong>Theft of a precious thing</strong>: Attention is arguably one of the best things you have, and its protection arguably worthy of great effort. In cases where it is vulnerable—for instance because you are outside and so do not personally control everything you might look at or hear—advertising is the shameless snatching of it. This might be naively done, in the same way that a person may naively steal silverware assuming that it is theirs to take because nothing is stopping them.</p>

    <p><img src="https://hosting.photobucket.com/images/i/katjasgrace/williamsantos_underground.png" alt="London underground" /></p>
  </li>
  <li>
    <p><strong>Cultural poison</strong>: Culture and the common consciousness are an organic dance of the multitude of voices and experiences in society. In the name of advertising, huge amounts of effort and money flow into amplifying fake voices, designed to warp perceptions–and therefore the shared world–to ready them for exploitation. Advertising can be a large fraction of the voices a person hears. It can draw social creatures into its thin world. And in this way, it goes beyond manipulating the minds of those who listen to it. Through those minds it can warp the whole shared world, even for those who don’t listen firsthand. Advertising shifts your conception of what you can do, and what other people are doing, and what you should pay attention to. It presents role models, designed entirely for someone else’s profit. It saturates the central gathering places with inanity, as long as that might sell something.</p>

    <p><img src="https://hosting.photobucket.com/images/i/katjasgrace/geib_ads.jpg" alt="Outdoor ads over darkened figures" style="margin:25px 0px" /></p>
  </li>
  <li>
    <p><strong>Market failure</strong>: Ideally, whoever my attention is worth most to would get it, regardless of whether it was initially stolen. For instance, if I have better uses for my attention than advertising, hopefully I will pay more to have it back than the advertiser expects to make by advertising to me. So we will be able to make a trade, and I’ll get my attention back. In practice this is probably too complicated, since so many tiny transactions are needed. E.g. the best message for me to see, if I have to see a message, when sitting on a train, is probably something fairly different from what I do see. It is also probably worth me paying a small sum to each person who would advertise at me to just see a blank wall instead. But it is hard for them to collect that money from each person. And in cases where the advertiser was just a random attention thief and didn’t have some special right to my attention, if I were to pay one to leave me alone, another one might immediately replace them.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup></p>

    <p><img src="https://hosting.photobucket.com/images/i/katjasgrace/clear_ad_wu.jpg" alt="Underground ads over crowd" style="margin:25px 0px" /></p>
  </li>
  <li>
    <p><strong>Ugliness</strong>: At the object level, advertising is often clearly detracting from the beauty of a place.</p>

    <p><img src="https://hosting.photobucket.com/images/i/katjasgrace/negspace_ads.jpg" alt="Ads overwhelming buildings" style="margin:25px 0px" /></p>
  </li>
</ol>

<p>These aren’t necessarily distinct—to the extent ugliness is bad, say, one might expect that it is related to some market failure. But they are different reasons for disliking a thing-a person can hate something ugly while having no strong view on the perfection of ideal markets.</p>

<p>What would good and ethical advertising look like? Maybe I decide that I want to be advertised to now, and go to my preferred advertising venue. I see a series of beautiful messages about things that are actively helpful for me to know. I can downvote ads if I don’t like the picture of the world that they are feeding into my brain, or the apparent uncooperativeness of their message. I leave advertising time feeling inspired and happy.</p>

<p><img src="https://hosting.photobucket.com/images/i/katjasgrace/newstory_ads.jpg" alt="Ads: we are building a new story" style="margin:25px 0px" /></p>

<hr />

<p>Images: <a href="https://unsplash.com/photos/QG7Wkq2ZrpE">London Underground: Mona Eendra</a>, <a href="https://www.pexels.com/photo/man-painting-wall-2448522/">painting ads: Megan Markham</a>, <a href="https://www.pexels.com/photo/woman-wearing-a-face-mask-on-the-subway-4429291/">Nescafe ad: Ketut Subiyanto</a>, <a href="https://unsplash.com/photos/SUi9mYSVTyc">Coca-Cola: Hamish Weir</a>, <a href="https://unsplash.com/photos/6b8I4nxXPb0">London Underground again: Willam Santos</a>, <a href="https://www.pexels.com/photo/people-waiting-for-the-red-bus-to-pass-3220846/">figures in shade under ad: David Geib</a>, <a href="https://www.pexels.com/photo/people-standing-inside-train-3380873/">Clear ad in train: Life of Wu</a>, <a href="https://www.pexels.com/photo/light-london-adverts-piccadilly-circus-34639/">Piccadilly Circus: Negative Space</a>, <a href="https://unsplash.com/photos/xennYrcP3aM">Building a new story: Wilhelm Gunkel</a>.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>For advertising in specific public locations, I could in principle pay by buying up the billboard or whatever and leaving it blank. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </description>
  <link>/2020/10/13/the-bads-of-ads.html</link>
  <guid isPermaLink="true">http://localhost:4000/2020/10/13/the-bads-of-ads.html</guid>
  <pubDate>Tue, 13 Oct 2020 22:30:00 -0700</pubDate>
 </item>
 

</channel>
</rss>
